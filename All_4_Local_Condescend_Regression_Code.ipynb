{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "All 4 Local Condescend Regression Code",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fc00e7a15c154e9c897db8f3190d9662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ffa73d8411f4ffea95bd5f2e3f6aa1c",
              "IPY_MODEL_27f8bbfd80e04454b1f097c72b5c2580",
              "IPY_MODEL_5b873e22daf7463a82ef2ca14f7db08d"
            ],
            "layout": "IPY_MODEL_cacaa16a4bf947f1968ad19264b2073a"
          }
        },
        "2ffa73d8411f4ffea95bd5f2e3f6aa1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc0036eb77b949d29b1516f1f47f54e0",
            "placeholder": "​",
            "style": "IPY_MODEL_504e66190e694a4bbc0ee236c2e7e1de",
            "value": "Downloading: 100%"
          }
        },
        "27f8bbfd80e04454b1f097c72b5c2580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e6017457be544839fce4742d7d41294",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4cec8bf797643dd991e8169a147bc87",
            "value": 28
          }
        },
        "5b873e22daf7463a82ef2ca14f7db08d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c244c621c8254dbfa4a6dab68b59dd43",
            "placeholder": "​",
            "style": "IPY_MODEL_f3690f70e3f54899869bcbad03fd5809",
            "value": " 28.0/28.0 [00:00&lt;00:00, 2.07kB/s]"
          }
        },
        "cacaa16a4bf947f1968ad19264b2073a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc0036eb77b949d29b1516f1f47f54e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "504e66190e694a4bbc0ee236c2e7e1de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e6017457be544839fce4742d7d41294": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4cec8bf797643dd991e8169a147bc87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c244c621c8254dbfa4a6dab68b59dd43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3690f70e3f54899869bcbad03fd5809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10c081a6f96a427c9647385705c89d94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09dbe4ee4af945bf94a9f817bf509212",
              "IPY_MODEL_b1d5f4a594b244cc94ca22356381c306",
              "IPY_MODEL_ef6aff74e7d54a818183c215afd18d39"
            ],
            "layout": "IPY_MODEL_6671f1a8226e42f796dc5a8a27406ee9"
          }
        },
        "09dbe4ee4af945bf94a9f817bf509212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8caee931326e4e029f3477436e40a399",
            "placeholder": "​",
            "style": "IPY_MODEL_84ea7d402a52465b8c46b0805693f906",
            "value": "Downloading: 100%"
          }
        },
        "b1d5f4a594b244cc94ca22356381c306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebb4f3537632421bb0e7e53d8dc295c8",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86db037ad7b14eee8e19bd0fcce73213",
            "value": 570
          }
        },
        "ef6aff74e7d54a818183c215afd18d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6704e6e21cb74df2aa2ad92c0cf716ab",
            "placeholder": "​",
            "style": "IPY_MODEL_44f7b24b1cdf4efeaa0aacf750a080f3",
            "value": " 570/570 [00:00&lt;00:00, 40.7kB/s]"
          }
        },
        "6671f1a8226e42f796dc5a8a27406ee9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8caee931326e4e029f3477436e40a399": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84ea7d402a52465b8c46b0805693f906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebb4f3537632421bb0e7e53d8dc295c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86db037ad7b14eee8e19bd0fcce73213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6704e6e21cb74df2aa2ad92c0cf716ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44f7b24b1cdf4efeaa0aacf750a080f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc4018ed0bb945aba2ecff1b423dea7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_beb89aca0cb746d2ace33d41ebdfec87",
              "IPY_MODEL_c6b2c629cd144bd982088aa8a5ccc360",
              "IPY_MODEL_7561820e8692492ab0cf5b9ca4b84fbd"
            ],
            "layout": "IPY_MODEL_6c9dcda1431441bfbce64f401fc92f6d"
          }
        },
        "beb89aca0cb746d2ace33d41ebdfec87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8b872473b6b44c8b75f577f85732e9d",
            "placeholder": "​",
            "style": "IPY_MODEL_e0ee01261a274035b5d586dd8bcdc919",
            "value": "Downloading: 100%"
          }
        },
        "c6b2c629cd144bd982088aa8a5ccc360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2178c6baf7324bd9bb155d0b18cced1b",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ad14c9335db4332b6690c8fd4d0502c",
            "value": 231508
          }
        },
        "7561820e8692492ab0cf5b9ca4b84fbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_151faaeca98d42669fd5cec260f55a0d",
            "placeholder": "​",
            "style": "IPY_MODEL_57100b9f1010478a8d5eeee74f09e7df",
            "value": " 226k/226k [00:00&lt;00:00, 10.8MB/s]"
          }
        },
        "6c9dcda1431441bfbce64f401fc92f6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8b872473b6b44c8b75f577f85732e9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0ee01261a274035b5d586dd8bcdc919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2178c6baf7324bd9bb155d0b18cced1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ad14c9335db4332b6690c8fd4d0502c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "151faaeca98d42669fd5cec260f55a0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57100b9f1010478a8d5eeee74f09e7df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "604c111371894b6b9ccbbbd61b400e64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef50a78983fd40df8d9743ec1de24aa6",
              "IPY_MODEL_1e1931e662404c9db758f7cbb7fd8ec2",
              "IPY_MODEL_78869b3f278843c09fd026a64d907760"
            ],
            "layout": "IPY_MODEL_c602ecc710104c80994012b1531d2b07"
          }
        },
        "ef50a78983fd40df8d9743ec1de24aa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9794a54257ec49e5922432e1792d23aa",
            "placeholder": "​",
            "style": "IPY_MODEL_a4895ba7198a40aaa85015462a0961da",
            "value": "Downloading: 100%"
          }
        },
        "1e1931e662404c9db758f7cbb7fd8ec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b3d569c1b8849f0aeb49bd71e0b440d",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7294cb0e2f44cedaca57e9a0ada910e",
            "value": 466062
          }
        },
        "78869b3f278843c09fd026a64d907760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_593ad0d934db4079a7c4daa84383aa8b",
            "placeholder": "​",
            "style": "IPY_MODEL_9e02a922fe9a4ee6bbb057dd69aef2eb",
            "value": " 455k/455k [00:00&lt;00:00, 16.9MB/s]"
          }
        },
        "c602ecc710104c80994012b1531d2b07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9794a54257ec49e5922432e1792d23aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4895ba7198a40aaa85015462a0961da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b3d569c1b8849f0aeb49bd71e0b440d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7294cb0e2f44cedaca57e9a0ada910e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "593ad0d934db4079a7c4daa84383aa8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e02a922fe9a4ee6bbb057dd69aef2eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "761e7eb97362404699d6eb0f9b57e9da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3934e88b127f46dcb35135279153e5c2",
              "IPY_MODEL_a5887d27d3e945c5986a21e640e85666",
              "IPY_MODEL_de8c6832db584a8fa0c1b4d0bd96a465"
            ],
            "layout": "IPY_MODEL_0ac2451ed7d54d60bd6848b35067c771"
          }
        },
        "3934e88b127f46dcb35135279153e5c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1b6d43454d0451f8491b5cc35ed33e9",
            "placeholder": "​",
            "style": "IPY_MODEL_ca3e4d58f0f24d819cf036d7ad358030",
            "value": "Downloading: 100%"
          }
        },
        "a5887d27d3e945c5986a21e640e85666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4746c63c25c947d0b2362b79444e4bc3",
            "max": 536063208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d56ef9ea90674eb38facf4e3ba4dc94b",
            "value": 536063208
          }
        },
        "de8c6832db584a8fa0c1b4d0bd96a465": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c2ccf19c9b14ea09c1461d2e21249be",
            "placeholder": "​",
            "style": "IPY_MODEL_5e7a743d323248ef867b75d94db95299",
            "value": " 511M/511M [00:10&lt;00:00, 45.0MB/s]"
          }
        },
        "0ac2451ed7d54d60bd6848b35067c771": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1b6d43454d0451f8491b5cc35ed33e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca3e4d58f0f24d819cf036d7ad358030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4746c63c25c947d0b2362b79444e4bc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d56ef9ea90674eb38facf4e3ba4dc94b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c2ccf19c9b14ea09c1461d2e21249be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e7a743d323248ef867b75d94db95299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d699a982173443e6b419db092f4f72a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fabb7a39d5a4681b3cd9b8bbdc99cfe",
              "IPY_MODEL_80c064247973460694774bd7cbd6a924"
            ],
            "layout": "IPY_MODEL_1ec1ba5447a541dda38582a909bcbc1d"
          }
        },
        "0fabb7a39d5a4681b3cd9b8bbdc99cfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9b96fbff3fa435c81b7bf855db7d060",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09232485d6fb4b858b22d981c0caff8d",
            "value": 440473133
          }
        },
        "80c064247973460694774bd7cbd6a924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f199216a11d64de28a4510928a1bff40",
            "placeholder": "​",
            "style": "IPY_MODEL_400ba32281d1453b8d185e82b8997ff7",
            "value": " 440M/440M [00:05&lt;00:00, 75.8MB/s]"
          }
        },
        "1ec1ba5447a541dda38582a909bcbc1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9b96fbff3fa435c81b7bf855db7d060": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09232485d6fb4b858b22d981c0caff8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "f199216a11d64de28a4510928a1bff40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "400ba32281d1453b8d185e82b8997ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL-MObmYVe-4",
        "outputId": "df643a41-8a51-4068-80e3-57d3362acb4f"
      },
      "source": [
        "cd Downloads/Audio-Signal-Feature-Extraction-And-Clustering"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/howard56k/Downloads/Audio-Signal-Feature-Extraction-And-Clustering\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2u3Z8QMc8Gj",
        "outputId": "8f1205bc-5793-403e-ccff-724e13976770"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Volume in drive D is PCI ssd\n",
            " Volume Serial Number is 86AE-1017\n",
            "\n",
            " Directory of D:\\Projects\\Audio-Signal-Feature-Extraction-And-Clustering\n",
            "\n",
            "08/08/2021  01:29 AM    <DIR>          .\n",
            "08/08/2021  01:29 AM    <DIR>          ..\n",
            "08/08/2021  10:57 AM       402,149,939 listOFEverything.pickle\n",
            "08/07/2021  11:32 PM    <DIR>          pickles\n",
            "08/07/2021  10:39 PM    <DIR>          real mmse scores\n",
            "08/07/2021  10:39 PM    <DIR>          test-mmse\n",
            "               1 File(s)    402,149,939 bytes\n",
            "               5 Dir(s)  121,833,979,904 bytes free\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mos00F3MEM1u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpU_E21d4u7S",
        "outputId": "ddcc4175-ad7c-4cf9-c764-9483ecadb923"
      },
      "source": [
        "!pip install mir_eval pyAudioAnalysis eyed3 pydub sklearn matplotlib plotly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mir_eval in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (0.6)\r\n",
            "Requirement already satisfied: pyAudioAnalysis in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (0.3.7)\r\n",
            "Requirement already satisfied: eyed3 in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (0.9.6)\r\n",
            "Requirement already satisfied: pydub in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (0.25.1)\r\n",
            "Requirement already satisfied: sklearn in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (0.0)\r\n",
            "Requirement already satisfied: matplotlib in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (3.4.2)\r\n",
            "Requirement already satisfied: plotly in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (5.1.0)\r\n",
            "Requirement already satisfied: scipy>=1.0.0 in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from mir_eval) (1.4.1)\r\n",
            "Requirement already satisfied: future in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from mir_eval) (0.18.2)\r\n",
            "Requirement already satisfied: six in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from mir_eval) (1.16.0)\r\n",
            "Requirement already satisfied: numpy>=1.7.0 in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from mir_eval) (1.19.0)\n",
            "Requirement already satisfied: deprecation<3.0.0,>=2.1.0 in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from eyed3) (2.1.0)\n",
            "Requirement already satisfied: coverage[toml]<6.0.0,>=5.3.1 in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from eyed3) (5.5)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.0.7 in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from eyed3) (1.0.7)\n",
            "Requirement already satisfied: scikit-learn in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from sklearn) (0.24.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from matplotlib) (8.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from plotly) (8.0.1)\n",
            "Requirement already satisfied: toml in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from coverage[toml]<6.0.0,>=5.3.1->eyed3) (0.10.1)\n",
            "Requirement already satisfied: packaging in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from deprecation<3.0.0,>=2.1.0->eyed3) (21.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/howard56k/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from scikit-learn->sklearn) (2.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvOsJSlpW2Zm",
        "outputId": "14c0299f-0911-42f7-cc82-1950db70a015"
      },
      "source": [
        "import pickle\n",
        "\n",
        "#listOFEverything22 = pickle.load(open(\"listOFEverything.pickle\", 'rb'))\n",
        "#listOFEverything22 = pickle.load(open(\"knn-pca-iterate.pickle\", 'rb'))\n",
        "#listOFEverything22 = pickle.load(open(\"knn-pca-iterate-20-40.pickle\", 'rb'))\n",
        "#listOFEverything22 = pickle.load(open(\"knn-pca-iterate-40-80.pickle\", 'rb'))\n",
        "listOFEverything22 = pickle.load(open(\"rf-pca-1-10.pickle\", 'rb'))\n",
        "\n",
        "\n",
        "print(len(listOFEverything22))\n",
        "print(listOFEverything22[len(listOFEverything22) - 1][0])\n",
        "print(listOFEverything22[len(listOFEverything22) - 1][1])\n",
        "print(listOFEverything22[len(listOFEverything22) - 1][3])\n",
        "\n",
        "\n",
        "rmseScores = []\n",
        "rmseScoresWLeaf = []\n",
        "for rmse in listOFEverything22:\n",
        "  for score in rmse[3]:\n",
        "    rmseScoresWLeaf.append(score)\n",
        "    rmseScores.append(score[0])\n",
        "rmseScores.sort()\n",
        "print(len(rmseScores))\n",
        "print(rmseScores)\n",
        "\n",
        "\n",
        "\n",
        "#\"\"\"\n",
        "with open(\"rf-pca-1-10.txt\", \"w\") as f:\n",
        "    f.writelines([str(line) + \"\\n\" for line in rmseScoresWLeaf])                 \n",
        "f.close()\n",
        "\n",
        "print(len(listOFEverything22[0]))\n",
        "for line in listOFEverything22[0]:\n",
        "  print(line)\n",
        "#\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
            "  from numpy.core.umath_tests import inner1d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "320\n",
            "0.1\n",
            "0.1\n",
            "[[6.660565183933263, 1], [6.600247739031484, 2], [6.700583288251026, 3], [6.652405462346653, 4], [6.727063905782251, 5], [6.702125590023791, 6], [6.754232615567865, 7], [6.77404566793297, 8], [6.844407270158729, 9], [7.749430589151941, 9], [6.660565183933263, 1], [6.651127475037755, 2], [6.7168033439626, 3], [6.806778045787925, 4], [6.832552718234007, 5], [6.88920715853189, 6], [7.01388024692735, 7], [7.101483644458355, 8], [7.173527108444594, 9], [7.8196152047297245, 9]]\n",
            "6400\n",
            "[6.383203797037977, 6.398191668899348, 6.40892855818639, 6.41405271644673, 6.419540022083339, 6.419683568861089, 6.420887331085196, 6.430701405006787, 6.439376239865391, 6.441469701214428, 6.442063206345962, 6.445522953400064, 6.445906870631107, 6.446860856400165, 6.45384749315158, 6.457268843158169, 6.457396365422196, 6.459942545730697, 6.460414504741406, 6.460465140448785, 6.460547269620539, 6.461156481406768, 6.463632715800954, 6.463632715800954, 6.465225289246925, 6.466284461651781, 6.468191022507587, 6.468964459347128, 6.470928504901436, 6.472064914544046, 6.473107512509019, 6.47422847716793, 6.474498643874792, 6.476873989587578, 6.477668894731742, 6.4780105777521655, 6.4785158893578805, 6.478930669880882, 6.479341510253518, 6.479341510253518, 6.479859239956919, 6.479915577153586, 6.479915577153586, 6.481376102738304, 6.481874564340265, 6.481911687818817, 6.481911687818817, 6.4821785529617735, 6.482398825751557, 6.4827336384828085, 6.484366074174186, 6.485462179871772, 6.485628324967332, 6.485767543105334, 6.485942890070632, 6.486944590546742, 6.486944590546742, 6.487147239974463, 6.4872305793926826, 6.4880792150824895, 6.488247733036831, 6.488420018381255, 6.489040247428409, 6.489461846187816, 6.489471215877923, 6.4896968631774845, 6.489724204340097, 6.489930614561178, 6.4900867053486895, 6.490250641509937, 6.491470415798303, 6.491470415798303, 6.491938871833895, 6.492088432893419, 6.4924724698652785, 6.492486365776037, 6.492561780427663, 6.492913381391661, 6.493070194961535, 6.493070194961535, 6.495210403172234, 6.496205923289788, 6.496205923289788, 6.496666623954586, 6.497631023778779, 6.497642746890786, 6.497649452078869, 6.49779189224269, 6.498031399846174, 6.498175850194155, 6.498175850194155, 6.498255692439767, 6.4985330930370315, 6.498701661093698, 6.498888526783675, 6.498960790848677, 6.499004407413983, 6.499212677234955, 6.49941312680018, 6.499502334158872, 6.500006754143104, 6.500006754143104, 6.500008967218343, 6.500008967218343, 6.500230696646865, 6.500411751350606, 6.500591907630425, 6.500654706383478, 6.501068673715316, 6.501068673715316, 6.501518614229042, 6.501653091334528, 6.502196407392742, 6.502485950330564, 6.502568217788861, 6.502696623937955, 6.50278801810247, 6.5034055544923675, 6.503492811264869, 6.503492811264869, 6.503523463620268, 6.503612252780066, 6.504067263350586, 6.504468700014911, 6.504758557316571, 6.504793514901067, 6.504824159845412, 6.504999269432517, 6.505217463559104, 6.505429067542229, 6.505449298533337, 6.506020051891094, 6.506141889936753, 6.50688288095894, 6.507168194812715, 6.507431411103893, 6.507519413245651, 6.507583193643621, 6.5077839069435734, 6.507960358615859, 6.508000579782365, 6.508156038410962, 6.508567977288484, 6.508567977288484, 6.509567241388013, 6.509830027508809, 6.509836166587576, 6.509889414439905, 6.510063197318363, 6.510110714524716, 6.5101683818767, 6.510216450261427, 6.510218156823655, 6.510276350555676, 6.510314017239045, 6.510619522895552, 6.51075686039309, 6.510870714443409, 6.510992504532144, 6.510992504532144, 6.5110060756325465, 6.511092112536577, 6.511096913156405, 6.511297931573982, 6.511498062340967, 6.5116932228329105, 6.511838765091344, 6.511907842860253, 6.512513216749768, 6.513370292543395, 6.513589183376975, 6.5137692995134975, 6.5137839599188005, 6.513908552315707, 6.514062748409513, 6.514159922768234, 6.514159922768234, 6.514763988517603, 6.514815124214447, 6.514969785006008, 6.515306350337027, 6.515406283700941, 6.5156625285068195, 6.515750409176778, 6.515979864804501, 6.51621322157163, 6.516624641403011, 6.516683728739593, 6.51675511090478, 6.517030926256378, 6.517122304952228, 6.517705465355664, 6.5179191810409, 6.517995262091736, 6.518151756305352, 6.518163345657964, 6.518847209862841, 6.518860205213356, 6.519066112672203, 6.519898414963997, 6.520029227138393, 6.520029227138393, 6.520049806129376, 6.52011008513374, 6.5203652781548, 6.520481270957511, 6.520660876432568, 6.520879678986879, 6.520883296303436, 6.520972479937458, 6.521596976607013, 6.521630458069115, 6.521827982661527, 6.521846519513123, 6.521902455790698, 6.52205244522964, 6.5222022531721295, 6.522370621485191, 6.522370621485191, 6.522469229309047, 6.522469229309047, 6.522608531912312, 6.522608531912312, 6.522777363368675, 6.5228173084334555, 6.522925694129696, 6.523009083456339, 6.523042697657963, 6.523064994831793, 6.523184942611997, 6.523239927324273, 6.52335164297901, 6.523548074648327, 6.523820834766654, 6.52383420002466, 6.523931528329079, 6.524053079456257, 6.524384418273994, 6.52440391191291, 6.524445455768217, 6.5245545731575625, 6.524622298337643, 6.524647228806776, 6.524905476537509, 6.525029217286819, 6.525544037129798, 6.52576330379178, 6.5259235208606, 6.526307950701507, 6.526307950701507, 6.526421080359741, 6.5264265644113735, 6.526485922659946, 6.526615809649744, 6.526711691687574, 6.526981769500907, 6.5272725102973315, 6.5273074288788075, 6.527625584926531, 6.52781756921554, 6.528010116182379, 6.528421683439837, 6.528801316851431, 6.529050986187157, 6.529066811676981, 6.52916418041058, 6.5292051611379165, 6.52933189632403, 6.5296952199523925, 6.529970300612963, 6.530519741091055, 6.53066103229346, 6.530846936418229, 6.530910776122712, 6.5311731167955385, 6.5312455328336645, 6.531250497336877, 6.5313064191537835, 6.531309109543789, 6.531368061877639, 6.531490370690052, 6.531696408718982, 6.53205029341125, 6.532182471681403, 6.532282721015657, 6.532366617852381, 6.532436603599795, 6.532579778374577, 6.532782897150741, 6.532983378257612, 6.533260084444592, 6.533289891449484, 6.533460280423708, 6.533924586819995, 6.534166665106, 6.534184476800945, 6.5342539451793975, 6.534273180793824, 6.534385252248243, 6.534498622282925, 6.53476845935677, 6.534781811945493, 6.534840961850702, 6.534840961850702, 6.534873418029246, 6.534884831102762, 6.534887253164164, 6.5348916098866, 6.535037699963198, 6.535096509909176, 6.535274833133407, 6.535358598109383, 6.53538123464495, 6.535451019224346, 6.535484063903537, 6.535612460633136, 6.5356136682745465, 6.535787941280045, 6.535787941280045, 6.535798314796121, 6.5360367751119774, 6.536273078212308, 6.536319935784275, 6.53685763656559, 6.537008747384847, 6.53721626332656, 6.537417638891799, 6.537507870533792, 6.537736814389786, 6.537921077029321, 6.537967581934155, 6.5385002290007055, 6.538567239515012, 6.538719675735507, 6.538731388637805, 6.538904281461537, 6.53892352218396, 6.53892352218396, 6.53899790524467, 6.539049579978012, 6.539110400024027, 6.539417806016658, 6.539423013319342, 6.539493919897969, 6.5395372531517015, 6.539700011020895, 6.5398204575659475, 6.539956470627585, 6.540147897111711, 6.540147897111711, 6.540380126002471, 6.540415409682448, 6.540499746113042, 6.5405511751987175, 6.540749621589234, 6.540775520950935, 6.540775520950935, 6.540952418690835, 6.540994467081522, 6.541002541469382, 6.541068639859488, 6.541145742975809, 6.541268827457629, 6.541315874287405, 6.5413405059331975, 6.5413712451142265, 6.541453036813687, 6.541618309788244, 6.541705151415483, 6.541835942000306, 6.541902216429269, 6.541911949404188, 6.541996662561837, 6.542118346761328, 6.542225269794976, 6.5425991086030475, 6.542738447440181, 6.54276172911342, 6.542865491106564, 6.542946478070138, 6.543492500544079, 6.543821974898674, 6.544081358414706, 6.544087752931981, 6.544234786743167, 6.544257104269268, 6.544424749573108, 6.544654635380866, 6.544735987673491, 6.545094374929913, 6.545233978515988, 6.545446779595611, 6.545453702428096, 6.54546143908046, 6.545558858133265, 6.545597426501723, 6.545660012779445, 6.545731203761214, 6.545796694360048, 6.545843226498894, 6.546075053252084, 6.54615787916513, 6.546542325727596, 6.546589609119214, 6.546644248415, 6.546825692485971, 6.54692160221333, 6.547077648467778, 6.547091726339806, 6.547147476231161, 6.547196453653851, 6.5472404809395455, 6.547389087638901, 6.547449567508775, 6.5475903178948345, 6.5476046113829875, 6.547742698573283, 6.547816170368335, 6.547896612006638, 6.5480316929005955, 6.548031948586754, 6.548042036298011, 6.548045559736674, 6.548196137840774, 6.548196300726043, 6.548215142764207, 6.548317271114524, 6.548490614814762, 6.54851684842198, 6.5485203428417424, 6.5485607764635345, 6.54857416092406, 6.5486836636066394, 6.548685034594853, 6.548731277719632, 6.54877123136504, 6.5487811037147186, 6.548857759684294, 6.548973594029876, 6.549129301560141, 6.549230576443322, 6.5493356810242, 6.549377825895561, 6.549386475769419, 6.549461368390232, 6.549498652759975, 6.549580924350004, 6.549646046425645, 6.549723341276042, 6.549725547648942, 6.5498875274906, 6.5498875274906, 6.550562610348757, 6.550608820773474, 6.550692605745409, 6.5509306653098, 6.551058571345881, 6.5512542894495205, 6.55175177853797, 6.551772717103397, 6.551939056435442, 6.552006134249769, 6.552210310363208, 6.552263134780679, 6.5524259987048135, 6.55242732681649, 6.5524297597859755, 6.552681481079556, 6.552915844580803, 6.553192467257881, 6.5533330906925515, 6.553339041065433, 6.553353705079264, 6.553590084900514, 6.553667893600722, 6.553677099668654, 6.5540836248983325, 6.554087886271546, 6.554087886271546, 6.554115011784396, 6.554200430148184, 6.5544016675749806, 6.554459355030762, 6.554465618079558, 6.554662793020643, 6.554667424840854, 6.554675496848967, 6.555070627580461, 6.555079315765357, 6.555178181157407, 6.555222167237149, 6.555247063357448, 6.55526941825587, 6.555460328295509, 6.555512763723214, 6.555826357416266, 6.556083614891089, 6.556353577122973, 6.556353577122973, 6.556388536542154, 6.556388536542154, 6.556436960846171, 6.556959794752508, 6.557107350956687, 6.55718131284632, 6.557218085921449, 6.557387190117725, 6.557616345913908, 6.557733214538446, 6.557819461225562, 6.557939875811829, 6.558233271401667, 6.558375361103678, 6.558425376500327, 6.558605391324904, 6.558708614549266, 6.558913325698101, 6.558952336695189, 6.558971651722984, 6.559225586357977, 6.559255430591388, 6.559318868421691, 6.559332858217326, 6.559668211476992, 6.559756484664845, 6.5597766536451685, 6.559815087456709, 6.559876655132075, 6.559882507027473, 6.559907901525768, 6.560016741461817, 6.560079529590349, 6.560236146589052, 6.560412061719484, 6.560464929865267, 6.560485080189903, 6.560551370502507, 6.560809634341745, 6.561240009372526, 6.561240009372526, 6.561577478698306, 6.561596703987148, 6.56184431972456, 6.56184431972456, 6.562008798172392, 6.5620288016754795, 6.562038634847157, 6.562082641401693, 6.562277512386248, 6.562403544538322, 6.562423482224612, 6.562425027654095, 6.562425027654095, 6.562654367984482, 6.562684053954235, 6.562762872293214, 6.562917582791849, 6.563084849749363, 6.563211696786992, 6.563235503367029, 6.563284625559651, 6.5634729653298765, 6.563508444130789, 6.563508543376096, 6.563770987655939, 6.563789734617777, 6.563995598873804, 6.564165446699438, 6.56417553883258, 6.564373180987499, 6.564376375034722, 6.5644103700678995, 6.564438967343169, 6.564570409965737, 6.564702225034499, 6.564702225034499, 6.564717061217593, 6.564910762080783, 6.565128580047027, 6.565236037019502, 6.565382502556359, 6.565469295213878, 6.56551179877488, 6.565894098994532, 6.566013305683903, 6.566178053791289, 6.566184336116205, 6.5662023843138355, 6.5662023843138355, 6.566244064161088, 6.5662694106723825, 6.566521380059024, 6.566563523159438, 6.566712310432904, 6.566827999781447, 6.566847994492331, 6.566992775128946, 6.567027108856571, 6.567284985705141, 6.567290781961062, 6.567374070726314, 6.567579371760658, 6.567770415335744, 6.567932949083672, 6.567933853095121, 6.567954864215784, 6.567972391545865, 6.568259622171739, 6.568274860458698, 6.568378839522708, 6.568476599254747, 6.568536690674546, 6.568633762813002, 6.568642373891968, 6.568642714591178, 6.568647382767294, 6.568766205612485, 6.5690646036784095, 6.569076552662376, 6.569253490462606, 6.5693392696137005, 6.569623529041222, 6.569726079966978, 6.570046281982105, 6.570159549694195, 6.570202680342788, 6.57020491091474, 6.570282227505584, 6.570300566338709, 6.570431233679498, 6.57088162697604, 6.57104009584393, 6.5711595806226795, 6.571190429243679, 6.571223533195808, 6.571812064861654, 6.5718431530017805, 6.572132394864959, 6.572349722938513, 6.572376703611946, 6.5724589401289055, 6.572768171353683, 6.5729475588338, 6.5729695002808555, 6.572974501291431, 6.573289688980915, 6.573437963051464, 6.573587525339091, 6.573774208733562, 6.573818721329672, 6.573980443545269, 6.573995389083062, 6.574020774398695, 6.574028052922872, 6.5740461694856345, 6.574087587777798, 6.574196103114183, 6.574719553379029, 6.5749121589462645, 6.575054111804855, 6.575071724821886, 6.575098448618229, 6.57514907182619, 6.575255862320396, 6.575283716638651, 6.575441480849071, 6.57553841205956, 6.57553841205956, 6.575790931820835, 6.5758381735696645, 6.575961026613294, 6.5760343925531375, 6.576328730319564, 6.576361830693567, 6.576486411159044, 6.576753020052211, 6.57681916256308, 6.576821447317809, 6.577165765395873, 6.577246248721543, 6.577355242803507, 6.577546033780762, 6.577618748800445, 6.577668925961828, 6.5777134697272475, 6.577888797901081, 6.577928032185227, 6.578056132739154, 6.578215001554484, 6.578215001554484, 6.578314651507061, 6.578749981583625, 6.578752180901438, 6.578840170347339, 6.5790878221223625, 6.579193761300243, 6.579347016921345, 6.579397895013177, 6.579412266804255, 6.579492306476785, 6.579766238204666, 6.579862015775184, 6.580126189353496, 6.580126189353496, 6.580309966854234, 6.580358622551962, 6.580407529241306, 6.580487253010162, 6.580505335795866, 6.580572610693586, 6.580603692797645, 6.580626720879908, 6.580810787957948, 6.580948198649391, 6.580970386844898, 6.580980138303486, 6.581006406806692, 6.581152556327239, 6.581330456189791, 6.581447939703563, 6.581536224016167, 6.581536224016167, 6.5816678967218625, 6.58190864510579, 6.5820377951576425, 6.582295605874714, 6.582331306416461, 6.582980944775279, 6.582986637276442, 6.583042226875845, 6.583112479023142, 6.583116491597779, 6.583143671242699, 6.583260182213699, 6.583431009774224, 6.583431009774224, 6.583434589635954, 6.583986837904348, 6.58406902664582, 6.584157140051973, 6.584163103713486, 6.584182510597964, 6.584238152076929, 6.584383248922103, 6.584472873069395, 6.584487023920912, 6.58480539374359, 6.584978077048179, 6.58516307853256, 6.585290845100395, 6.5857079681790855, 6.585867921925729, 6.586305209354851, 6.5866527714712415, 6.5870582207620085, 6.5870582207620085, 6.587064681242078, 6.587072630973655, 6.587084966906795, 6.5871224570511, 6.587305479967747, 6.587384470089245, 6.5874916306759035, 6.5878312455518016, 6.587977465756992, 6.5879956963312525, 6.588022212187636, 6.588123757320325, 6.588167089835305, 6.588316297404038, 6.588354026451797, 6.588419472323932, 6.588430130302064, 6.5885437348199405, 6.588581764690391, 6.588748522373897, 6.588827519496859, 6.588898582255608, 6.588899641910934, 6.589231048342228, 6.589377058540617, 6.589444877301156, 6.589769927594402, 6.589788499502933, 6.589871683550479, 6.589924746458503, 6.5899923934566305, 6.5902760416982815, 6.590313872135509, 6.5906813122185435, 6.5906957648650835, 6.5907579322124095, 6.590982929906547, 6.591051423973284, 6.591103011471381, 6.591374354499553, 6.591375952186127, 6.591409687413877, 6.591598924766051, 6.592283908122451, 6.592346545920449, 6.592387822780626, 6.592393697371652, 6.592433322367324, 6.592514340052154, 6.592525262090107, 6.592637356307395, 6.5927281299121825, 6.592759841408702, 6.592806164698317, 6.592846397481877, 6.592888659208586, 6.593094591040157, 6.593179732135089, 6.593231135801423, 6.593406784915679, 6.593417478072522, 6.593483116424985, 6.593513085274157, 6.593543504691664, 6.593615387698077, 6.5937090385139125, 6.593782974725706, 6.59387582305313, 6.594143292275619, 6.5941756260454465, 6.594221850743682, 6.5945422632778925, 6.594899036165031, 6.5949309820904, 6.595176281898796, 6.595316618878098, 6.5953372922011955, 6.595568979808595, 6.595602396384916, 6.595766302028586, 6.595766302028586, 6.595776257397793, 6.595878530351676, 6.595970348299678, 6.596241929586224, 6.5964608683214925, 6.596614395899899, 6.59662738344788, 6.596633788282773, 6.596636863528681, 6.597273133810225, 6.597281878075838, 6.5973464068069605, 6.597415652691929, 6.597436510879209, 6.597489626001675, 6.59756333338183, 6.597834229870053, 6.597843294003021, 6.59794504410612, 6.597994206467153, 6.598042185000684, 6.598151470085129, 6.598341393526636, 6.598464598807784, 6.598472464972658, 6.598502140240056, 6.598563717103837, 6.598567692042595, 6.598762624554755, 6.5989156848244495, 6.598959981569325, 6.598974742178831, 6.59943052994354, 6.599771472599498, 6.599834737867728, 6.600023952004083, 6.600042032833364, 6.600052082653477, 6.600133140437398, 6.600247739031484, 6.600308264917373, 6.600340845407591, 6.6004069070340705, 6.600476351665579, 6.600673246288367, 6.600732208633992, 6.600777218704612, 6.6007965167426965, 6.6008907931464735, 6.601036555988524, 6.601135394145624, 6.601181151559432, 6.601276449531456, 6.601329148993904, 6.601377927644718, 6.601446002712686, 6.601524918342447, 6.601548802664587, 6.601593197278261, 6.601667904689387, 6.601723584434637, 6.601773727526873, 6.601784653684082, 6.601864577496835, 6.602270221367649, 6.602291666547046, 6.602322821820369, 6.602375228614647, 6.602441774974414, 6.602464364307637, 6.602502204585613, 6.602502204585613, 6.602557000737861, 6.6025830673822465, 6.602712220244236, 6.602831379112198, 6.603046773332615, 6.603054710627186, 6.603251424722645, 6.603251424722645, 6.6036682280420465, 6.60377601154023, 6.603911025865376, 6.60409422516704, 6.6042590734500255, 6.604353233292261, 6.604358653354623, 6.605042457859789, 6.605057858529146, 6.60507749894928, 6.60516314983041, 6.605178411980921, 6.60524167800767, 6.605464926039506, 6.605633716530188, 6.60567167242676, 6.605831853865331, 6.6060671322189135, 6.606103676164686, 6.606132078521874, 6.606168512172259, 6.606233436632175, 6.606308682048859, 6.6063249033190425, 6.6069394645358175, 6.606991496342139, 6.607051585217136, 6.607433374041123, 6.607449781137685, 6.607492666886043, 6.607630942822882, 6.607935307309635, 6.6080929017873125, 6.6083165245745095, 6.6083373458758174, 6.608343097862548, 6.608620229513713, 6.608645757494107, 6.608695639758975, 6.6087552663376545, 6.6090438367642195, 6.609086527868834, 6.609106711437877, 6.60921354219524, 6.609220305075726, 6.609304443663423, 6.609495665775561, 6.609714930486536, 6.609761433815975, 6.609786765687689, 6.609826992756112, 6.6099562037974735, 6.609957388770872, 6.609968971846183, 6.610045420970396, 6.6103157690152825, 6.610493549960075, 6.610548667797951, 6.610591186457634, 6.610687213068119, 6.611176190378465, 6.611190591298768, 6.611263313765488, 6.611273809048156, 6.611289398656235, 6.611333014238598, 6.611341036823138, 6.611351151004095, 6.611451912119681, 6.611685727719874, 6.611736703934109, 6.611757622350602, 6.611988745474154, 6.612042455329037, 6.61215357696662, 6.612175117492151, 6.612278127363082, 6.612498034496529, 6.612623845696312, 6.612651083171328, 6.612734630910835, 6.6127875508675364, 6.612966523832926, 6.613021597777021, 6.613316293888849, 6.61354690588698, 6.61360617992895, 6.61360617992895, 6.6136856064502485, 6.613685803077723, 6.613685803077723, 6.6137031640393475, 6.613912076346121, 6.6141320899671925, 6.61444417876557, 6.614448142861863, 6.614565573868874, 6.6147051637932615, 6.614784556322675, 6.614807184565728, 6.6151108223044535, 6.615162543511722, 6.615472291264956, 6.615529894655568, 6.615557027561166, 6.615568687250074, 6.615794332260566, 6.615859024343695, 6.6159275131366595, 6.616051547630138, 6.616056429332201, 6.616111061674604, 6.616130352958824, 6.616351302355774, 6.616415827697179, 6.616494665679381, 6.616580266641658, 6.6168861675875394, 6.616970730982382, 6.6169973625855825, 6.617118186476055, 6.617208428464754, 6.617310363967086, 6.617414173960643, 6.617439126248746, 6.617540761055555, 6.617559224126245, 6.617618203775609, 6.617882127901985, 6.618021776751675, 6.618030355788936, 6.6181957231795145, 6.618338345293116, 6.618405354730476, 6.6184774162818085, 6.618615864525559, 6.618675871202188, 6.6187990290297085, 6.6189856382958565, 6.619090317035292, 6.61931761337252, 6.619372430336328, 6.619414361104336, 6.619522255203169, 6.619807973283557, 6.61981803228262, 6.619833513317104, 6.619893926854992, 6.619931884594046, 6.6202139450046, 6.620304878816421, 6.620669899905267, 6.620813431613327, 6.6208677462064, 6.620914676945277, 6.620920692772372, 6.620939348526302, 6.621094099247184, 6.621144039763219, 6.621299384393326, 6.621661223831897, 6.622065193010478, 6.622134113110612, 6.622167874818491, 6.62266384277978, 6.622693515516853, 6.622718084428598, 6.622720155187728, 6.622914641959118, 6.623131242314541, 6.623215565449044, 6.623231737109594, 6.623413373887687, 6.623496374077352, 6.623844061464691, 6.623847977441501, 6.623992265418353, 6.62401560572189, 6.624203877246701, 6.624294048781029, 6.624312081025506, 6.624346304955611, 6.624484486836504, 6.624537061761392, 6.625046662185713, 6.625063236462318, 6.62508671166294, 6.6251240805580425, 6.625267772427195, 6.625308343118567, 6.625326919748454, 6.625482384855518, 6.62548465515304, 6.62551050323866, 6.625523393919105, 6.6255884083274434, 6.625650297406418, 6.625749954737404, 6.625926646052243, 6.626047936169303, 6.626068102798116, 6.6261267796815595, 6.626392869040319, 6.626412469043463, 6.626476967121733, 6.626626269464966, 6.626684954080921, 6.626714418820146, 6.626747834663665, 6.626758685243884, 6.62689734269071, 6.627198501959937, 6.627314514116186, 6.62737654591515, 6.627612573982155, 6.627858384594231, 6.627993096102018, 6.6280404125112975, 6.628120397647113, 6.628170287650636, 6.628304296941615, 6.6283182741633055, 6.628379535207352, 6.628468696130163, 6.628871342198859, 6.628872920541545, 6.6289998497550355, 6.629067419934077, 6.62938871825863, 6.62954468519764, 6.629708023575914, 6.629727584816013, 6.630002383014155, 6.630133620796969, 6.630483805243001, 6.6306339034619945, 6.630639441820997, 6.630830030644127, 6.630977283037133, 6.631081193034653, 6.6311618220812445, 6.63137678372272, 6.631406656198082, 6.63151224661572, 6.631517916932005, 6.631740610672841, 6.631827770037118, 6.631846869707318, 6.632042172429963, 6.632375533391994, 6.632553142950723, 6.632725498799415, 6.632962942839886, 6.63315610024862, 6.6333039300957335, 6.633468211168721, 6.633558937239793, 6.633635320743585, 6.633680285090768, 6.633794619566344, 6.63383355910712, 6.6338432809121155, 6.6338432809121155, 6.633979167692916, 6.634082903949092, 6.634085816841079, 6.634096346099972, 6.634274563230458, 6.634445316314781, 6.634456456608117, 6.6346774432851765, 6.634726304899478, 6.635066815993548, 6.635189835323182, 6.635695622663232, 6.635792563167086, 6.635792563167086, 6.635817832493687, 6.635819297120706, 6.6360302163463905, 6.636221168343757, 6.636252894499595, 6.636362238294425, 6.636380783240587, 6.636418517766336, 6.63672666399276, 6.636731514955197, 6.636858635385347, 6.636980406395037, 6.637150710801272, 6.637159229787801, 6.637531719909603, 6.63756635088673, 6.637756976968184, 6.637853295352967, 6.637871012174506, 6.637871012174506, 6.637944521742203, 6.637958924121598, 6.638149387932751, 6.6383133741737534, 6.638365649767223, 6.638677875337125, 6.638677997952711, 6.6388051186643935, 6.638967995655868, 6.639036500471876, 6.63930093216811, 6.639429266413806, 6.639578629557147, 6.640083393848419, 6.64010939677722, 6.640230350740606, 6.640272479284699, 6.640985726768397, 6.641016318307981, 6.6411840110751745, 6.641475022093538, 6.641475022093538, 6.64151752261443, 6.641703806212552, 6.6418883312065295, 6.642024483819514, 6.64223252663471, 6.6422653081376195, 6.642276236674272, 6.642333130483108, 6.642398569928792, 6.642399775525961, 6.642399865146091, 6.642696905539317, 6.642707449838317, 6.642737760970139, 6.6427488259231335, 6.642785552478355, 6.642865720407678, 6.642917020162966, 6.64320308776019, 6.6433358527360244, 6.6433713745105765, 6.643396832226448, 6.643465296611655, 6.643664978086569, 6.643832140269989, 6.644083076052046, 6.644208665390526, 6.64423012113697, 6.644402546071884, 6.644538170943232, 6.644558686653974, 6.644774942361923, 6.644875053304652, 6.644950201243888, 6.6449676543372425, 6.645083115179618, 6.6451147545117095, 6.645202031079175, 6.645290062864959, 6.645394313025876, 6.645580558342178, 6.6458231811733945, 6.645878988645051, 6.646074589977208, 6.646097196063838, 6.646106066059099, 6.646162961593614, 6.646296406636664, 6.646454957469476, 6.646665495084711, 6.646794231804021, 6.646999428161903, 6.647063145353811, 6.647074576518982, 6.647244955943213, 6.647546955772538, 6.647570070742289, 6.647690628951725, 6.64785847228458, 6.648231714723062, 6.648504757119595, 6.648677198320808, 6.648811116864398, 6.6488474840274865, 6.64884806877175, 6.64900232692342, 6.6490836703486575, 6.6490836703486575, 6.649108171810398, 6.649246050664846, 6.64937185937892, 6.649387996899052, 6.6495325668338126, 6.649996590175558, 6.650073626336376, 6.650118300236035, 6.650283531732347, 6.65039566446034, 6.650431475745009, 6.650509137082413, 6.650514676976057, 6.6506586130855085, 6.650754682679873, 6.650786562280641, 6.650939292087472, 6.651002092331358, 6.651028371058756, 6.651127475037755, 6.651137203871801, 6.651199854445095, 6.65122087740783, 6.651257761704968, 6.651386329949035, 6.651424535010122, 6.651434376930541, 6.651474656609706, 6.651504059139924, 6.6515065949369525, 6.651535094187208, 6.651595234681891, 6.651660101916635, 6.65169318082937, 6.6518374961326785, 6.651866065709019, 6.651989662799035, 6.6520441960988, 6.652109869938349, 6.652121645132693, 6.652303932042367, 6.652405462346653, 6.652553207355187, 6.652573130292017, 6.652648220505171, 6.652777220806922, 6.6528480380748185, 6.653317916504258, 6.653392612296092, 6.65343658708432, 6.653502741414746, 6.653505388276465, 6.653633174905682, 6.653668154222887, 6.653708239685659, 6.653712279468984, 6.653913947501893, 6.653921150569893, 6.654304443714133, 6.6546601892191966, 6.654684296532701, 6.6547707884890315, 6.654773468428768, 6.6548550102766875, 6.65500687356097, 6.655112778962212, 6.655265030646431, 6.65565184907816, 6.655752270539467, 6.655760309008226, 6.655803795228439, 6.65592746950714, 6.656080055329021, 6.656167015956016, 6.656444689271899, 6.656595910894439, 6.6565971071712005, 6.656629134781565, 6.656648422445185, 6.656923791502031, 6.657100188809119, 6.657179123215949, 6.657633126787878, 6.657688486474651, 6.657725859830444, 6.657747116431045, 6.657759229424649, 6.657791190397969, 6.6579052411516315, 6.657913591014319, 6.658092861594731, 6.6581045541229, 6.6581283731327, 6.658176208289234, 6.658241440830809, 6.658395985940487, 6.658442299467211, 6.658451055583862, 6.658455419003538, 6.658460147424303, 6.6584610535009565, 6.658604049292727, 6.658775445791785, 6.659165781517982, 6.659186345379692, 6.659302819818278, 6.659310534443615, 6.659351847490556, 6.659440540717124, 6.659482235222159, 6.659598379355561, 6.659608368018014, 6.6597944369303335, 6.659848284603064, 6.659889143275706, 6.660152888427249, 6.660290223360593, 6.660356107568326, 6.660419716950744, 6.660541382779156, 6.660565183933263, 6.660565183933263, 6.660565960633854, 6.660656208643439, 6.660704972737897, 6.660840225642121, 6.660895847917508, 6.660987644885596, 6.661333775471077, 6.66136380095112, 6.661386285189313, 6.661485612451899, 6.661537432380465, 6.66159032459677, 6.661633558182701, 6.661701601018122, 6.661724799021801, 6.662183058959115, 6.6621920139415245, 6.662420394528205, 6.662850681857992, 6.663044439930581, 6.663312961957785, 6.6633358229204855, 6.663467711348851, 6.6635618326330714, 6.6636227919698685, 6.66375431846528, 6.663958531812072, 6.664000166328452, 6.6640257060896255, 6.664143525728007, 6.664266180174917, 6.664575857245824, 6.664855731139956, 6.664912290449912, 6.665043253777811, 6.665213589304262, 6.665371456956365, 6.6657279268217735, 6.665747087461795, 6.666009002133885, 6.666019923723978, 6.666035426749046, 6.666127518958495, 6.6662367624512004, 6.666352753463501, 6.666514799041979, 6.666523501551595, 6.666747657012682, 6.666768171183363, 6.666792845838605, 6.666950167027907, 6.667052171843942, 6.66709600209448, 6.6671129997514536, 6.667117653222674, 6.667154752946328, 6.667260512205845, 6.667385096042986, 6.667406636642516, 6.667428286174421, 6.6674675124840475, 6.667558738129685, 6.667633957309699, 6.667645259927992, 6.667769665177407, 6.667829235159342, 6.668090608928143, 6.668289149349996, 6.668517059551717, 6.668525388194459, 6.668579048184925, 6.668638584397781, 6.668808658964348, 6.66886577857906, 6.668985993815691, 6.6692943579553585, 6.669415399955433, 6.6696012646055305, 6.669705229827119, 6.670222697305207, 6.67026566675442, 6.670279748082365, 6.670563046662329, 6.670693004652259, 6.670695936559026, 6.6707339736020135, 6.670819977585131, 6.670878037151503, 6.670982585112514, 6.671072509077925, 6.671135794868669, 6.6711874234503705, 6.671609977996264, 6.671733497408905, 6.6717371998824095, 6.672080955657758, 6.672173163017367, 6.67230281359339, 6.672545307433789, 6.672577338380934, 6.67258678699659, 6.672627504909338, 6.672627504909338, 6.672697366524972, 6.672796290891417, 6.672855268738601, 6.672877086027033, 6.672992630091782, 6.6731155419693575, 6.673130571492007, 6.673134262430709, 6.673223560931648, 6.673262100099443, 6.673688756669162, 6.673751938540903, 6.673751938540903, 6.673871756555273, 6.674371108511504, 6.674429962647366, 6.6744527489627705, 6.674515595571693, 6.6746031538433055, 6.6746031538433055, 6.674667413354574, 6.674884268404619, 6.674945310437196, 6.675067961978717, 6.675228638193491, 6.675382019010564, 6.675382019010564, 6.675420501357906, 6.675537711065878, 6.675554850155045, 6.6756251499710935, 6.675772809380146, 6.67594733018749, 6.675959976851315, 6.676364687864611, 6.676414834075631, 6.676414951262381, 6.676434499995828, 6.676622312495444, 6.676640096364388, 6.676948695716622, 6.676969549194491, 6.677117025658565, 6.677120988932836, 6.677316040247386, 6.67732261344521, 6.6773872628004725, 6.677595721228708, 6.677792589504225, 6.678105432438408, 6.678145187482009, 6.678433720769029, 6.678558706965016, 6.678694229379238, 6.6788164697642065, 6.678905252837649, 6.678956927253664, 6.67909364691639, 6.679129605952129, 6.679148927185333, 6.679395392955741, 6.679828893498566, 6.680018286605844, 6.680099883326916, 6.680219273584217, 6.680345047371381, 6.680560963474024, 6.680695923587555, 6.680730364981383, 6.680975954288769, 6.681027495395498, 6.681122336373114, 6.681194077522714, 6.681318463406003, 6.681321107838747, 6.681461158413377, 6.681503649312476, 6.681926451589788, 6.68205406091319, 6.682067292591692, 6.682189817510989, 6.682227622730292, 6.682810419127737, 6.682921495798851, 6.6829842147960274, 6.6831487870500625, 6.683150764460671, 6.683179297207688, 6.683213871479816, 6.6833951742337065, 6.683588911251588, 6.683660697303968, 6.684226837489399, 6.6842572180753015, 6.684319012871145, 6.684392124184048, 6.68479962953182, 6.684938232835482, 6.684952126959867, 6.6850572549449865, 6.6853077861261765, 6.685397818078012, 6.685397818078012, 6.685547634587603, 6.685646650816488, 6.685675338436231, 6.685908978221494, 6.6860329750500105, 6.686064322460485, 6.686115507497634, 6.686145659260153, 6.6861875775558275, 6.686188677019977, 6.68631028424487, 6.686411786158079, 6.68646860563174, 6.686745537938987, 6.686872113481781, 6.686956796430839, 6.687252930743811, 6.687439066631861, 6.687862546029615, 6.687902391441661, 6.68803886376889, 6.688101458067844, 6.68842091353319, 6.688500499525527, 6.688570563790488, 6.68857947717597, 6.688766247954797, 6.688907088934913, 6.6890655267640176, 6.689158688841172, 6.689205230191596, 6.6892110206413005, 6.689300091792006, 6.689359860822195, 6.689732342221743, 6.689745168365881, 6.689934313629667, 6.690273176021065, 6.690386751866305, 6.690423726108072, 6.690527616087487, 6.690566305125618, 6.6907646948449395, 6.690773800961833, 6.690774098369978, 6.690922460489709, 6.690941812920101, 6.691168610817209, 6.69126739954532, 6.691372286787981, 6.691393497514769, 6.6914286872584885, 6.691431132637411, 6.691493433193247, 6.691571408802036, 6.691961615571699, 6.692085492279951, 6.692244519621195, 6.692291092437707, 6.692366881359664, 6.6924232103638515, 6.692590451158556, 6.692596864468465, 6.692679131526302, 6.692711739675203, 6.692757698576947, 6.6928809047455005, 6.6928816613831374, 6.692954724515785, 6.693222150107987, 6.693261618127517, 6.693695286129645, 6.69378383174017, 6.693839703580076, 6.693904847555221, 6.6940491830636795, 6.694052609310591, 6.694411089707737, 6.694422693332078, 6.694746484571917, 6.694780805811862, 6.6948047367782575, 6.694818109032805, 6.694818575282121, 6.694936222054425, 6.6949438755786295, 6.695013981801751, 6.69517528336934, 6.695272382616568, 6.6952865317206935, 6.6953112572184805, 6.695460866442894, 6.6954730339114965, 6.695509824495726, 6.695711756886039, 6.695863855952446, 6.695918526953919, 6.696046034076836, 6.696056267167675, 6.696068621540081, 6.696168883387316, 6.696172022382766, 6.696172022382766, 6.696623514536814, 6.696625245911087, 6.696637761983179, 6.696740117687947, 6.69681485195943, 6.696826196353095, 6.696952555302766, 6.697042133369971, 6.697264766945775, 6.697341363046625, 6.697383283927931, 6.697396547227972, 6.6973980367135315, 6.6974016613930205, 6.697411833621048, 6.697463017432514, 6.697613468333942, 6.697754093197938, 6.697808651753065, 6.697830699010949, 6.697888273321005, 6.698091061792899, 6.698225736958582, 6.6982739279207495, 6.6982739279207495, 6.698475659964138, 6.6985005003485485, 6.698659843736961, 6.698665054644274, 6.6987210783812285, 6.69872952250437, 6.6987919526725825, 6.699022716949572, 6.69910255708746, 6.69923837870766, 6.699246207091062, 6.6994827545760005, 6.699536160405766, 6.6996074346708765, 6.700002103553178, 6.70001000263374, 6.700089021340195, 6.700176795666697, 6.700529454271033, 6.700583288251026, 6.700643750053161, 6.700741855462572, 6.700916084384156, 6.700929000621357, 6.700929000621357, 6.701456315517015, 6.701726337716144, 6.701993617208577, 6.702125590023791, 6.702215672677635, 6.702422546250322, 6.702511493880637, 6.702578441491311, 6.702646366616688, 6.702669342226927, 6.702669757676833, 6.70272084834387, 6.702731172091651, 6.702880845539334, 6.702891290371333, 6.702938030724977, 6.70312794348894, 6.703285447644096, 6.703336109695703, 6.703377775157803, 6.7034486265799345, 6.703528328444494, 6.703563229616845, 6.703582259721128, 6.703604577602958, 6.7036488399831535, 6.703792690314544, 6.703855083714925, 6.704052782851001, 6.7040833484497835, 6.70410431986553, 6.70427948337975, 6.70432873233469, 6.7044187296794355, 6.704581044044095, 6.7047370642487625, 6.704759256048534, 6.704911877378263, 6.704918532126109, 6.705068168263037, 6.705121988720122, 6.705271252486378, 6.705307206184619, 6.705338110718493, 6.705351467382988, 6.705404856658395, 6.705469776798404, 6.705755594326106, 6.706140870145863, 6.7061540562152855, 6.706263281780039, 6.706509464304255, 6.706680617756362, 6.70693701249707, 6.707029461410165, 6.707068837730686, 6.707217653950822, 6.707337368038593, 6.707352637008416, 6.707459744153655, 6.7075658693172215, 6.707628411384682, 6.707785851206052, 6.707926324033285, 6.708039458007142, 6.708056177280152, 6.70822889132122, 6.70824356260563, 6.708330573969743, 6.708394425575576, 6.708498631955102, 6.70850146249974, 6.708674291456506, 6.708941316682458, 6.709050873947298, 6.709057804517879, 6.709120418739498, 6.70914072914674, 6.7092661319461815, 6.709375548111902, 6.7094078584239085, 6.70954299573507, 6.709612667485271, 6.709806185390264, 6.709822760667096, 6.709997512023769, 6.710062977358306, 6.710358101604009, 6.710466441238775, 6.710612217366394, 6.710906358901625, 6.711073064150641, 6.711124394062901, 6.711780097558436, 6.712111889540643, 6.712306674793122, 6.712514916295769, 6.712599632988765, 6.712676052541448, 6.712888136260501, 6.713073104146268, 6.713233755098173, 6.713320408884863, 6.713327996188454, 6.713522710914079, 6.7135430129097795, 6.713548602710114, 6.713723366918837, 6.71391146707879, 6.714028413685877, 6.7140708960546664, 6.714455157030618, 6.714737426277153, 6.7147686532064315, 6.714845538010354, 6.714863544677606, 6.714867925735505, 6.714939263429742, 6.715348153464219, 6.715484424422812, 6.7155018249838845, 6.715572940522478, 6.715635941612767, 6.715772937539496, 6.715849186659986, 6.715849186659986, 6.715916857893137, 6.716016131214362, 6.7160238903290495, 6.716055398829734, 6.716261717098338, 6.716285065897514, 6.716488610954908, 6.716632589261375, 6.7168033439626, 6.716813360183127, 6.716820656249796, 6.716881412155466, 6.716889288691107, 6.717030244250506, 6.7170608772537825, 6.717123294716006, 6.7171470588040165, 6.7171908409338394, 6.717529351087381, 6.717616497512866, 6.71778179932593, 6.717869579698537, 6.717938400487058, 6.717964639828857, 6.718510885161848, 6.718736813364171, 6.71874005532047, 6.719022864665883, 6.719067339192489, 6.71911512097832, 6.719232978415762, 6.719239780401655, 6.71928116968241, 6.719293609400157, 6.719298906425276, 6.7193166465587035, 6.7194640237774514, 6.719555757482087, 6.719588453264675, 6.7195980284534285, 6.719685492075657, 6.719685492075657, 6.7199956699574495, 6.720067346494636, 6.720087475337476, 6.7203505641962344, 6.7204581115225785, 6.720517094777808, 6.720688957375023, 6.720717576773994, 6.720751382646197, 6.720765388698956, 6.7208806215252, 6.720926513252288, 6.72103480281142, 6.721319943108153, 6.721442950657746, 6.721462765101135, 6.721501092066435, 6.721520048149664, 6.72152114248806, 6.721547109098918, 6.721722767320542, 6.721743737489817, 6.721862867362426, 6.7218765874193735, 6.722003210795982, 6.722372681826195, 6.722377932812856, 6.722605848814762, 6.722611185214386, 6.72273392524184, 6.72319698946887, 6.723270421486307, 6.7233520462557905, 6.72341465353381, 6.72341465353381, 6.723437180112194, 6.723437534447311, 6.723517170626924, 6.723567476465019, 6.723580037361037, 6.723609052979996, 6.723726878412608, 6.724044647638284, 6.724049635423458, 6.724117883702274, 6.724262987337237, 6.724278427062633, 6.724384567727979, 6.72457499500022, 6.724687395747706, 6.724827499682532, 6.724986190943495, 6.72507793808496, 6.725192571465027, 6.725208252741188, 6.7252095243014764, 6.7252095243014764, 6.725330764081915, 6.725576676599172, 6.725791846184893, 6.725896215193834, 6.725981011915815, 6.726032737233174, 6.726219750029186, 6.726231876230139, 6.726320740869517, 6.726677280127413, 6.726902977006727, 6.726939565685594, 6.727063905782251, 6.727138189573314, 6.727254597549549, 6.7276382162988675, 6.727660759394369, 6.727720772228544, 6.727910159646464, 6.727912763280433, 6.727931270640199, 6.7279312835319, 6.728038522859087, 6.728127075198919, 6.7281711278630745, 6.72841254461264, 6.728569371848523, 6.728591904309666, 6.7287026732665245, 6.7288502829837, 6.729048591270788, 6.729073592509403, 6.729107852813292, 6.7291682593903595, 6.7291963152830006, 6.729226442869995, 6.7293781597160605, 6.729750287193602, 6.72978589984666, 6.729926038202135, 6.730289781062211, 6.730329641847056, 6.7304487616257225, 6.730584180470493, 6.731438084980493, 6.73147387558919, 6.731563891678367, 6.731666970052517, 6.731698194943014, 6.731784071655747, 6.731924073107415, 6.731924073107415, 6.731951050991873, 6.731951050991873, 6.732150703476679, 6.732406589543215, 6.732461281643275, 6.732592114651061, 6.732648047172599, 6.732787525494361, 6.732801304025273, 6.732858941232398, 6.732878069168004, 6.73310276568224, 6.733111820169793, 6.733276761346669, 6.733607736119502, 6.73360822866632, 6.73410485441262, 6.734159072884838, 6.734541352950924, 6.73470920470167, 6.734720163825895, 6.734799398246781, 6.73480796513868, 6.7348166875164726, 6.73492174737968, 6.734969901937579, 6.735064183367248, 6.735097644248884, 6.735312128463044, 6.73531387758783, 6.735449989835495, 6.735466249270881, 6.7354809105775795, 6.735573476133734, 6.735576280752197, 6.735791198644585, 6.735917259397177, 6.735947299779367, 6.736118929407813, 6.736168310595066, 6.736345995241218, 6.736384155612739, 6.73663358313194, 6.736671860152597, 6.736715914159764, 6.736715914159764, 6.736795425291146, 6.736813739658569, 6.736939150434441, 6.73696456029275, 6.737237720733448, 6.737346110583337, 6.737364670710292, 6.73744868056108, 6.737450987685598, 6.737564919957095, 6.737616261775931, 6.737718407111901, 6.7383364875000895, 6.738346632869802, 6.73853039399166, 6.738721300048052, 6.738740997294059, 6.738838761273684, 6.738999670687258, 6.739162514666611, 6.739162514666611, 6.739177558414133, 6.739214190120863, 6.739229345806583, 6.7400318781020205, 6.7400938392455885, 6.740243940387302, 6.740425167007259, 6.740426955467717, 6.740607738892448, 6.740647074046238, 6.7406648340896655, 6.7406648340896655, 6.740738139624283, 6.740788159670608, 6.740788159670608, 6.741030702311813, 6.741065766262797, 6.741065766262797, 6.741221484912269, 6.741246639645339, 6.741329530315222, 6.741493123173315, 6.74150327698995, 6.741519733158906, 6.7416821716543085, 6.741784116545179, 6.741822435515719, 6.742035652579373, 6.742146385859407, 6.742196246124046, 6.742326162590462, 6.742422524966602, 6.742451006286087, 6.7427662709408684, 6.742854660501282, 6.742891556104695, 6.743008009551325, 6.743008009551325, 6.743027275677232, 6.743220101666243, 6.743303503635437, 6.743342837887109, 6.743751035211482, 6.743872779911043, 6.744082539558517, 6.744133367849677, 6.74421168874279, 6.744329129117962, 6.74442991856794, 6.744452199512176, 6.744565201145628, 6.744810276919524, 6.744814655402902, 6.745129624891986, 6.745216792051312, 6.745221724929822, 6.745248384453726, 6.745248384453726, 6.745311156911643, 6.745480917056497, 6.745510780764096, 6.745569887113223, 6.74567756307357, 6.74569881385778, 6.745724094245309, 6.745724094245309, 6.745760493720141, 6.745986926942459, 6.746172158452833, 6.7462108499363325, 6.74631439458213, 6.746356458320904, 6.7463784453217865, 6.746412418683915, 6.746432230845729, 6.746488221740248, 6.746601901150966, 6.746699563222229, 6.746699563222229, 6.7467127523439885, 6.746792636693748, 6.746792636693748, 6.7470742312426815, 6.747234597928559, 6.747381005635076, 6.747453192456457, 6.747453192456457, 6.747557485504866, 6.747584963500905, 6.747658856544836, 6.747786871168452, 6.747804670357243, 6.748007784067588, 6.748007784067588, 6.748267390971809, 6.748418029009454, 6.748669990425421, 6.748674402750052, 6.74870300426444, 6.748705996524253, 6.748705996524253, 6.748795647073768, 6.748811700234242, 6.748844587430282, 6.748877939584227, 6.748967444745337, 6.748967444745337, 6.748988687823142, 6.748996604725929, 6.749172408689278, 6.749220119443043, 6.749220119443043, 6.749279048314439, 6.749478397544884, 6.7495397374982, 6.74968051529993, 6.749846831017377, 6.749870545312402, 6.749910397796521, 6.750031517278856, 6.750174952239143, 6.7502356360668445, 6.750261179877839, 6.750391275807794, 6.750664716507359, 6.750666964857958, 6.750718388425699, 6.750970096823652, 6.75101766478447, 6.751037688108593, 6.751048602227378, 6.751183273582017, 6.7512168180589995, 6.751401721726648, 6.751431392405769, 6.751431392405769, 6.751551846091562, 6.751681569786261, 6.751806140377727, 6.751877472917904, 6.7518836916209395, 6.751910990469919, 6.751961945801254, 6.752139511074201, 6.752208529584289, 6.752347196684129, 6.752502604145301, 6.7525095597897, 6.7525473323423615, 6.752613999739877, 6.752695936035004, 6.752778942847226, 6.753108751168273, 6.753208373862157, 6.7532419316300425, 6.753279237623151, 6.7533382571584974, 6.753538896777922, 6.753538896777922, 6.7535686604654765, 6.753634706125484, 6.753685008349775, 6.753687195219611, 6.753780250505654, 6.75380935976726, 6.753828146771113, 6.75406673444359, 6.754114068815612, 6.754158653337035, 6.754161205282369, 6.754232615567865, 6.7542461072674795, 6.754282771248352, 6.754292213022447, 6.754318328243539, 6.754575522028065, 6.754749275155903, 6.754765554181974, 6.754812688127105, 6.75503200756959, 6.75517586847319, 6.75517586847319, 6.755179727400156, 6.7552978347739, 6.755302088211068, 6.755367641161069, 6.755592615967148, 6.75572878515195, 6.755769004118498, 6.755769004118498, 6.755875605333191, 6.756200589949379, 6.756260780701117, 6.756404266300709, 6.7564292780050845, 6.756863917306516, 6.75702255712566, 6.757051114540098, 6.757410503231773, 6.757479303691136, 6.757758979372594, 6.757804690914167, 6.757921713825449, 6.758090635038201, 6.758190505161655, 6.758193349120061, 6.758203263256213, 6.758299909607694, 6.758444894328237, 6.7584459166136615, 6.758539459739597, 6.758805185988138, 6.758812750389827, 6.758812750389827, 6.758828287914975, 6.758877091605354, 6.75924861274864, 6.759423669528479, 6.759512371642069, 6.759567774810783, 6.759726041345351, 6.759758736329271, 6.760030074991159, 6.760030074991159, 6.760065186233604, 6.760230513807138, 6.76028761984699, 6.760354561005779, 6.760435697330223, 6.7604991466665245, 6.760612602830094, 6.760639726970774, 6.760675249278073, 6.76075836614221, 6.760771340714596, 6.760900922875638, 6.76091376155932, 6.760979570946918, 6.761239820148684, 6.761285450056771, 6.761534142080892, 6.761814030310538, 6.761880318854502, 6.762037617858845, 6.762063996779948, 6.762063996779948, 6.762103744432681, 6.762130163153992, 6.762199252761003, 6.762249650232183, 6.762275252574517, 6.762375454877722, 6.76241374565081, 6.762521977348273, 6.762743454619311, 6.762774283195584, 6.762813954446004, 6.762834442584822, 6.762887902138261, 6.763053650355915, 6.763053650355915, 6.763055617140481, 6.763485833396314, 6.763513218998191, 6.763543406338312, 6.763573365769079, 6.763617631820369, 6.7638111414770155, 6.763827970332615, 6.76385617953327, 6.764002280984761, 6.764080065412708, 6.764171426640024, 6.764171426640024, 6.764229952976824, 6.76425566360261, 6.764436377510151, 6.764590689664785, 6.764647412400144, 6.764732991249924, 6.764743100527735, 6.764792679209951, 6.764841376715402, 6.764863949450181, 6.764948427002261, 6.7651253164623455, 6.7652121632675195, 6.765273347097317, 6.76529198742059, 6.765420906092485, 6.765423037285983, 6.765515062823187, 6.765593107952631, 6.765645096888165, 6.765662904169883, 6.765683618928018, 6.765955346526069, 6.7662034956488, 6.766377036743436, 6.766495310041298, 6.766646507767817, 6.766758948329752, 6.766758948329752, 6.767239818343288, 6.767239818343288, 6.767305962174096, 6.767305962174096, 6.767380634019251, 6.76742630040475, 6.767429270469574, 6.767429275667929, 6.767429275667929, 6.767435823879632, 6.7674787084535515, 6.767483454008132, 6.767634117051833, 6.767661439750931, 6.767668048796402, 6.76791801548767, 6.768202316672863, 6.768211143925366, 6.768239143220437, 6.768239143220437, 6.768392855917214, 6.7684201222529845, 6.76853703575665, 6.768849771799391, 6.7689629471266874, 6.769002030267734, 6.76902438673682, 6.769108174068171, 6.7691870242131635, 6.769375046496133, 6.769535980293093, 6.7697101222994664, 6.7697485252868885, 6.769818539984427, 6.76983442208273, 6.770081887279739, 6.770160712188219, 6.770160712188219, 6.770165381867281, 6.770188777803031, 6.770218182051593, 6.7704960194132555, 6.770839109551391, 6.77089945058237, 6.770940260805587, 6.770941029093834, 6.770982770902082, 6.771062976248052, 6.7714640141195, 6.771512774363573, 6.771571945240259, 6.771588780662046, 6.771718165652541, 6.771748945885714, 6.771794527689318, 6.771822847061953, 6.7718869870189815, 6.771898571895324, 6.772222011979693, 6.772247591725558, 6.772481340982135, 6.772509585389891, 6.77263038986224, 6.772732171375778, 6.772774622261762, 6.772811119308447, 6.772863292231337, 6.772873221164644, 6.772873221164644, 6.773030074234861, 6.773046519329848, 6.773204257453392, 6.773218073608667, 6.7732364130319915, 6.773448338209078, 6.773448338209078, 6.773523874040204, 6.773523874040204, 6.773723347070598, 6.773723347070598, 6.7738065239621035, 6.773869503523648, 6.7739259538103145, 6.77393299389559, 6.77404566793297, 6.774047145687346, 6.774077584408844, 6.7741110254182235, 6.774126562160061, 6.774168182965207, 6.774255313218314, 6.774306701311263, 6.774353587927992, 6.774360009275457, 6.774486538898665, 6.774504761430998, 6.774522810057539, 6.774682955635724, 6.774731334161462, 6.77486497706847, 6.774882759344974, 6.774896976010863, 6.774896976010863, 6.775034903496038, 6.775053818395175, 6.775114896732437, 6.775114896732437, 6.775180140176875, 6.775373910298922, 6.775508459183492, 6.775682891859114, 6.7757638820304, 6.775872817393851, 6.7758936731863555, 6.775912680341912, 6.775965656458566, 6.776044409796856, 6.776077211826954, 6.776097349509544, 6.776169159249337, 6.776345921626735, 6.776345921626735, 6.776348641991977, 6.776371028216238, 6.776562470930847, 6.776755357983179, 6.7769722670145836, 6.777100967065101, 6.7774562020643065, 6.777517913524936, 6.777517913524936, 6.777618939596304, 6.777721388217695, 6.778111718480266, 6.778376983131334, 6.778519090866311, 6.778519090866311, 6.778655817363344, 6.778697813470619, 6.778706340273732, 6.778866872311342, 6.779065679792273, 6.779161815817561, 6.779191221426535, 6.779354514883086, 6.779431387617849, 6.779463859475803, 6.779586238743109, 6.779646492020343, 6.779666676086755, 6.779683181559742, 6.779683181559742, 6.77985835738541, 6.779911179691681, 6.779950572936196, 6.779953626463777, 6.780011011291641, 6.780020215501573, 6.780062919422603, 6.780194886093196, 6.780383234492375, 6.780397668708605, 6.7804380585189445, 6.780577787786792, 6.780803385787711, 6.780803385787711, 6.780972076357629, 6.781166465038056, 6.781166465038056, 6.781245739619968, 6.781332420260734, 6.781341608072677, 6.781341957172385, 6.781418573421739, 6.781624766264257, 6.781811240439509, 6.781883615463834, 6.781912952990432, 6.781930988564173, 6.782032632545695, 6.782099666294051, 6.782168277164752, 6.782259899070576, 6.782338548537544, 6.782459432163597, 6.7825486454348445, 6.782751623017461, 6.7828468091593965, 6.7828604114438145, 6.782946089230965, 6.783049172093025, 6.783055621357296, 6.783087893621088, 6.783159627350087, 6.783217974161726, 6.78351909710712, 6.783599949399181, 6.7836763585871696, 6.783685668809634, 6.783723050348018, 6.783955265825533, 6.784112705974539, 6.7842744756344295, 6.784357623848166, 6.784357623848166, 6.784370325493101, 6.78444471827843, 6.784543960325541, 6.784627215412265, 6.784828405765374, 6.784917951912509, 6.784926173470257, 6.785019472098731, 6.785124055787291, 6.785213213406356, 6.785253711999564, 6.785342472790956, 6.785480529306378, 6.785518493727498, 6.785518493727498, 6.785567450305642, 6.785768125852564, 6.785788621254301, 6.78587410288555, 6.78601618466833, 6.786017151391489, 6.786043684713373, 6.786300212037232, 6.786414373921934, 6.786531456082843, 6.786531456082843, 6.786567026712905, 6.786880740378552, 6.78694651465307, 6.787069300993496, 6.787072187635533, 6.787072957915231, 6.787090601983648, 6.78710640988188, 6.787126884425425, 6.78713173053786, 6.787249169777686, 6.787390588583655, 6.787407022017138, 6.787862040772579, 6.788183504463183, 6.78838144684382, 6.788656750764527, 6.788892400502616, 6.789076232529087, 6.789108672596812, 6.789202567099549, 6.78923415108347, 6.789238909264219, 6.789295185184324, 6.789377779667791, 6.789396341936935, 6.7894881692234, 6.789554300935584, 6.789850582350353, 6.789872319240769, 6.789921400080688, 6.789959829576834, 6.790271908913479, 6.790556666959446, 6.790706299933238, 6.79072812825704, 6.790854253310624, 6.790989984867778, 6.791252637655059, 6.791258854230805, 6.791280649581091, 6.791441368082155, 6.7914621539538675, 6.7915454753778075, 6.791610084984555, 6.791663701010369, 6.791755712518187, 6.792197583641483, 6.792212229912059, 6.792353934821551, 6.792372527100923, 6.792513027232833, 6.79270220991609, 6.792879472995602, 6.792916355997838, 6.792954287243572, 6.793056325380965, 6.793099460514743, 6.793104873486801, 6.79324415525001, 6.793278814487836, 6.793756593250632, 6.793758517263868, 6.793832411935391, 6.793921874913537, 6.793921874913537, 6.794032308474185, 6.794032308474185, 6.794047898354684, 6.79419056871863, 6.794439782499465, 6.794448416859594, 6.7944863015587185, 6.7945449183914475, 6.7946074389781215, 6.794612110805289, 6.794694016667017, 6.794769080486177, 6.794999783525646, 6.795009053977975, 6.795233101004447, 6.795307996822526, 6.795336494602817, 6.795337458635533, 6.795375485874955, 6.795507381235863, 6.795961914649069, 6.795969447161445, 6.7959859798232, 6.79601383002283, 6.7962086744570795, 6.796259781971959, 6.7963275413811335, 6.796498621498541, 6.7966102735805265, 6.796619698577301, 6.796685849000417, 6.796759062761119, 6.7967937243444565, 6.7968052715817215, 6.796907305410862, 6.79695366723477, 6.797039043796245, 6.797042003553598, 6.797042003553598, 6.797059758854656, 6.797155665671214, 6.79718922968389, 6.797267793867253, 6.797267793867253, 6.797425005411829, 6.797523596769087, 6.797524899814335, 6.797525997013407, 6.797646173613605, 6.797646173613605, 6.798003765382856, 6.798036929509009, 6.798179199534767, 6.798228809902387, 6.7983036590411166, 6.7983036590411166, 6.798333015597766, 6.798333015597766, 6.798386826542814, 6.798686595723027, 6.79874207391909, 6.798766375605872, 6.798799864377921, 6.79887039938319, 6.79893790030532, 6.799078161273072, 6.799167903272088, 6.799167903272088, 6.799185620983782, 6.799185620983782, 6.7992390793170605, 6.7992390793170605, 6.799254867749285, 6.799277106522758, 6.799511972615095, 6.799527050346826, 6.799625879968789, 6.79967546872562, 6.79967546872562, 6.79972597225097, 6.799821269456114, 6.799867718607914, 6.799953540616234, 6.80010643496165, 6.800139026128165, 6.800300481604272, 6.800372184349662, 6.800540915469953, 6.800556287958602, 6.8007732064649495, 6.800832006231222, 6.800839245926839, 6.801047889352724, 6.801050906002866, 6.801104389591004, 6.801104389591004, 6.801206112819864, 6.80123360917978, 6.8012519872984445, 6.801252925008839, 6.80135357591043, 6.8014144321374586, 6.801597008066588, 6.801607327568743, 6.801708836670643, 6.801809260490186, 6.802047356596102, 6.802078133498035, 6.802149227125866, 6.80240462074326, 6.802424408962769, 6.802425635263396, 6.8028529103435025, 6.802861271907502, 6.802939523745292, 6.803018471057993, 6.803074498980951, 6.80307813033288, 6.803098682573266, 6.803101137950661, 6.803213189127264, 6.80327661118645, 6.803343124926656, 6.803348255201151, 6.8035155480226095, 6.8035155480226095, 6.803624402646413, 6.803656582654783, 6.803670819563966, 6.803734469024579, 6.8038403577732005, 6.803858195128696, 6.803924146387505, 6.803988069021518, 6.803988069021518, 6.8044123289240765, 6.804721853612724, 6.804765972437013, 6.8049941934809945, 6.8049941934809945, 6.804995791699143, 6.805116945111322, 6.805128083908895, 6.805128083908895, 6.8051371218165295, 6.805244780558388, 6.8052493729059265, 6.805369234868101, 6.805426625847087, 6.805426625847087, 6.805442761159875, 6.8054681040165015, 6.8056399917484125, 6.8056666409794895, 6.805681767813568, 6.805702337406977, 6.80581789732241, 6.80592317832103, 6.805996366898353, 6.806028113850672, 6.806146388530191, 6.806153813214775, 6.806165740395117, 6.80623797993073, 6.806289475968745, 6.806300523529961, 6.806358575175202, 6.806456704987196, 6.806456704987196, 6.8064866114400155, 6.806516986349477, 6.806516986349477, 6.806600735804882, 6.806615318458661, 6.806753674845775, 6.8067752612788786, 6.806778045787925, 6.806810101759563, 6.806810101759563, 6.806841309145622, 6.806841309145622, 6.806896256517914, 6.8070715780994595, 6.807234840952123, 6.8073490528655825, 6.807383056636109, 6.807400196407002, 6.807439688391335, 6.807439974271313, 6.807486034962063, 6.807486034962063, 6.807510679882608, 6.807510679882608, 6.807537219035532, 6.807537219035532, 6.807656264780226, 6.807714971870392, 6.807727455294784, 6.807727455294784, 6.807801282249477, 6.807801452207125, 6.807884519203245, 6.80793619352764, 6.807946782893081, 6.808178927753019, 6.8082743694772905, 6.8082743694772905, 6.808290445341039, 6.8082996031857155, 6.808460203012028, 6.808509151355173, 6.808541197220519, 6.8086899357331205, 6.8088521226032235, 6.808911149768128, 6.808964925671726, 6.80897891543479, 6.809021716321313, 6.809390010088411, 6.809495817257314, 6.8095623448960625, 6.809699171400187, 6.809712776296151, 6.80988109463263, 6.809924912296465, 6.809993716535475, 6.8100549464260505, 6.810166581035085, 6.810209490724206, 6.810243438904214, 6.810424188854688, 6.810522515007942, 6.810549802118702, 6.810647201200142, 6.810669422922231, 6.810669422922231, 6.810811282575863, 6.810814053698527, 6.810848115128554, 6.810883548373076, 6.810994245132756, 6.811155015980614, 6.811187860895196, 6.811202412820478, 6.811261986326896, 6.811577925621668, 6.811699737003487, 6.811819625090375, 6.8119773771842596, 6.812094967842683, 6.812101853993362, 6.812126902080421, 6.812147320289429, 6.812147320289429, 6.812296940324757, 6.812555036015724, 6.812605008155295, 6.812615234863208, 6.812661387431825, 6.812661387431825, 6.812989598050782, 6.81305977607374, 6.813435356052806, 6.813449841494796, 6.813482481905446, 6.813538298342572, 6.813720318923589, 6.81373635193527, 6.813788000647613, 6.813921953846145, 6.814101493384001, 6.814175338617194, 6.814542435429998, 6.814622721401726, 6.814832289709584, 6.814838758024858, 6.814838758024858, 6.814922968207277, 6.815182565990647, 6.815262744911521, 6.815270653897371, 6.815270653897371, 6.815297636703491, 6.815342606309304, 6.8154822694311115, 6.815512973180037, 6.815693872838367, 6.815699784206395, 6.815701227753296, 6.815755466884584, 6.815827759454031, 6.815914331503295, 6.8159391059485595, 6.81602095460212, 6.816023672443116, 6.816035007007069, 6.816283961928415, 6.816350535417459, 6.816468363757148, 6.81648758576369, 6.81648830285896, 6.81703689178079, 6.817362856513427, 6.81756475570919, 6.817567962264651, 6.817573267642288, 6.817618251221106, 6.8176633008330185, 6.8176633008330185, 6.817919552931761, 6.818004090547017, 6.8180097229341, 6.81803729874279, 6.81803729874279, 6.818103809905266, 6.818160906010131, 6.818302458870895, 6.818578531233752, 6.818682301982888, 6.818738779622018, 6.818887882016931, 6.818963555516523, 6.818968119087486, 6.819381733181699, 6.8194248523935, 6.819489621150785, 6.819533213868264, 6.819614219852548, 6.819651450224719, 6.819737148061678, 6.819785936631108, 6.819919155650362, 6.819925266903642, 6.820151313950538, 6.820330020958523, 6.820398386823215, 6.82044027565223, 6.820478454928463, 6.820516852196959, 6.820670398455734, 6.820670398455734, 6.820674015149151, 6.820702774577406, 6.820735606262913, 6.820735629498927, 6.820792110720057, 6.82084604065754, 6.8210142676482866, 6.8210187416165455, 6.8210187416165455, 6.821045749776119, 6.8211150720959575, 6.82112656716081, 6.821377604582352, 6.8214312360541935, 6.821504816647327, 6.821527929301143, 6.8216458092832495, 6.821782878581499, 6.821914809765713, 6.822183909824345, 6.822211628398148, 6.822364192404235, 6.822413434528613, 6.822466324355013, 6.822536811740806, 6.822588729701207, 6.822636659209614, 6.822719151651451, 6.822869996600182, 6.82292079435044, 6.82295648396581, 6.823060311584309, 6.823076075764683, 6.823277046590943, 6.823306057829434, 6.823421107787493, 6.823500501308531, 6.823564615964914, 6.8236317650994165, 6.82368191245547, 6.82373678679378, 6.823754407866961, 6.8238125429552845, 6.823850746130808, 6.823974435805678, 6.824011522743074, 6.824082112555681, 6.8243930671558, 6.824406924181975, 6.824459977517536, 6.824504120136909, 6.824507802251572, 6.824841707952871, 6.824841707952871, 6.824878904218926, 6.8250333472534574, 6.825082109705268, 6.825156581030053, 6.825228444767022, 6.825287706276271, 6.825355436556959, 6.8253647375896245, 6.825461789903976, 6.825601205842424, 6.825655535345966, 6.825687441030111, 6.825702134251782, 6.8257471247617065, 6.825766048120475, 6.825773402679141, 6.8257743621333065, 6.8257743621333065, 6.8258528359252235, 6.826004462922196, 6.826041419151092, 6.826108464807727, 6.826271802990808, 6.826343151357143, 6.826364544373038, 6.826422849655637, 6.826437775985414, 6.826656398935806, 6.826722787785089, 6.82683403490473, 6.82683650285994, 6.826848765842546, 6.826975480510764, 6.827158596505565, 6.827192015662128, 6.827242307142871, 6.827285309500397, 6.827347755763306, 6.8273997664284085, 6.827423395635479, 6.827427583951761, 6.827439524395713, 6.827773081379555, 6.827773686401093, 6.827775198875734, 6.827825947315907, 6.827940262657393, 6.828085882629722, 6.828086024732161, 6.828286527756924, 6.828351831075327, 6.828388223037242, 6.828510969750234, 6.828552780562699, 6.828708050062989, 6.828724157474381, 6.829078900048495, 6.829158336890168, 6.829214555965418, 6.829290800198612, 6.829391950640724, 6.829427641989735, 6.829462099587441, 6.829511722429191, 6.8296008760707, 6.829616184973563, 6.829643817799402, 6.82973356959924, 6.829784501478352, 6.829811015207004, 6.829811015207004, 6.8298627764115984, 6.8299232379200125, 6.829980966086069, 6.830039109551599, 6.830080361927225, 6.83015094754201, 6.830211056832415, 6.830323060921428, 6.8303327940371075, 6.8304929579590326, 6.830585999315105, 6.830639287768742, 6.830647669806384, 6.8306600226567395, 6.830687274567438, 6.830712046340543, 6.830753364087752, 6.830766266228523, 6.830774754136326, 6.831045342467908, 6.831138766490528, 6.831193377704825, 6.831259957339559, 6.8313862464018085, 6.831393196979846, 6.831412791161393, 6.831541588797485, 6.831541793204213, 6.831554940591827, 6.8317529958572205, 6.831768418730551, 6.831859049194011, 6.831929097898013, 6.831938647365629, 6.831965332660761, 6.832089587939663, 6.832166770722446, 6.832191887055122, 6.83227780324467, 6.832314435552505, 6.832358736382776, 6.83245544217643, 6.832552718234007, 6.832562353502332, 6.832594009394865, 6.832651666218775, 6.832813911481772, 6.832835321622046, 6.832855687246612, 6.832883314712859, 6.832981158710815, 6.833058252762784, 6.833080583243952, 6.833267293066961, 6.833418575403209, 6.833761190903571, 6.83387554319498, 6.833937724110962, 6.833982085868498, 6.83402886481837, 6.834280634899673, 6.834296671244229, 6.83430890012422, 6.834386206591941, 6.834425667242526, 6.834671477139755, 6.834846296686171, 6.835031857133011, 6.835119208753828, 6.835173930064426, 6.8352130659331465, 6.835289195647677, 6.835294976245056, 6.835338082388595, 6.835407172499895, 6.835450887393291, 6.8354656985836435, 6.835489965130977, 6.835540285364539, 6.835711310738863, 6.8358785842081655, 6.835971545152847, 6.836009668282438, 6.8360267842369975, 6.836142908497901, 6.836346419083367, 6.836507613397931, 6.836567712953546, 6.836686229734347, 6.8366885969927, 6.836711374830544, 6.837168377718275, 6.837170509015845, 6.837320215802093, 6.837421258223083, 6.837571009491945, 6.83757216052201, 6.837618580963115, 6.837619353460707, 6.837812023983587, 6.83787316109018, 6.837877364545887, 6.837915394007227, 6.837937595685394, 6.837993788534209, 6.8380785722881265, 6.838376041940593, 6.8383774994151825, 6.838394218542423, 6.838397747399966, 6.838408260483828, 6.8384789952590985, 6.838561242100546, 6.83860426482565, 6.8388032922586515, 6.838806386224238, 6.838965572421621, 6.8390235341896615, 6.839136824799351, 6.839143668900621, 6.839229718370965, 6.839262765967031, 6.839276232047231, 6.839566314314401, 6.839568319255846, 6.839635700322429, 6.839741807494594, 6.839821495183063, 6.84000897791952, 6.840011081809031, 6.840140866653172, 6.840164682222531, 6.8402182825567515, 6.840288556413218, 6.840443071877492, 6.840497095539518, 6.84067373571319, 6.840763553370224, 6.840787504397057, 6.840834627192531, 6.840893395757864, 6.840901013984224, 6.8410159092063045, 6.841087614482845, 6.841235056666362, 6.841247041009911, 6.841325474663288, 6.841353021465515, 6.841373390568424, 6.841509661580774, 6.841684063171572, 6.841826528167725, 6.841954116666337, 6.842031778883803, 6.84204597452068, 6.842046276510665, 6.842094039488139, 6.842245616759971, 6.842246472292962, 6.842246472292962, 6.842312476887374, 6.842312476887374, 6.842330852154996, 6.842340377607331, 6.842403210550034, 6.8424801908982165, 6.842524776708923, 6.842552825154829, 6.842673460846901, 6.842791437804595, 6.842806444008248, 6.842897136057119, 6.842947263870413, 6.843121390275749, 6.843269281603039, 6.843341154872381, 6.843438920789709, 6.843438920789709, 6.843474257485899, 6.8435181596077355, 6.843570405065349, 6.843728740015546, 6.843737454475388, 6.843896232424382, 6.843947652413002, 6.8439547179588995, 6.844066898664051, 6.844100168886634, 6.844140568515654, 6.844229630216544, 6.844238554062093, 6.844265468336996, 6.844275085011458, 6.8443165272620705, 6.84432657114287, 6.844407270158729, 6.844517183014999, 6.844625784376515, 6.844819300570859, 6.844836580829985, 6.845008591744092, 6.8450531255790334, 6.845061792320184, 6.845214762381099, 6.845344778449134, 6.845456261686787, 6.84557385809683, 6.845663431875747, 6.845707364462289, 6.845912400060985, 6.846149439226119, 6.846185557805969, 6.846205435272159, 6.846294902183935, 6.84629567734233, 6.846305464793813, 6.8464746718108564, 6.846645950946813, 6.846897347277771, 6.846901021625968, 6.847076975220621, 6.8471714177440575, 6.847199932784673, 6.847233347674079, 6.847413946087754, 6.847413946087754, 6.847576674927627, 6.847687618448753, 6.847880893357671, 6.848060872613197, 6.848154220336933, 6.848203029671764, 6.848245150322345, 6.848273230064195, 6.848279336067124, 6.848464496226486, 6.848512804533679, 6.848587145791327, 6.848632427102764, 6.848668133561091, 6.848700970837947, 6.84886771985792, 6.848966003868036, 6.848973012733999, 6.8491690648240935, 6.849353211700197, 6.849353211700197, 6.849599845336588, 6.84962524069941, 6.849639375233271, 6.849800394290433, 6.84989761418182, 6.8499090696669915, 6.850053777939568, 6.850400431674525, 6.8504089047575745, 6.850516393701162, 6.850631184092676, 6.850712355264217, 6.850730300464496, 6.8509441256797725, 6.8510015918170355, 6.851191901494207, 6.851205382754455, 6.851231469315422, 6.851284250622011, 6.851305474705376, 6.851414796345919, 6.851430139504859, 6.851618190811568, 6.851746371114634, 6.851788588015698, 6.851884526118723, 6.851953587538058, 6.852301278565848, 6.852661519494741, 6.853041990636919, 6.853223624149123, 6.853432946425339, 6.853966241098188, 6.85398378156942, 6.853988345076307, 6.854017219550273, 6.8540273482004395, 6.854062142891745, 6.854111149882948, 6.854217385613027, 6.854493648351649, 6.85451728092114, 6.854533545901995, 6.854580578179485, 6.854600769950702, 6.854607438736771, 6.854731312182614, 6.85486266698443, 6.855023144877078, 6.8551311407949145, 6.855233680763465, 6.855321793623771, 6.855356008153396, 6.8554017235578195, 6.855403474881846, 6.855526503548599, 6.855634913475458, 6.855957201704883, 6.855962715196149, 6.85607659852981, 6.856133450667153, 6.8561989879446825, 6.85620628169242, 6.85627400571591, 6.856497495508972, 6.856553255797132, 6.856567560314032, 6.856831884305291, 6.8569102188023425, 6.857033007453755, 6.857178910105235, 6.857204183901086, 6.857209981686878, 6.857342897767709, 6.857649520521083, 6.8576963890109495, 6.857974399772673, 6.858020547840521, 6.858169212998573, 6.858209702254549, 6.858275605595416, 6.85838350423778, 6.858454294815685, 6.858500168032666, 6.8586763948803044, 6.858707655363682, 6.858756350476896, 6.858818445670362, 6.859052814170857, 6.859115006412825, 6.859150455900459, 6.859423873671844, 6.8596840918228725, 6.859906414571637, 6.8601468273689035, 6.860167210077496, 6.860259313970885, 6.860339895105386, 6.860424279019273, 6.860433259731144, 6.860476169567478, 6.860502086768859, 6.860503858381468, 6.860557422702693, 6.860585522573817, 6.860608719531379, 6.8606430661803985, 6.860730963528327, 6.8608134591927445, 6.861104258538877, 6.861114410986162, 6.861229530128835, 6.861355983874692, 6.861383312631364, 6.861410093835641, 6.861436147643372, 6.861620296003848, 6.861664421606528, 6.861728708210044, 6.861775961379493, 6.861820928244626, 6.86205426090683, 6.862122927571109, 6.862316611964844, 6.862356925711167, 6.862369400189572, 6.862680777498764, 6.862893682726989, 6.863001797098588, 6.863076344128359, 6.863083984118505, 6.863115170318528, 6.863131810929944, 6.863210958673783, 6.863332898448499, 6.863346613247924, 6.863463576748722, 6.863507905566615, 6.863512904510634, 6.863521763840096, 6.863746404095372, 6.863838544145816, 6.863870529341486, 6.863960197600386, 6.864146684489599, 6.86427830876919, 6.864280367713713, 6.864424162261916, 6.864519250194183, 6.864543160610067, 6.864627790582099, 6.864704414040463, 6.86470789835775, 6.864735575153592, 6.864860031963526, 6.864938078703775, 6.8650328924119455, 6.865044517478338, 6.865130417227535, 6.865214709482633, 6.865610910005584, 6.865660130892179, 6.865761506612313, 6.865792335859774, 6.865961929735578, 6.866184498773012, 6.866300731052426, 6.866490826316672, 6.866584095692799, 6.866592648683041, 6.866626828828622, 6.86667049542114, 6.866792743546642, 6.866799508866541, 6.866856757292237, 6.866967553734986, 6.867064448306591, 6.867123430999454, 6.867137058449269, 6.867142082746431, 6.867231268683143, 6.8673003873647795, 6.867305720516822, 6.8673354999284495, 6.867427247332184, 6.867437959819196, 6.867640094105721, 6.867672574098751, 6.867800205472763, 6.867949233566201, 6.867962035009697, 6.8680459388592645, 6.868108884228571, 6.8681308985957195, 6.868149493442129, 6.868267549879561, 6.86837750337217, 6.868402419347673, 6.868572054929456, 6.868694137722713, 6.868734224492501, 6.868757050662356, 6.868797373158438, 6.868833209593662, 6.868849191774639, 6.869161537609536, 6.8691796724336145, 6.869296900422701, 6.869423759312984, 6.869456984320864, 6.869458170447966, 6.869502766861908, 6.869535049279671, 6.869548078891124, 6.869799855423506, 6.869826757145407, 6.869990249976557, 6.8701115526325705, 6.870212127634805, 6.8702497815687655, 6.870254478899253, 6.870357437389607, 6.8706062407384945, 6.870632604593906, 6.870820165970799, 6.870826530507575, 6.870990518698708, 6.8710157680772035, 6.871022664082005, 6.871253501654539, 6.8713291232496125, 6.871372435805511, 6.871395838605892, 6.871521485547277, 6.871761701681974, 6.871790846449319, 6.871865636783595, 6.87201748290134, 6.872238845976211, 6.8723950716832665, 6.872483146305623, 6.872549357979473, 6.872586255501126, 6.872873203724778, 6.872946294496003, 6.873060483454552, 6.873062740451424, 6.873134365933964, 6.873142400338108, 6.873146354010571, 6.873201941590155, 6.8733039483765825, 6.8733750482893115, 6.873384102536314, 6.873696708958448, 6.873983127850714, 6.87408935114928, 6.874179363065485, 6.8744622274698095, 6.874480632559209, 6.87449850306993, 6.874567334997318, 6.874861538607264, 6.874886293042073, 6.875098617527562, 6.875119540161753, 6.875307990504466, 6.875403549762223, 6.875575616411095, 6.875680811346268, 6.87571565551151, 6.875716178048807, 6.875750801546668, 6.875755387914762, 6.875873244944105, 6.875873730106204, 6.876052212991394, 6.876072905496242, 6.876116640291812, 6.876128999005577, 6.87639824214105, 6.876779894922881, 6.876917893882642, 6.876948789798919, 6.877021154321881, 6.877033794556357, 6.87706441230642, 6.877137011074629, 6.877218972530491, 6.877360517184367, 6.87739333075593, 6.877454751164228, 6.877540466797813, 6.877717621713131, 6.877975417063827, 6.8780525706633595, 6.878072277361511, 6.878073120448327, 6.878109896800452, 6.8781289888834864, 6.878399812253335, 6.87841580397175, 6.878517082584053, 6.878524904667918, 6.878602736346901, 6.878704939303241, 6.878790500568156, 6.878826646598127, 6.878868213085427, 6.879025828330738, 6.8790576850688145, 6.879097145544946, 6.879294424015741, 6.879303437201966, 6.879335859697148, 6.879368400303536, 6.87948291710981, 6.879566207667077, 6.879582110490914, 6.8798479227833385, 6.879871272349947, 6.879892477014018, 6.880031589890663, 6.880150338043757, 6.880225018814744, 6.880313040012909, 6.880357868778194, 6.880444293420494, 6.8805305949453155, 6.880628663141298, 6.880669258449022, 6.880753918382379, 6.880924234543464, 6.88101123649261, 6.881080468515236, 6.881097337932774, 6.881161054097731, 6.881175795758779, 6.8811828944641595, 6.881279193800905, 6.881783492904812, 6.881916068070122, 6.881918724535068, 6.882000342545458, 6.882203733819527, 6.882303277330118, 6.8823127139172895, 6.882314064596126, 6.882404557413138, 6.882540177377953, 6.882811188237105, 6.882999434684768, 6.883126866338551, 6.883190953860605, 6.88327298775574, 6.8834080008370035, 6.883506673488257, 6.88357342482654, 6.883592603520966, 6.883687248140867, 6.883708001453515, 6.883777042112563, 6.883788411155615, 6.883820891341664, 6.883820891341664, 6.883890862052368, 6.883909671031496, 6.884013500146523, 6.884152152786392, 6.884207638931601, 6.884254091836548, 6.884349899299884, 6.884418234762967, 6.884473120218036, 6.884538151245161, 6.884598799767981, 6.884630265295984, 6.884770278951576, 6.884781138315223, 6.884880482232216, 6.885262559657021, 6.885576107002593, 6.8855956873032005, 6.8856843566319, 6.885707030683843, 6.88572108686143, 6.885859341319017, 6.885916064581177, 6.885931611628082, 6.885981531589036, 6.886017557766413, 6.886073814820525, 6.8860917576055725, 6.886471031004974, 6.886513862797127, 6.886541210209554, 6.8866997460810095, 6.8870121500509445, 6.887040554943251, 6.88704292814222, 6.887110181095521, 6.887218659363551, 6.887300022985778, 6.887355481557417, 6.8873695184803445, 6.887748311926643, 6.887874787487451, 6.8882271516063245, 6.888239932643438, 6.888318622714692, 6.888327026391591, 6.8883656439741685, 6.888542423645587, 6.888720338630236, 6.888756017546424, 6.8888401492072875, 6.888872180399331, 6.889112187309128, 6.88920715853189, 6.889292913711934, 6.889304810968161, 6.889503985188728, 6.889699679394088, 6.889783064358037, 6.889854826035656, 6.8900403238786785, 6.89026924776905, 6.8906442147401386, 6.890646840760553, 6.8906658058341534, 6.890734062258626, 6.890766104499143, 6.890816145975311, 6.891024203700525, 6.891345385108089, 6.891446429485919, 6.891606498608383, 6.891786508684291, 6.891900546219783, 6.891939657720072, 6.891986263976835, 6.892295231656799, 6.892367996616438, 6.8923841662799505, 6.892419779337848, 6.89243949227165, 6.892453073522789, 6.8924717343373825, 6.892474315578985, 6.892500596825018, 6.892508193135324, 6.892746312513282, 6.892746371766677, 6.892749307901732, 6.8929349359258465, 6.892966474531392, 6.893063882786599, 6.893176926486254, 6.893325240451894, 6.893351235103339, 6.893375983918931, 6.893705338821762, 6.89381606588653, 6.893856963633815, 6.8938610299281615, 6.893945775478645, 6.893946109065519, 6.894145316019127, 6.894261115351676, 6.894285011501258, 6.894353874610328, 6.894410142671439, 6.894411777352094, 6.894414525946399, 6.894477369359416, 6.894542380388659, 6.895034454113512, 6.895132346259802, 6.895176202052842, 6.895237146506679, 6.895346438415464, 6.895393962078365, 6.8956494587808566, 6.895674498802021, 6.895812947669037, 6.8958686459084735, 6.896096323004826, 6.896119714002067, 6.896172278036188, 6.89634318259259, 6.896372426581726, 6.896432478451959, 6.896521818057387, 6.896704306855021, 6.896740913537894, 6.897020259900255, 6.897047275878569, 6.897131582055958, 6.8973309441127135, 6.897495152411008, 6.897554731355989, 6.897608978496248, 6.89763828761605, 6.897691742080838, 6.897697283561371, 6.897880864653304, 6.89824768583043, 6.898299131517479, 6.898347999840336, 6.898652428510821, 6.898804644607424, 6.898862883738842, 6.898944299604805, 6.898956822604959, 6.899061620585932, 6.899158602742722, 6.899190796988447, 6.899356913991906, 6.899416024356235, 6.899594751640433, 6.899973889944124, 6.900003398800521, 6.900092098511425, 6.900118791873911, 6.900215700330573, 6.90042272871897, 6.900443170875814, 6.9004856721755, 6.900491536637477, 6.900579072608604, 6.900862064089991, 6.9008662154480485, 6.901090809814234, 6.901194022153636, 6.901312666280207, 6.901399310269397, 6.9014854123635185, 6.901497969406648, 6.901642707071169, 6.901894745563452, 6.9018976207545615, 6.901912501982059, 6.901928395979998, 6.902018430398831, 6.9021183111029965, 6.902128921485467, 6.902212220320223, 6.902218046185163, 6.902318833268366, 6.902429524325575, 6.9027198896959865, 6.902844009299951, 6.902896107181709, 6.902939241541616, 6.903026359817406, 6.903032053058016, 6.903216391509129, 6.90344579080483, 6.90347179356835, 6.903490086691685, 6.903571452119298, 6.903931599222688, 6.904017502589929, 6.904180492475112, 6.90418432199852, 6.904194305508198, 6.904195897206962, 6.904225255964839, 6.904274289363644, 6.904337628707755, 6.904355187018464, 6.904442940811542, 6.904490275449037, 6.904513951769429, 6.904727176085449, 6.90473895824362, 6.904755618458156, 6.904889803412059, 6.904913250481511, 6.905271511903601, 6.905566603622823, 6.905633989592345, 6.905706635706727, 6.905719806380233, 6.9058411893559715, 6.9058539098633585, 6.905883736015045, 6.906408326025342, 6.906454919469995, 6.906522642352302, 6.906556114641146, 6.906563900568866, 6.906630691945831, 6.906836909215041, 6.906877256052257, 6.906906287439782, 6.906983152030334, 6.907052986547553, 6.907150176192589, 6.907199277021671, 6.907230479301723, 6.907496579972306, 6.90767904981194, 6.907719981014813, 6.907724012519279, 6.907754190104123, 6.907897733359912, 6.907949799906973, 6.907995533337595, 6.908009717171228, 6.908066918456399, 6.908245517924193, 6.908249962962013, 6.9083859684184885, 6.908511763700898, 6.908519617063228, 6.908573263319779, 6.908587595996674, 6.90858767069044, 6.908654567798958, 6.908744157077384, 6.908777525294887, 6.90882730234594, 6.9088347130398535, 6.908841762345733, 6.90894615358755, 6.909116601475143, 6.909145263404864, 6.909297613908598, 6.909566155634532, 6.909607940621218, 6.909607993868962, 6.909753496378229, 6.909830790365768, 6.909918331483766, 6.909918896479383, 6.910012343832692, 6.91019553790572, 6.910221282665835, 6.910258666177195, 6.910304597245871, 6.910390291222869, 6.910391045515196, 6.910391585430023, 6.910765777068028, 6.910770808951109, 6.911421233128432, 6.91149494030352, 6.9116710197997575, 6.911685298293323, 6.9117497059430555, 6.911933744507284, 6.911987053392174, 6.912359063615409, 6.912691098214925, 6.912724806253017, 6.912761717602475, 6.912797355026783, 6.913136623790933, 6.913312966500324, 6.913344564561115, 6.913440870440359, 6.9136105220496, 6.913624779071129, 6.913629478947024, 6.914131322961924, 6.914174549154816, 6.9144464619092485, 6.914504426116392, 6.914552360004461, 6.91455407692094, 6.914615310733825, 6.91481407724994, 6.914895613548353, 6.914895619291216, 6.914904205372147, 6.914999015985225, 6.915003632204716, 6.91525668716642, 6.915394857377835, 6.915589215008713, 6.915660095505842, 6.915682413948726, 6.9159594512065174, 6.916214144080228, 6.916362882906864, 6.916466730692404, 6.916470183057191, 6.916658265403918, 6.916873052262401, 6.91697784007585, 6.917024096515374, 6.917135702917439, 6.917149964147233, 6.91715490489848, 6.917734564184583, 6.917896459274619, 6.917940865977948, 6.917978093176672, 6.918090951400914, 6.91810312870661, 6.918190560396604, 6.91826755044, 6.918291932432915, 6.918363837069896, 6.918439054533826, 6.918610817487033, 6.918624208901428, 6.918679920385025, 6.918766231043147, 6.918808135454825, 6.91885908324329, 6.919164890208079, 6.919246193424428, 6.919399330309106, 6.919731436391733, 6.919754677230793, 6.91983926882424, 6.919840707783935, 6.919864090434372, 6.92022033642183, 6.920294655692648, 6.920349728783522, 6.920367931039775, 6.920421629596918, 6.920454991584219, 6.920494387161479, 6.920903245179561, 6.921092755378183, 6.9212861425640915, 6.921482243857559, 6.921631581705171, 6.921782317017144, 6.921910347214435, 6.922080423728757, 6.9221148600697475, 6.922150286133424, 6.922366252323398, 6.922444655034556, 6.922455377834952, 6.922471343178617, 6.922493445609248, 6.922596279325066, 6.922824895048734, 6.9228937649895865, 6.922966549160631, 6.923040450780549, 6.923106611916768, 6.923161394058002, 6.92332468111604, 6.92334696040811, 6.923360841263789, 6.923440602698741, 6.9236996466124685, 6.923770006727376, 6.9239272440988655, 6.924003801500175, 6.924057056049824, 6.924148232408417, 6.924390425051787, 6.924494169561717, 6.924534354349608, 6.924623488740657, 6.924772917231579, 6.925186643079533, 6.925307103297218, 6.925611752424237, 6.925755799940806, 6.9258668178627465, 6.925924328660084, 6.925998979403765, 6.926006870681328, 6.926259486150653, 6.926404661161736, 6.92658356652017, 6.926876704077173, 6.926895883333101, 6.92689738787314, 6.927098800755009, 6.927309686778383, 6.927932718154518, 6.927967094550146, 6.928059763894995, 6.928142953327782, 6.928174926449254, 6.928180739228879, 6.928364229125827, 6.928796383387684, 6.92885919769671, 6.9288993520135, 6.92903762401589, 6.929090295774033, 6.9291453187429966, 6.92914751370611, 6.929295364271859, 6.929296603195526, 6.929386636824051, 6.929430990609127, 6.929655185799241, 6.929684185914467, 6.929839245298044, 6.930205038580967, 6.930332977810409, 6.930447457967645, 6.930527233095919, 6.930644265844877, 6.9308610861631985, 6.930864101461752, 6.931032681531745, 6.931045996735728, 6.931073971924369, 6.931311167518872, 6.9314215212844585, 6.931474045402092, 6.931496632327745, 6.93170710157726, 6.931806565708292, 6.9318231102220915, 6.931963894457684, 6.9320949198317185, 6.932144835707132, 6.932210770820086, 6.932219672600984, 6.932220154796289, 6.932226867235502, 6.932240315283719, 6.932301583887802, 6.932448513932735, 6.932530377854451, 6.932573319448253, 6.932752914770781, 6.9328104735361755, 6.933017680931049, 6.9331253307449465, 6.933136548296465, 6.933363900399779, 6.933377011983519, 6.933387644496191, 6.933478414971983, 6.933558570349704, 6.933755121191632, 6.933815950910408, 6.933872654663744, 6.933955654344415, 6.933990287161479, 6.934128636510212, 6.934202608002801, 6.934316225551601, 6.934385050034824, 6.93454189426365, 6.934542446875189, 6.934625765040539, 6.934644092705725, 6.934849797278728, 6.935081862324281, 6.935238936539734, 6.935279296831734, 6.935300799149358, 6.935797775026872, 6.936043003609841, 6.936088660881753, 6.93615928173506, 6.936199422478994, 6.93652125138419, 6.936661031527164, 6.9371987768611945, 6.937231271873558, 6.937434040102063, 6.937536548890045, 6.93757255311284, 6.937574159781419, 6.937656440276668, 6.937688068270198, 6.937836009715032, 6.937840529681884, 6.938118027387746, 6.938199586906421, 6.938203552045734, 6.938531143740956, 6.938536153138077, 6.93872877080463, 6.938932039921992, 6.938997807756125, 6.939218091872878, 6.93927519360923, 6.939275970666447, 6.9392771394551085, 6.939354839186547, 6.939527267833386, 6.939692745609922, 6.939966894508862, 6.939991792462998, 6.940089739769325, 6.94009273086318, 6.940185280241239, 6.940233233433629, 6.9404185560692335, 6.940453777045174, 6.940624954427346, 6.9406965094139546, 6.940709318086074, 6.94079161055055, 6.940818805230813, 6.9411038763845445, 6.941150422529658, 6.941181607828078, 6.941190808965558, 6.941195306929245, 6.941246022485361, 6.941270600470684, 6.941348697545971, 6.941440536338054, 6.941448012589569, 6.941541826794186, 6.941545908119302, 6.941695675416752, 6.941701317634603, 6.9417132685160405, 6.941759456546875, 6.941871134743948, 6.942288266328171, 6.942318400010543, 6.942447243171128, 6.942547068224233, 6.942557299385773, 6.9425768859071075, 6.942657919699456, 6.942702367655585, 6.94290671084743, 6.942961197516773, 6.9429636633493885, 6.943024845151676, 6.9431012368018, 6.943276054458235, 6.943321326371435, 6.943496045620413, 6.943529668611957, 6.94353888984866, 6.943697663860533, 6.943864307530244, 6.943867809858603, 6.944084555400808, 6.944351516487145, 6.944369459247424, 6.944565176261475, 6.944585048847241, 6.944599769773351, 6.944744545003748, 6.944764566282915, 6.944784486123962, 6.944948170910726, 6.944967290336668, 6.945134055607782, 6.94523224146537, 6.945366010128737, 6.945402378901151, 6.9454573688367995, 6.945489268728009, 6.945696460316781, 6.945706764762815, 6.945897369403338, 6.945955823263982, 6.9459818535515065, 6.946102719207029, 6.946442949626275, 6.946555040684729, 6.946689760582468, 6.946709194044827, 6.947165140613895, 6.947319839924593, 6.947630095377402, 6.947819551141908, 6.947948746675841, 6.948085539665201, 6.9482666568738285, 6.948308644829718, 6.948320928117394, 6.948533654317944, 6.948639327370738, 6.94881642986116, 6.948836937711088, 6.948970895020524, 6.948976864306451, 6.949187559402791, 6.949366311124543, 6.949378982626005, 6.949395487836216, 6.949438285190064, 6.949526130595679, 6.94957986958233, 6.949582916476596, 6.949614571163581, 6.94975670037568, 6.949804463647479, 6.949861390753844, 6.950017534678069, 6.9500968246572805, 6.95010591996241, 6.950181273683353, 6.950299400296355, 6.950380745668003, 6.950587690302681, 6.9505894407115525, 6.950770320157679, 6.951010652482204, 6.9510264367038035, 6.951140917652633, 6.951145716250012, 6.9511491992822405, 6.95164845078093, 6.9516611702918665, 6.951697536093001, 6.951738929877283, 6.951814376223887, 6.9520518885383105, 6.952111645858935, 6.952112311980437, 6.952117044760748, 6.952210931476602, 6.952243374421231, 6.95231490402085, 6.952695116252065, 6.953015825207374, 6.953189356869631, 6.953217559955767, 6.95322336991547, 6.953311646691213, 6.953384056560881, 6.953474127929945, 6.9536170231789525, 6.953661324075322, 6.954084490061756, 6.954234915697473, 6.9543335952005485, 6.9544044060234995, 6.954622836684444, 6.954726536809018, 6.95489904681235, 6.95524314975655, 6.955244310288978, 6.95532692906048, 6.9557532779666325, 6.955777024053495, 6.955804431505859, 6.956062104897783, 6.956097915695722, 6.956234035538815, 6.956279806767665, 6.956387207944968, 6.9564034337421665, 6.956776540293236, 6.95707238043711, 6.957133058307075, 6.9572078468807215, 6.957291272264983, 6.957409219750671, 6.95757521124131, 6.957748657732209, 6.95784935649364, 6.957940115575587, 6.957957190854648, 6.958132848569637, 6.958185265425719, 6.958189625680888, 6.958407340542163, 6.958435909656587, 6.95865174999132, 6.958716711647706, 6.9587993429523225, 6.9588297422457925, 6.95885345504674, 6.958945586336845, 6.959005278623756, 6.959644300094877, 6.959814734347256, 6.9598166653565405, 6.960040137964499, 6.960115761945462, 6.960135417743222, 6.96013618325334, 6.960394518627892, 6.960405362320349, 6.96043020563909, 6.960466412473606, 6.960688820438727, 6.96084224092838, 6.960879054348714, 6.960938842010422, 6.960987395667883, 6.961180400429599, 6.961245961660764, 6.961375603660033, 6.961759189342463, 6.961862849295568, 6.961879590389872, 6.96190361868567, 6.962125015870486, 6.962181431019465, 6.962320652702201, 6.962598860766761, 6.9626092425286155, 6.9627261952109984, 6.962736563294796, 6.963346756076112, 6.963372676963107, 6.9633728319074, 6.963469568211134, 6.963820234936271, 6.963959554022902, 6.964072325131671, 6.9641098551271226, 6.964152709841463, 6.964187145597633, 6.964278171913635, 6.964303467444547, 6.964324711823831, 6.96435935032214, 6.964425974577762, 6.964658018352596, 6.964684677996098, 6.964737464050501, 6.964740355515854, 6.9647959844055825, 6.964819925241557, 6.965032784981388, 6.965065811871395, 6.965098438952018, 6.965502399056684, 6.9658780613624005, 6.9661345430890345, 6.966275603315817, 6.966555186215123, 6.9665977791248, 6.966643204281285, 6.966702829471902, 6.966723931447203, 6.966727363013528, 6.966869874960735, 6.966963629045641, 6.967071154409331, 6.967182889297675, 6.967262120864078, 6.967323919976258, 6.967360966265096, 6.967421046472523, 6.9675893444223185, 6.9677609737753095, 6.967797211137341, 6.968083451748071, 6.968158104707935, 6.968326634751914, 6.9686260083605935, 6.968734343545733, 6.968797994797901, 6.96883498997305, 6.968922202239898, 6.969192437786142, 6.9695360096777295, 6.969743904035649, 6.969779401861203, 6.969822857212046, 6.969846878021602, 6.9699594940191245, 6.970076715855495, 6.97011883125205, 6.970279421965882, 6.970441318916765, 6.970490233230615, 6.9705068552120135, 6.970777263110214, 6.970843772625769, 6.970855374090818, 6.97085924979978, 6.970917411529112, 6.97098208397104, 6.971029318474556, 6.971181344474441, 6.971340754830062, 6.971482836992996, 6.9715148651510885, 6.971515632099356, 6.971579687599583, 6.971583688545925, 6.972075478811327, 6.972386678212015, 6.973237818100776, 6.9733562160774865, 6.973361307298241, 6.973449442818289, 6.973704459156123, 6.973809760168084, 6.97382859832596, 6.973892335868747, 6.973964085878059, 6.973977792538762, 6.973995352214698, 6.9740598955500115, 6.974131590046652, 6.974136259161557, 6.974495959175409, 6.974669790098359, 6.974688057564344, 6.974781626798137, 6.974944887255752, 6.975087016403972, 6.975258654781414, 6.975291282843719, 6.975348346155608, 6.975401100348325, 6.975402391685682, 6.975406009104698, 6.975499155549967, 6.975503236762208, 6.975546014584979, 6.975552504754863, 6.975567431577551, 6.975577117634239, 6.975586070251235, 6.975629067270609, 6.97565075260926, 6.975671882772976, 6.976152269342626, 6.976326163901388, 6.976589475194115, 6.976665106975987, 6.9767732860641996, 6.976820703732488, 6.976881377310735, 6.976897286568244, 6.976943651997852, 6.976958839890943, 6.977250009728264, 6.977346608439447, 6.97740667215732, 6.977505372900311, 6.97756272198816, 6.977621525705746, 6.977984684151075, 6.978129024766512, 6.97820899414592, 6.978468876958549, 6.978575665502352, 6.978700597203906, 6.978983554001699, 6.978985368789451, 6.9794353152714965, 6.979545509555935, 6.979596433340806, 6.980110090875635, 6.980177796834375, 6.980198997461072, 6.980204929812888, 6.980265438105993, 6.980373216780564, 6.98056429235709, 6.980601676407663, 6.980857214609068, 6.980973984341601, 6.9809943164169, 6.981027796927488, 6.981042903379183, 6.981195453683295, 6.981229957830199, 6.981804838316781, 6.981837604403667, 6.982031940547196, 6.982058189679551, 6.982142665596497, 6.982185047971121, 6.982463864890141, 6.982558541528227, 6.982612924621846, 6.982636217411917, 6.9826384029873765, 6.982892780026656, 6.98291208732118, 6.98310372751687, 6.983152490638196, 6.983188629088297, 6.9833882085198224, 6.983547147001801, 6.983598986424798, 6.983720271484725, 6.9838872869994395, 6.9839030213555615, 6.983964731019925, 6.983973209462403, 6.9839787016504555, 6.984146045522661, 6.984221364935114, 6.984530867738231, 6.984674790786869, 6.985219310914608, 6.985376953047153, 6.98540261612124, 6.985583134563631, 6.985651092793143, 6.9857685811907, 6.985963818532707, 6.986049727168167, 6.986105012151716, 6.986155643492734, 6.9862659710466355, 6.986280189478271, 6.9863515124469595, 6.986356215529256, 6.986502622139639, 6.986603420931904, 6.986617456230441, 6.986881343855012, 6.9873362191461235, 6.987377642688852, 6.987648524054728, 6.987717062571207, 6.9877900493657705, 6.9877939416040835, 6.988083586908378, 6.988199309892087, 6.988302188953021, 6.988306306120735, 6.988352363534224, 6.988462433698376, 6.988564478163914, 6.988599395635658, 6.988705303828984, 6.98880419579584, 6.98883957371835, 6.988917024798388, 6.98893652644505, 6.988997659202958, 6.989079074356446, 6.989123844559785, 6.989209157772125, 6.989287782734171, 6.989335149047568, 6.989648653139494, 6.989913618749377, 6.990014812757032, 6.990128414250057, 6.990130255426148, 6.990387244554796, 6.990462327039239, 6.990569008080345, 6.990942909854424, 6.991044572881515, 6.991232733934436, 6.991239925452803, 6.991550905149943, 6.991645000904202, 6.992056540161929, 6.992074317306181, 6.992186716633646, 6.992389038966779, 6.992476877377365, 6.9924807255041355, 6.992545096608506, 6.992796367509389, 6.992806135839029, 6.992846587287183, 6.992922359503242, 6.992986583684661, 6.993025036605215, 6.993116061914791, 6.993460142900043, 6.99348868194471, 6.993591009287642, 6.993662480353658, 6.993707374358952, 6.994344512262361, 6.994721776616382, 6.9948794790521625, 6.995051112910109, 6.995092804201305, 6.995102131471933, 6.995128392939308, 6.995357900272856, 6.995460788896766, 6.995461339774322, 6.99548589068666, 6.995563450206612, 6.995694550449109, 6.995792713638538, 6.99594773946694, 6.996417164433671, 6.996425400423386, 6.996569055611602, 6.996653949033618, 6.9966586596794045, 6.997004558079774, 6.997174667924071, 6.997287147412712, 6.9972878168429355, 6.997443561930731, 6.997461858113179, 6.997576675813004, 6.9976160639447365, 6.9977048766000065, 6.997737988565943, 6.997879877250909, 6.998246178093675, 6.998401170367607, 6.998503817060479, 6.998807742139333, 6.998844944740438, 6.998976209618186, 6.999351540497996, 6.999528551442658, 6.99952956812006, 7.000644293057027, 7.000780634144308, 7.000976727730385, 7.0013096377191415, 7.001400991519454, 7.001528053796015, 7.002007172303899, 7.002203407208522, 7.0022900054837995, 7.002545531588734, 7.002702028201999, 7.0027547605576554, 7.002767767884842, 7.00291720710042, 7.003041768845137, 7.003115117073827, 7.00315861724451, 7.003161410738788, 7.003364883549097, 7.003453944954067, 7.003501697452904, 7.003586476450652, 7.0036995513903575, 7.003864935637658, 7.004075716501205, 7.004671074517881, 7.004825169739688, 7.004873453169029, 7.004881823061815, 7.004914089595722, 7.005134058584047, 7.005148278616548, 7.005275027405503, 7.005586034657021, 7.005733021206036, 7.005764988165811, 7.005860756041631, 7.006426734176823, 7.006430869921708, 7.006561819922561, 7.006565248352751, 7.007228491326672, 7.007416991941834, 7.007445531364705, 7.007490379678525, 7.007634275496501, 7.007799222547582, 7.008004469838751, 7.008084234745701, 7.008175423479414, 7.008267264046055, 7.00838523009672, 7.008415828065224, 7.008533121051288, 7.010196408880094, 7.0103087001393, 7.010355969878199, 7.010684248381355, 7.010877692469305, 7.010990432563336, 7.011438084045061, 7.011463418866022, 7.011743991313234, 7.011774951203978, 7.0118361865554055, 7.012111381274175, 7.012345987605, 7.0124276122963165, 7.012659494097102, 7.0127107035627825, 7.013217726767054, 7.0134921554213445, 7.013648374663018, 7.01388024692735, 7.013917283451977, 7.013924918174639, 7.014200499148245, 7.014660739462823, 7.014753708818181, 7.014847927598618, 7.01518212299262, 7.015552626835141, 7.0157683213552255, 7.015924808186585, 7.016061972293112, 7.016240452841443, 7.016443682668575, 7.016484585378233, 7.016584636653248, 7.016715892968259, 7.016870177854014, 7.01690827992168, 7.016913140943993, 7.017091788712556, 7.017690167026038, 7.017694836660138, 7.017780774400346, 7.017961174324888, 7.018064698758427, 7.018143068344676, 7.018192525133668, 7.018243135970612, 7.018248050497325, 7.018466264790922, 7.0185935558130375, 7.018712057601055, 7.01871505820492, 7.01881957106851, 7.019034374047127, 7.0191289752507915, 7.019237558031869, 7.019432724462156, 7.019451546650353, 7.0198218664627205, 7.0198761908182155, 7.019940145492249, 7.0200110699338145, 7.020090210311944, 7.020251722188683, 7.020368292812769, 7.020453620590364, 7.020714204943448, 7.020861305023811, 7.021002497473676, 7.021183508685857, 7.021353782666837, 7.0216997257103255, 7.022083756451771, 7.022139163996566, 7.0222690967763395, 7.022367154079783, 7.022578308556937, 7.022861072625078, 7.02305977947684, 7.023117906650177, 7.023134764876248, 7.023148290345064, 7.023399324033708, 7.023429841555605, 7.0236463517039835, 7.023718339531811, 7.023846815823602, 7.023854322062939, 7.024054570179503, 7.024174240130256, 7.024226370231587, 7.024409788123387, 7.024412745226358, 7.024418054851518, 7.024511802177578, 7.024534407833449, 7.024685607998078, 7.024772260403273, 7.024854042037991, 7.025011578514348, 7.025169638360637, 7.0252000031716975, 7.025365116725317, 7.025510761112745, 7.025652558539652, 7.025746009452188, 7.025973955224424, 7.026045049668277, 7.026431865470745, 7.026458440859668, 7.026478457504683, 7.026692078745385, 7.026739452510581, 7.026857124486476, 7.026940549074281, 7.027034909122519, 7.027314291576692, 7.027360154875106, 7.027493788695401, 7.02777465026339, 7.028470475722968, 7.028614216237587, 7.028658116102295, 7.028680681667542, 7.028700090620466, 7.0292428693521956, 7.029246146896698, 7.029370208275942, 7.029443316273079, 7.029638288417354, 7.029663720096345, 7.030247383703019, 7.030273624356306, 7.030581386252066, 7.030744575979048, 7.030806038384514, 7.030867864141454, 7.030879578259105, 7.031051851559662, 7.031085135857322, 7.031236465717701, 7.031416776320674, 7.031560309934416, 7.031791291810607, 7.031920105239863, 7.032033094658424, 7.032331432743055, 7.032393510182766, 7.032415424798679, 7.032519004203338, 7.03350213202923, 7.0338407423033, 7.0338732220802065, 7.034221370607047, 7.0342629239055166, 7.034507881835594, 7.034663423985689, 7.034858807807248, 7.0349075178336244, 7.034921829691537, 7.034968588793232, 7.035207261317986, 7.035337960525372, 7.0354353694334995, 7.035441846496746, 7.0355991867721945, 7.035652418711884, 7.03600168481292, 7.036356517838881, 7.036755168588124, 7.036918689543799, 7.037278109972096, 7.037343199337811, 7.03748147460891, 7.03749294941209, 7.037676808494967, 7.037848829897613, 7.03787373421115, 7.037967047707939, 7.038130010916838, 7.0384482058968585, 7.038647129462362, 7.038667253835997, 7.038745405409118, 7.038880017508392, 7.039176571237477, 7.039661635168213, 7.039723707209608, 7.040554604415662, 7.041181578572379, 7.041270033338876, 7.041362679751322, 7.041484743380324, 7.0414982388162475, 7.041592328633244, 7.041815876600454, 7.041927405006946, 7.042067813977644, 7.042458893135864, 7.042908746480394, 7.042939681668885, 7.042968003130876, 7.043612934530705, 7.0437591133852875, 7.043880650062674, 7.044513796673695, 7.044850855304113, 7.045086680765977, 7.0451263837663465, 7.045184223353403, 7.045395297923225, 7.045481506596337, 7.045547065695769, 7.045665112100374, 7.045673229337108, 7.045844241405593, 7.045944078482, 7.046003497541087, 7.046760966055851, 7.0468847190896975, 7.047097650580421, 7.047122516132534, 7.047156877595669, 7.0471838276600804, 7.047259148271058, 7.047621714179924, 7.047721032816868, 7.047734937089499, 7.047823657476775, 7.047861237204049, 7.047897770168672, 7.048097243572181, 7.048170335449901, 7.048219421427091, 7.0483951938642475, 7.0484256044108955, 7.048579926364635, 7.048659695503177, 7.04870496266531, 7.04888975657938, 7.0490106125217835, 7.049111334802073, 7.049199292669363, 7.049229172533121, 7.049254394508732, 7.0494570021908975, 7.04994791050467, 7.049953706950394, 7.050163011449451, 7.05035679191825, 7.050552991407892, 7.050791130424431, 7.051003086942897, 7.051180742235625, 7.05121071771331, 7.051234722491525, 7.051429898411547, 7.051567862221014, 7.0516141749402985, 7.051683905732509, 7.051879125323117, 7.05197384367156, 7.052085804758263, 7.052175748580677, 7.052200272518815, 7.0524588600697475, 7.052710640998051, 7.052790437366542, 7.052929419218819, 7.053204281591117, 7.053346266108118, 7.053385169179612, 7.053425361428889, 7.0540374252463955, 7.054058847756501, 7.054296061461567, 7.054348947224913, 7.054419002141254, 7.054862491648867, 7.054869186233051, 7.054986086414986, 7.05526620773019, 7.0554717781149545, 7.055645991199227, 7.055723953970946, 7.056037481381241, 7.056099846945456, 7.056228649368731, 7.056459952329942, 7.056526807676072, 7.057061926276601, 7.05713143479667, 7.0574126828241885, 7.057483342287126, 7.05748436452735, 7.05752154259971, 7.057576414181619, 7.057746610054576, 7.057950526681539, 7.058063912765556, 7.058141699486471, 7.058264836911442, 7.058282198410955, 7.058287810198446, 7.058663278441167, 7.058905803705646, 7.058940347359473, 7.059030966001713, 7.0591873469024575, 7.05930178458681, 7.0594594902334915, 7.0597255399311925, 7.059924815749076, 7.059945499097227, 7.0599602341706555, 7.0600806527414175, 7.060306213696324, 7.060378945495422, 7.0605791961244195, 7.060612161654152, 7.060763175368499, 7.061245344370106, 7.061679983445879, 7.061840883554487, 7.061861843066036, 7.062240711926558, 7.062393812992655, 7.06249203353555, 7.062957994204906, 7.063195120021656, 7.063288158967253, 7.063477346360918, 7.064126673151706, 7.0641277399493765, 7.064359572283719, 7.0650874642209756, 7.065281246701128, 7.06539693801142, 7.065759899816188, 7.065873190319362, 7.066196058176267, 7.066302915128517, 7.06632333991884, 7.066375728942551, 7.066617589288033, 7.066778487056864, 7.067499106487722, 7.068120116181165, 7.068136498018572, 7.068482934113936, 7.0695633196947245, 7.070797126218189, 7.070887266388518, 7.071838787138642, 7.071840295210306, 7.072051687518199, 7.072388161790147, 7.072396391906007, 7.072519860915969, 7.072590691864036, 7.072623115019929, 7.072637209942033, 7.072805006721185, 7.07295934135064, 7.073274775668929, 7.073464726503193, 7.073742292885868, 7.074357744093437, 7.074672588339828, 7.0746777879477705, 7.075025884524622, 7.075479667233458, 7.075662655891352, 7.075914225954663, 7.075946837740023, 7.075971399044013, 7.0760766908909805, 7.076468113827661, 7.076640977149481, 7.0768234505340075, 7.07688853624478, 7.077078182987578, 7.078250674034388, 7.078320847533501, 7.079535506353669, 7.079746196575344, 7.0797559331978945, 7.079765771472621, 7.080110255227283, 7.080221412096929, 7.080290597765574, 7.080497522981246, 7.080783294517879, 7.081040137626162, 7.081090086547719, 7.081251196239064, 7.081703015265097, 7.081862318797109, 7.081890926589108, 7.082674675118336, 7.082795472705658, 7.082843357949234, 7.083648166406075, 7.083795062578755, 7.08389899324072, 7.083930760477647, 7.084068661983239, 7.08432675310267, 7.084476352940631, 7.0847051984112674, 7.0847941755370245, 7.084855101007459, 7.085279228719857, 7.085916691702662, 7.086319282435538, 7.086569020945189, 7.087876370811051, 7.0880711128551, 7.088114482711883, 7.088338069032894, 7.088399549658715, 7.088407231308142, 7.089284062859927, 7.089291187183925, 7.089501653009733, 7.090415931184536, 7.090560016584447, 7.090567808500312, 7.090587516030799, 7.091105056778326, 7.091186328318227, 7.091538344042922, 7.091577223129939, 7.0918110310188816, 7.091992303972515, 7.092514037810296, 7.092860195605358, 7.0928933452843665, 7.093114340568102, 7.093208744681553, 7.093266235374182, 7.093911684646755, 7.0942855299014544, 7.094917070463455, 7.09499483513269, 7.0955338951572635, 7.095987532394416, 7.0961981639699925, 7.097681149437391, 7.0976915872163735, 7.097722807434509, 7.097851365337045, 7.097979262881732, 7.09809761250874, 7.098176501387317, 7.098235831341426, 7.09837949552911, 7.098459331748845, 7.098466331142753, 7.0984727752628425, 7.0989857868402755, 7.099205888640067, 7.100052308226731, 7.100591500578962, 7.100998407823287, 7.101475037488189, 7.101483644458355, 7.1018489058594385, 7.1020130019630665, 7.102328102619165, 7.102498834958749, 7.102551479269192, 7.102562388109547, 7.102599329428022, 7.102991992403636, 7.103264570004785, 7.103789556460862, 7.103795092904623, 7.103893342428719, 7.103912314571667, 7.104198832350659, 7.104494205743309, 7.104923169818299, 7.105025544863232, 7.105221532590239, 7.10536455989965, 7.105806368942403, 7.106184486766514, 7.106586781968143, 7.106883029254248, 7.107446636554857, 7.1075307654092725, 7.107947194421626, 7.108116964386805, 7.108178212895088, 7.108419924685898, 7.1088414173496615, 7.10891586965545, 7.109061744785288, 7.109087903458359, 7.109331975328726, 7.1093997143078305, 7.10940098154364, 7.10941841723407, 7.109476286129363, 7.109922052006945, 7.1105098605431465, 7.110659016404285, 7.110908293625503, 7.1110950104619395, 7.111517816877144, 7.111522547718304, 7.111530952272036, 7.112300284988297, 7.1124157105567445, 7.112458623527129, 7.112707454500592, 7.112728988603128, 7.113151035070099, 7.113358812607744, 7.113740807513872, 7.1145424215351065, 7.114857468061374, 7.115651132905124, 7.115677092170895, 7.116070173763222, 7.11615366982939, 7.116322229491033, 7.116763739527476, 7.116894873319863, 7.117715020409149, 7.117727023764895, 7.117886763073154, 7.118428858214822, 7.118517904953752, 7.118866197091947, 7.119230139484249, 7.119314157301959, 7.119433512243111, 7.119800356983122, 7.119911625499726, 7.121222002118422, 7.1215039747463225, 7.121553388004236, 7.121627814353646, 7.12164651609612, 7.121800314703698, 7.1220991117301615, 7.122414042433693, 7.122438781014796, 7.122476031850653, 7.123310856309551, 7.123399424048936, 7.123537527662865, 7.123785615662698, 7.125249699421104, 7.125384655061221, 7.125710970492344, 7.125966993304684, 7.126110146307848, 7.12725856044741, 7.127627027017641, 7.128138461329768, 7.1283017806065265, 7.128699052673383, 7.128974748226148, 7.1289763577521885, 7.1294562400188735, 7.129508452122475, 7.1297798917581625, 7.129891616730576, 7.130324114241405, 7.130386329004939, 7.131030367398539, 7.131280041543156, 7.13214501523139, 7.132379297910786, 7.133314639594391, 7.133522604195502, 7.134202282631008, 7.134969941480218, 7.1353768787316945, 7.135380434247108, 7.135740016565866, 7.136725335999985, 7.1376221492708956, 7.137850559595883, 7.138084816621551, 7.1391893172739715, 7.140131947554556, 7.140133853126541, 7.140292773703884, 7.140379228866514, 7.14060580522034, 7.140751123619259, 7.140911056004872, 7.14227798399806, 7.142811493399983, 7.1432593627677505, 7.144163814112895, 7.144395053403564, 7.144565178389117, 7.144815992659144, 7.144965239736439, 7.145204791310456, 7.145697382149876, 7.14578844082559, 7.145861157584333, 7.145954386698803, 7.145977607786855, 7.146030024627388, 7.146193688468144, 7.146366896011131, 7.146499992657013, 7.14650086320483, 7.147072003259211, 7.147127133970529, 7.14712790769846, 7.147257611193331, 7.147665972773945, 7.14779016049484, 7.147810275078469, 7.1482166199818495, 7.148645180591295, 7.150661490677603, 7.151067719772606, 7.151406265592481, 7.151470345146083, 7.1515743601708746, 7.151644142100058, 7.152343959296117, 7.153192336467037, 7.153376936175496, 7.153670384690299, 7.153919266463366, 7.154256344804597, 7.154773315018663, 7.154931849817971, 7.155097270858902, 7.155441617110148, 7.155462487472839, 7.155505210222874, 7.155591703083124, 7.155669238867005, 7.156738624609551, 7.156914471521597, 7.157550659104862, 7.157922960040307, 7.158328007207082, 7.158593117152051, 7.158783400352801, 7.160220636360311, 7.160589925790886, 7.161947495736514, 7.162990536749155, 7.16339347011992, 7.164297217852192, 7.164488092672361, 7.164536797485436, 7.165087280690348, 7.165362146362808, 7.165679802992659, 7.1661288468865, 7.168541100013403, 7.168550223596179, 7.16959573163061, 7.1700785064216905, 7.171536173010884, 7.171581119911935, 7.171717945805858, 7.1723825958574725, 7.172417202990075, 7.172712771804273, 7.173117718610943, 7.173527108444594, 7.174082230675891, 7.174306717271651, 7.174397848976356, 7.174634673592946, 7.174654095398393, 7.174768322854313, 7.1752807992564245, 7.176062807496994, 7.176682217238107, 7.1771459996150355, 7.1774824740667125, 7.177554331153852, 7.178005845729092, 7.179574834330134, 7.180358154496033, 7.180780321010524, 7.1813597433869525, 7.185097144689593, 7.185815463900703, 7.185856921837377, 7.186187052467016, 7.188075616141574, 7.18834261128548, 7.188359697044088, 7.18849173168647, 7.1888420063980405, 7.1897455770739995, 7.1908989976942275, 7.1917275450581934, 7.191759170994709, 7.19177973352474, 7.192473869324242, 7.194604438716264, 7.19580722251001, 7.196002504912282, 7.196228697341622, 7.1964825415531894, 7.196512718474036, 7.196794323172978, 7.1969222144539975, 7.1970033761991115, 7.197523002262913, 7.197787669835199, 7.197918909420217, 7.197936699338046, 7.198101669981788, 7.198568219866784, 7.198707821202698, 7.199809024199786, 7.200042096636573, 7.200526076130055, 7.20123309985556, 7.20180555299899, 7.201956687375579, 7.202379965289639, 7.20304761698956, 7.203602823504894, 7.204082954240741, 7.204100599526269, 7.20497187961219, 7.205020071051865, 7.205150286291885, 7.2053422125032505, 7.20602274500106, 7.209690091206312, 7.209960973480435, 7.213388760875076, 7.213552281918295, 7.21566038385688, 7.21718889452913, 7.219366432964952, 7.219700229679952, 7.219772793743278, 7.220190291627617, 7.220961183874134, 7.223498666708368, 7.224557119581431, 7.224606493933576, 7.224657576743812, 7.225342532192028, 7.2266481801160225, 7.22740440096278, 7.227727632358515, 7.228405320972448, 7.229842253516738, 7.230361250097886, 7.2334399865690955, 7.233515443646916, 7.235791346119181, 7.236550527774334, 7.237480338060256, 7.238781946915957, 7.2408206915283735, 7.243861001199558, 7.243982169417759, 7.244234370741846, 7.245483759342715, 7.246609038900254, 7.247673045730155, 7.248300361621731, 7.248693725531618, 7.249665950442284, 7.249835241389856, 7.250003751145213, 7.2505468605293535, 7.2525264925089195, 7.25349560897576, 7.253823631101513, 7.254959692513173, 7.255895234489032, 7.25706854741182, 7.2583721416952836, 7.258984560816485, 7.259272030582518, 7.260390437410925, 7.262735348208305, 7.263013273084111, 7.2630157123746555, 7.2630441122686165, 7.263226769528945, 7.263598396615184, 7.265120646712509, 7.266540702315634, 7.266752397529557, 7.266911760411849, 7.268759734034374, 7.269663701396189, 7.2699363280804095, 7.271266888828178, 7.273740795511273, 7.275026742804879, 7.278794587222179, 7.279578002360985, 7.28011970782532, 7.282073678176383, 7.282517451756155, 7.283818276902509, 7.28710244433328, 7.288264710033427, 7.288337091867881, 7.288664763641178, 7.290465052144379, 7.290509648726216, 7.29106712466159, 7.29158141299578, 7.292800227654144, 7.296322186441864, 7.29975661663727, 7.299757151590717, 7.2999707482332035, 7.299984082111762, 7.3045822493300765, 7.308624710928135, 7.308958262969048, 7.313071908191011, 7.314588097291458, 7.318019506040636, 7.318105359611223, 7.318949896056857, 7.3190861537909875, 7.320505347963132, 7.32146036619682, 7.322411592021125, 7.323216533935132, 7.323296972453077, 7.333703296509772, 7.333938087046093, 7.335160709380104, 7.337953944407787, 7.339496104096434, 7.34002691856027, 7.343658153025177, 7.3437680472890134, 7.349202240359279, 7.3499244397518355, 7.350328523725672, 7.351184413823368, 7.3534153563338, 7.354498324244345, 7.356419365975115, 7.356442817904162, 7.356492196526165, 7.35791507438837, 7.358995652501055, 7.360332241616483, 7.360338647921509, 7.360606866337419, 7.362321150955208, 7.373450381420154, 7.374811214617564, 7.377559890154749, 7.3814961206603265, 7.38423850125063, 7.384406551159832, 7.386663396429506, 7.387894905599328, 7.393144787361983, 7.395246859076132, 7.396396426415786, 7.396940042908851, 7.402281482914342, 7.402811704783679, 7.405610173830141, 7.411667172461889, 7.411795594885944, 7.4145668634630235, 7.415170546282757, 7.415781786191048, 7.417060098748138, 7.418360935447515, 7.421070031943973, 7.421845273807047, 7.421881645210986, 7.422918969475368, 7.42353011412755, 7.424080426917155, 7.426290493344513, 7.42887684108873, 7.42922230636269, 7.432005508767666, 7.433395906167271, 7.436448989248958, 7.4399795516688485, 7.440916998869007, 7.44120223709043, 7.441946146649907, 7.44223197794116, 7.444493500639494, 7.4450920510542, 7.446169969855904, 7.447420311688636, 7.448139798804142, 7.448163554653029, 7.45046007697389, 7.451078695790443, 7.454973084496825, 7.456060382148211, 7.456633065587936, 7.456826042852294, 7.460955040802101, 7.462973334478436, 7.466113616753738, 7.466328390134684, 7.4667055776776685, 7.47011990385168, 7.470549753401683, 7.471138026427913, 7.471401828710569, 7.471513493102063, 7.471558211725359, 7.473515957939054, 7.47393968235653, 7.476941723341607, 7.480844191310603, 7.482160895624316, 7.483907650606072, 7.484313436975016, 7.484979456044245, 7.485090202694003, 7.485290975177795, 7.488181394457089, 7.490827052300555, 7.491137390523997, 7.492990546690695, 7.493216916535879, 7.493561566279607, 7.49428307494542, 7.496269821780345, 7.498913163261525, 7.504033296376993, 7.506550456651809, 7.507013105725919, 7.508105746034359, 7.5098994565105635, 7.512289152839784, 7.512713908163586, 7.514915600770528, 7.516239737427988, 7.516686781905735, 7.518821936972793, 7.520466836196219, 7.523712715259445, 7.523886217873075, 7.524407772626835, 7.524913228569617, 7.525533222438834, 7.525736663527838, 7.526171901915323, 7.52654555513707, 7.527109211790458, 7.5286934337037925, 7.5295437450660865, 7.531367122460544, 7.532259924144084, 7.53250329989527, 7.532938147319789, 7.533306116675619, 7.534564992172618, 7.534910958912716, 7.537802662356699, 7.539155912800726, 7.539159653677737, 7.539425860032152, 7.540518538053177, 7.540539376292322, 7.542454645469774, 7.5443608837487695, 7.546126755419883, 7.546525581226527, 7.5479578701573065, 7.548187929252307, 7.550616199752904, 7.551403218748106, 7.552149610469871, 7.553347204300804, 7.5533957441889665, 7.553404278642554, 7.556856974629139, 7.55789313167677, 7.557941109286469, 7.559859963998134, 7.561880098260443, 7.562462964660481, 7.56332279705645, 7.563859743169704, 7.572410054736, 7.572718645727498, 7.5740672536153575, 7.575042783166374, 7.576819051951239, 7.57707163120634, 7.57920200162699, 7.580611109037484, 7.5809597575829315, 7.581169683043262, 7.582502441249537, 7.5831692638238914, 7.583296776999962, 7.58363308271699, 7.584818268807103, 7.585574651143718, 7.5864562937844715, 7.586526395910191, 7.587933081202191, 7.591315174312378, 7.592339962618734, 7.593624600208786, 7.594844300685295, 7.595863839072184, 7.596144426819592, 7.597139925812682, 7.597170685025137, 7.597224248184057, 7.597488876467138, 7.598162867994719, 7.598771583152441, 7.599260428835818, 7.599635259918951, 7.600681720135224, 7.600884209983053, 7.601761957002249, 7.601804357673571, 7.601995687761508, 7.602927889302086, 7.6031176013707675, 7.603154165443258, 7.6045890339651905, 7.607532634173587, 7.607635377399161, 7.607669801365434, 7.607938832536261, 7.610667266735798, 7.610777378996911, 7.611151113541683, 7.6113358565598555, 7.61171697412073, 7.612090662533741, 7.6135529523091146, 7.614665755559863, 7.6148144345005235, 7.620398639378778, 7.621895809393527, 7.623737263508869, 7.623854585637716, 7.6244887280539935, 7.624660465881997, 7.625330999011142, 7.627231843353413, 7.627656536480738, 7.6276803059196885, 7.628863930300678, 7.629474420662007, 7.6298002419771, 7.63042702602656, 7.631094940566654, 7.631376344761856, 7.631700501085953, 7.6321360314725055, 7.6329911824670456, 7.633548562650274, 7.633681040021891, 7.633760208668388, 7.634379806916936, 7.6346009290393795, 7.635589303576563, 7.6360620739211384, 7.636383393306868, 7.637392638688016, 7.637654820185089, 7.639191856208017, 7.639589515237158, 7.6419423471014385, 7.642249712012651, 7.642367804334494, 7.648034618210993, 7.648212148818047, 7.64872785976095, 7.649309375640041, 7.650105201174759, 7.650700831106327, 7.651205316031856, 7.651481768087875, 7.652215239136687, 7.652454776378234, 7.653689866576177, 7.653806729573618, 7.654890522546198, 7.65505578880612, 7.656239390954514, 7.656435149219134, 7.657603806124427, 7.658176230334412, 7.6591620886413265, 7.659674958336353, 7.660092067033076, 7.660869417084753, 7.661296978582482, 7.6617671109141385, 7.663121078094372, 7.66321256084019, 7.663318763577556, 7.66620565244372, 7.666929306832755, 7.667433774933405, 7.668055381122474, 7.669029987609, 7.670172562557097, 7.670408410858148, 7.67107236020965, 7.671232509784727, 7.6728106038432635, 7.673213871648859, 7.675401020267137, 7.675902832036465, 7.676030379380559, 7.676078143478378, 7.676437676873948, 7.67672896530604, 7.677344570980467, 7.682804139899364, 7.682896437098112, 7.683944142339576, 7.684064215547284, 7.68872779153854, 7.6889567826932215, 7.689908306756165, 7.689970130787852, 7.692518629724228, 7.69339482493344, 7.693607966716773, 7.694608135531503, 7.695832249005615, 7.696389266123239, 7.696676658625429, 7.696732146652152, 7.697006440077721, 7.701615096895275, 7.703654542608027, 7.7037429290469746, 7.7042224993244774, 7.708172458049302, 7.708756284176559, 7.709869978450297, 7.710310499166268, 7.71096156673357, 7.7117426707680705, 7.712852799568799, 7.7139157613946, 7.71660829077748, 7.716612467750182, 7.716843764084568, 7.717094892886693, 7.717310512551597, 7.717839877654578, 7.718389564525908, 7.718574350750144, 7.718729901825873, 7.718771137994787, 7.719589552077123, 7.720338992945528, 7.720659935970348, 7.721501630479161, 7.72207192681269, 7.72269591736402, 7.723435666930145, 7.724027206924626, 7.724816899587426, 7.725506380269674, 7.7259288002121025, 7.729491853873546, 7.734777100402373, 7.735240683306922, 7.735921943496734, 7.737455601041094, 7.73772845086796, 7.738491754428649, 7.739685501879884, 7.740673471065468, 7.741829932325272, 7.74276299048558, 7.742887355054967, 7.743278127794685, 7.744415991484052, 7.744570503289322, 7.7450371414423, 7.745128697022457, 7.745336253676408, 7.745498549741335, 7.747940382919605, 7.749288652337333, 7.749430589151941, 7.7497617649432495, 7.750683471720892, 7.7512027607667005, 7.751266175063278, 7.751368052683005, 7.753883381503521, 7.754252816652346, 7.756485160608284, 7.7571089798988515, 7.758133679887453, 7.759788598763629, 7.7602449764865415, 7.760292741431573, 7.763445097015596, 7.763624659172368, 7.764298239732922, 7.7644264106177525, 7.765037658040148, 7.765696070235945, 7.766177521517858, 7.76842925890974, 7.768765848202028, 7.768956178065121, 7.769430167322393, 7.769975167369006, 7.772800672739952, 7.772882052812193, 7.773848181690598, 7.7739601287762845, 7.773967902819596, 7.77408917688867, 7.774684636214297, 7.775848992773756, 7.776315308317655, 7.777086222021344, 7.7775338142672785, 7.777886067828823, 7.778148175568263, 7.7785180121764474, 7.778561521209596, 7.781027671179531, 7.781506102808241, 7.781742201083618, 7.782159497621093, 7.7824095541790035, 7.783039578246482, 7.7855224222649735, 7.786941795738399, 7.787442111076023, 7.791600676105675, 7.792955867984532, 7.79314870902429, 7.794176424755184, 7.794681444647332, 7.798363430864122, 7.800527885606054, 7.800730869289403, 7.803037178064464, 7.803186398477197, 7.803283983985849, 7.803308767412769, 7.804786853228935, 7.805367066080946, 7.805818714380496, 7.809566129210436, 7.811194677415039, 7.811716133116953, 7.8118832391519675, 7.814002184347423, 7.814096025530575, 7.814115618602426, 7.814281642134143, 7.8168287846278774, 7.8196152047297245, 7.819951136522446, 7.8209305117293395, 7.821485316382791, 7.821708360458583, 7.822849237032145, 7.823522353422685, 7.82510886615153, 7.825488325094387, 7.8258039261826395, 7.827729177568484, 7.8283745970403995, 7.829221693678569, 7.833481519449189, 7.834143305717595, 7.8358676557103895, 7.836511376626925, 7.839649016136675, 7.839677796007019, 7.840486158848256, 7.840529324004865, 7.8407081485507035, 7.840893649014063, 7.8409563379291844, 7.84111768250409, 7.8415636750756486, 7.842005018328792, 7.842096983120472, 7.842156579839733, 7.842222854969787, 7.842791051316951, 7.8429205080177, 7.843881092178495, 7.844203144064445, 7.845149704882792, 7.8451779510089885, 7.84582142157278, 7.846846859765546, 7.84691155492695, 7.847583130263697, 7.847695556271038, 7.848449705002772, 7.851533824704794, 7.853586153560298, 7.855145051955969, 7.855997467714184, 7.856560565298063, 7.857274892409659, 7.85747384633061, 7.861384729359942, 7.861584091766096, 7.865145113417606, 7.867504746918687, 7.8675856595012075, 7.867840682323035, 7.8681694344871955, 7.870289095274136, 7.871872836246803, 7.8736839709743505, 7.873968985883339, 7.874147562678747, 7.874610615445259, 7.875780661713876, 7.876731611930291, 7.880066958979575, 7.881760173509185, 7.882797292248795, 7.882930180795344, 7.884429110676897, 7.8856395964617905, 7.88615153158048, 7.887204930240088, 7.887397510223944, 7.887853144937469, 7.887924143940791, 7.888208133563707, 7.88962997102997, 7.890670140227988, 7.892653077561419, 7.893239082093679, 7.895034088301276, 7.8956490022544035, 7.899527214812699, 7.899935739597273, 7.9000025500051985, 7.900258056352981, 7.900289675326425, 7.900590049262832, 7.90095568429678, 7.9024639411806215, 7.902779527143179, 7.905436271517836, 7.905889337752565, 7.90691208136078, 7.907825151717569, 7.907922974431447, 7.90841461714343, 7.908721305199717, 7.908767663987426, 7.908838984668672, 7.911490129405923, 7.912170472087383, 7.913622115562062, 7.916544452454452, 7.9166482745732605, 7.918125047683283, 7.918211548939766, 7.919757217972005, 7.924090402226564, 7.924970989812102, 7.926617507410286, 7.927051573941136, 7.928845531254926, 7.93050597758019, 7.931156241250484, 7.931163353214628, 7.931740417053076, 7.931999472551317, 7.932851248339631, 7.9334078752899835, 7.935830982090623, 7.939675358764901, 7.940987015378158, 7.941141253988228, 7.94134419496377, 7.942849347777073, 7.943476283806357, 7.944633145075409, 7.944880116431219, 7.944922714410334, 7.945526668325195, 7.945863868633527, 7.94736816546545, 7.948401792536681, 7.949315672532023, 7.955527576278134, 7.956643694925238, 7.959319412593273, 7.9600756397266235, 7.961261971805902, 7.962621177686356, 7.963000661348395, 7.963212152394606, 7.963394293291466, 7.96377272827944, 7.966343889878025, 7.969160440757852, 7.970573903308743, 7.971441774518881, 7.9720224928755545, 7.972372217551142, 7.9729361920791355, 7.975669589374418, 7.97575546653015, 7.977911690383949, 7.9779354263116256, 7.981756999719959, 7.982862385222615, 7.986438444828701, 7.987347468401702, 7.988187290023913, 7.994224849401477, 7.994759564839778, 7.996615420219231, 7.997534874024678, 7.998661249789238, 7.999517511638869, 8.000992081918557, 8.001333490796567, 8.001372766989771, 8.0099273618217, 8.010169805079332, 8.015014114085686, 8.015698236031598, 8.019527436589058, 8.020740640303545, 8.02191046710848, 8.02436308381459, 8.025786904669346, 8.02698209661461, 8.027207963379851, 8.030942878223934, 8.031242378977186, 8.034921263454383, 8.03640838873259, 8.044889669756172, 8.049263608642608, 8.052417918063837, 8.053576139715512, 8.057834861744551, 8.05962570338389, 8.05985765375989, 8.06568121967821, 8.067581188739641, 8.069031337179624, 8.070433798265354, 8.072757882082406, 8.076765554422884, 8.085129753226655, 8.087806801337521, 8.095742556572842, 8.110647988770303, 8.125257325622034, 8.127465596268829, 8.13613669912108, 8.145817532424552, 8.160287377821824, 8.166104449832622, 8.170875046960262, 8.175009086826867, 8.200952166569207, 8.214682419437525, 8.223579529254435, 8.363821692009536, 8.389115103272507, 8.48693169936116]\n",
            "4\n",
            "0.025\n",
            "0.001\n",
            "[RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=1,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=3,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=4,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=6,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=7,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=8,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=9,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=1,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=3,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=4,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=6,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=7,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=8,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=9,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
            "           max_features='auto', max_leaf_nodes=None,\n",
            "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "           min_samples_leaf=1, min_samples_split=2,\n",
            "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
            "           oob_score=False, random_state=0, verbose=0, warm_start=False)]\n",
            "[[6.803098682573266, 1], [6.839276232047231, 2], [6.857204183901086, 3], [6.836346419083367, 4], [6.878602736346901, 5], [6.91149494030352, 6], [6.9658780613624005, 7], [7.006430869921708, 8], [7.032519004203338, 9], [7.804786853228935, 9], [6.764436377510151, 1], [6.770188777803031, 2], [6.758299909607694, 3], [6.746356458320904, 4], [6.743872779911043, 5], [6.771794527689318, 6], [6.818682301982888, 7], [6.856831884305291, 8], [6.933955654344415, 9], [7.498913163261525, 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6GqDSTkMeyP",
        "outputId": "cd6fca2e-19b9-4fac-d4d8-fcb3cac150d8"
      },
      "source": [
        "from pyAudioAnalysis import audioBasicIO\n",
        "from pyAudioAnalysis import audioTrainTest as aT\n",
        "\n",
        "sampling_rate, signal = audioBasicIO.read_audio_file('./ADReSS data/ADReSS-IS2020-data/train/norm_wave_sorted/real mmse scores/'+'S156-306-25879-28140-1-700-2029.wav')\n",
        "print(sampling_rate*aT.shortTermWindow)\n",
        "print(sampling_rate*aT.shortTermStep)\n",
        "print(signal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2205.0\n",
            "2205.0\n",
            "[-97 115 229 ...  27  27  19]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K_rBEBqHtwA",
        "outputId": "4f3bf9e2-dd8c-4791-d8f6-3e32b1b9810d"
      },
      "source": [
        "from pyAudioAnalysis import MidTermFeatures as aF\n",
        "from pyAudioAnalysis import audioTrainTest as aT\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "import glob\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "def createProperData(baseDir, shortTermWindow, shortTermStep):\n",
        "  print('shortTermWindow: {} \\nshortTermStep: {}'.format(shortTermWindow,shortTermStep))\n",
        "  folder_name = baseDir \n",
        "  features2, _, filenames2 = aF.directory_feature_extraction(baseDir, 1.0, 1.0,shortTermWindow, shortTermStep)\n",
        "\n",
        "  f = features2\n",
        "  fn = _\n",
        "  feature_names = filenames2\n",
        "  features_Real = []\n",
        "  class_names = []\n",
        "  file_names = []\n",
        "\n",
        "  for i, d in enumerate([baseDir]):\n",
        "    if f.shape[0] > 0:\n",
        "        # if at least one audio file has been found in the provided folder:\n",
        "        features_Real.append(f)\n",
        "        file_names.append(fn)\n",
        "        if d[-1] == os.sep:\n",
        "            class_names.append(d.split(os.sep)[-2])\n",
        "        else:\n",
        "            class_names.append(d.split(os.sep)[-1])\n",
        "\n",
        "  features = features_Real\n",
        "  filenames = file_names\n",
        "\n",
        "  features = features[0]\n",
        "  filenames = [os.path.basename(f) for f in filenames[0]]\n",
        "  f_final = []\n",
        "\n",
        "  # Read CSVs:\n",
        "\n",
        "  csv_files = glob.glob(folder_name + os.sep + \"*.csv\")\n",
        "  regression_labels = []\n",
        "  regression_names = []\n",
        "  f_final = []\n",
        "  for c in csv_files:\n",
        "      cur_regression_labels = []\n",
        "      f_temp = []\n",
        "      # open the csv file that contains the current target value's annotations\n",
        "      with open(c, 'rt') as csvfile:\n",
        "          csv_reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "          for row in csv_reader:\n",
        "              if len(row) == 2:\n",
        "                  # ... and if the current filename exists\n",
        "                  # in the list of filenames\n",
        "                  if row[0] in filenames:\n",
        "                      index = filenames.index(row[0])\n",
        "                      cur_regression_labels.append(float(row[1]))\n",
        "                      f_temp.append(features[index, :])\n",
        "                  else:\n",
        "                      print(\"Warning: {} not found \"\n",
        "                            \"in list of files.\".format(row[0]))\n",
        "              else:\n",
        "                  print(\"Warning: Row with unknown format in regression file\")\n",
        "      f_final.append(np.array(f_temp))\n",
        "      # cur_regression_labels is the list of values\n",
        "      # for the current regression problem\n",
        "      regression_labels.append(np.array(cur_regression_labels))\n",
        "      # regression task name\n",
        "      regression_names.append(os.path.basename(c).replace(\".csv\", \"\"))\n",
        "      if len(features) == 0:\n",
        "          print(\"ERROR: No data found in any input folder!\")\n",
        "\n",
        "  #TRAIN SPLIT THE DATA SET\n",
        "\n",
        "  features_norm, mean, std = aT.normalize_features([f_final[0]])\n",
        "\n",
        "  first8 = []\n",
        "  for feature in features_norm[0]:\n",
        "    temp = []\n",
        "    temp.extend(feature[0:7])\n",
        "    temp.extend(feature[34:41])\n",
        "    temp.extend(feature[68:75])\n",
        "    temp.extend(feature[102:109])\n",
        "    print(len(temp))\n",
        "    first8.append(temp)\n",
        "  print(len(first8))\n",
        "  first8 = np.array([first8])\n",
        "  mfccFeatures = []\n",
        "  for feature in features_norm[0]:\n",
        "    temp = []\n",
        "    temp.extend(feature[8:20])\n",
        "    temp.extend(feature[42:54])\n",
        "    temp.extend(feature[76:88])\n",
        "    temp.extend(feature[110:122])\n",
        "    #,42:54,76:88,110:122]\n",
        "    #print(len(temp))\n",
        "    mfccFeatures.append(temp)\n",
        "  #print(len(mfccFeatures))\n",
        "  mfccFeatures = np.array([mfccFeatures])\n",
        "  chroma = []\n",
        "  for feature in features_norm[0]:\n",
        "    temp = []\n",
        "    temp.extend(feature[21:33])\n",
        "    temp.extend(feature[55:67])\n",
        "    temp.extend(feature[89:101])\n",
        "    temp.extend(feature[123:135])\n",
        "    chroma.append(temp)\n",
        "  chroma = np.array([chroma])\n",
        "  return [features_norm[0], first8,mfccFeatures,chroma], regression_labels[0]\n",
        "\n",
        "\n",
        "def train4Models(f_train, l_train, f_test, l_test,shortTermWindow, shortTermStep):\n",
        "  actual = l_test\n",
        "  classifiers = []\n",
        "  rmses = []\n",
        "\n",
        "  #RMSE SCORE\n",
        "\n",
        "  neigh = KNeighborsRegressor(n_neighbors=5)\n",
        "  neigh.fit(f_train, l_train)\n",
        "\n",
        "  predicted = []\n",
        "  for itest, fTest in enumerate(f_test):\n",
        "    R = neigh.predict(fTest.reshape(1,-1))[0]\n",
        "    predicted.append(R)\n",
        "  mse = sklearn.metrics.mean_squared_error(actual, predicted)\n",
        "  rmse = math.sqrt(mse)\n",
        "\n",
        "  rmses.append(rmse)\n",
        "  classifiers.append(neigh)\n",
        "\n",
        "  disNeigh = KNeighborsRegressor(n_neighbors=5, weights='distance')\n",
        "  disNeigh.fit(f_train, l_train)\n",
        "\n",
        "  predicted = []\n",
        "  for itest, fTest in enumerate(f_test):\n",
        "    R = disNeigh.predict(fTest.reshape(1,-1))[0]\n",
        "    predicted.append(R)\n",
        "  mse = sklearn.metrics.mean_squared_error(actual, predicted)\n",
        "  rmse = math.sqrt(mse)\n",
        "\n",
        "  rmses.append(rmse)\n",
        "  classifiers.append(disNeigh)\n",
        "\n",
        "  return [shortTermWindow, shortTermStep, classifiers, rmses]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def tryDifferentWindows():\n",
        "  listOFEverything22 = []\n",
        "  notFirstRun = False\n",
        "  valMet = True\n",
        "  listOFEverything = listOFEverything22.copy()\n",
        "  shortTermStepList = [.001, 0.005, 0.01,0.015, 0.02]\n",
        "  for shortTermStep in shortTermStepList:\n",
        "    for shortTermWindow in range(25,105,5):\n",
        "      lof = len(listOFEverything22) - 1\n",
        "      print(shortTermStep)\n",
        "      print(str(shortTermWindow) + \"\\n\")\n",
        "      trainDir = './real mmse scores'\n",
        "      testDir = './test-mmse'\n",
        "      shortTermWindow = float(shortTermWindow)/1000\n",
        "      \n",
        "      temp = pickle.load(open(\"./pickles/train/SS{}-SW{}.pickle\".format(shortTermStep, shortTermWindow), 'rb'))\n",
        "      trainFeats = temp[0]\n",
        "      trainLabels = temp[1]\n",
        "\n",
        "      temp = pickle.load(open(\"./pickles/test/SS{}-SW{}.pickle\".format(shortTermStep, shortTermWindow), 'rb'))\n",
        "      testFeats = temp[0]\n",
        "      testLabels = temp[1]\n",
        "\n",
        "\n",
        "      for i in range(4):\n",
        "        listOFEverything.append(train4Models(np.matrix(trainFeats[i]), trainLabels,np.matrix(testFeats[i]), testLabels,shortTermWindow, shortTermStep))\n",
        "\n",
        "      \n",
        "\n",
        "      with open(\"neigh.pickle\", 'wb') as handle:\n",
        "        pickle.dump(listOFEverything, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  return listOFEverything\n",
        "\n",
        "listOFEverything = tryDifferentWindows()\n",
        "\n",
        "with open(\"neigh.pickle\", 'wb') as handle:\n",
        "  pickle.dump(listOFEverything, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.001\n",
            "25\n",
            "\n",
            "0.001\n",
            "30\n",
            "\n",
            "0.001\n",
            "35\n",
            "\n",
            "0.001\n",
            "40\n",
            "\n",
            "0.001\n",
            "45\n",
            "\n",
            "0.001\n",
            "50\n",
            "\n",
            "0.001\n",
            "55\n",
            "\n",
            "0.001\n",
            "60\n",
            "\n",
            "0.001\n",
            "65\n",
            "\n",
            "0.001\n",
            "70\n",
            "\n",
            "0.001\n",
            "75\n",
            "\n",
            "0.001\n",
            "80\n",
            "\n",
            "0.001\n",
            "85\n",
            "\n",
            "0.001\n",
            "90\n",
            "\n",
            "0.001\n",
            "95\n",
            "\n",
            "0.001\n",
            "100\n",
            "\n",
            "0.005\n",
            "25\n",
            "\n",
            "0.005\n",
            "30\n",
            "\n",
            "0.005\n",
            "35\n",
            "\n",
            "0.005\n",
            "40\n",
            "\n",
            "0.005\n",
            "45\n",
            "\n",
            "0.005\n",
            "50\n",
            "\n",
            "0.005\n",
            "55\n",
            "\n",
            "0.005\n",
            "60\n",
            "\n",
            "0.005\n",
            "65\n",
            "\n",
            "0.005\n",
            "70\n",
            "\n",
            "0.005\n",
            "75\n",
            "\n",
            "0.005\n",
            "80\n",
            "\n",
            "0.005\n",
            "85\n",
            "\n",
            "0.005\n",
            "90\n",
            "\n",
            "0.005\n",
            "95\n",
            "\n",
            "0.005\n",
            "100\n",
            "\n",
            "0.01\n",
            "25\n",
            "\n",
            "0.01\n",
            "30\n",
            "\n",
            "0.01\n",
            "35\n",
            "\n",
            "0.01\n",
            "40\n",
            "\n",
            "0.01\n",
            "45\n",
            "\n",
            "0.01\n",
            "50\n",
            "\n",
            "0.01\n",
            "55\n",
            "\n",
            "0.01\n",
            "60\n",
            "\n",
            "0.01\n",
            "65\n",
            "\n",
            "0.01\n",
            "70\n",
            "\n",
            "0.01\n",
            "75\n",
            "\n",
            "0.01\n",
            "80\n",
            "\n",
            "0.01\n",
            "85\n",
            "\n",
            "0.01\n",
            "90\n",
            "\n",
            "0.01\n",
            "95\n",
            "\n",
            "0.01\n",
            "100\n",
            "\n",
            "0.015\n",
            "25\n",
            "\n",
            "0.015\n",
            "30\n",
            "\n",
            "0.015\n",
            "35\n",
            "\n",
            "0.015\n",
            "40\n",
            "\n",
            "0.015\n",
            "45\n",
            "\n",
            "0.015\n",
            "50\n",
            "\n",
            "0.015\n",
            "55\n",
            "\n",
            "0.015\n",
            "60\n",
            "\n",
            "0.015\n",
            "65\n",
            "\n",
            "0.015\n",
            "70\n",
            "\n",
            "0.015\n",
            "75\n",
            "\n",
            "0.015\n",
            "80\n",
            "\n",
            "0.015\n",
            "85\n",
            "\n",
            "0.015\n",
            "90\n",
            "\n",
            "0.015\n",
            "95\n",
            "\n",
            "0.015\n",
            "100\n",
            "\n",
            "0.02\n",
            "25\n",
            "\n",
            "0.02\n",
            "30\n",
            "\n",
            "0.02\n",
            "35\n",
            "\n",
            "0.02\n",
            "40\n",
            "\n",
            "0.02\n",
            "45\n",
            "\n",
            "0.02\n",
            "50\n",
            "\n",
            "0.02\n",
            "55\n",
            "\n",
            "0.02\n",
            "60\n",
            "\n",
            "0.02\n",
            "65\n",
            "\n",
            "0.02\n",
            "70\n",
            "\n",
            "0.02\n",
            "75\n",
            "\n",
            "0.02\n",
            "80\n",
            "\n",
            "0.02\n",
            "85\n",
            "\n",
            "0.02\n",
            "90\n",
            "\n",
            "0.02\n",
            "95\n",
            "\n",
            "0.02\n",
            "100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-T2_AcctNtH"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import random\n",
        "import pandas as pd\n",
        "import glob\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import math\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import autokeras as ak\n",
        "import pickle\n",
        "import os\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2cRMHR5u51M",
        "outputId": "1b0e671c-4657-4c4e-8bcb-15be4f91e488"
      },
      "source": [
        "from pyAudioAnalysis import MidTermFeatures as aF\n",
        "from pyAudioAnalysis import audioTrainTest as aT\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import random\n",
        "import pandas as pd\n",
        "import glob\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import math\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import autokeras as ak\n",
        "import pickle\n",
        "import os\n",
        "import math\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "def createProperData(baseDir, shortTermWindow, shortTermStep):\n",
        "  print('shortTermWindow: {} \\nshortTermStep: {}'.format(shortTermWindow,shortTermStep))\n",
        "  folder_name = baseDir \n",
        "  features2, _, filenames2 = aF.directory_feature_extraction(baseDir, 1.0, 1.0,shortTermWindow, shortTermStep)\n",
        "\n",
        "  f = features2\n",
        "  fn = _\n",
        "  feature_names = filenames2\n",
        "  features_Real = []\n",
        "  class_names = []\n",
        "  file_names = []\n",
        "\n",
        "  for i, d in enumerate([baseDir]):\n",
        "    if f.shape[0] > 0:\n",
        "        # if at least one audio file has been found in the provided folder:\n",
        "        features_Real.append(f)\n",
        "        file_names.append(fn)\n",
        "        if d[-1] == os.sep:\n",
        "            class_names.append(d.split(os.sep)[-2])\n",
        "        else:\n",
        "            class_names.append(d.split(os.sep)[-1])\n",
        "\n",
        "  features = features_Real\n",
        "  filenames = file_names\n",
        "\n",
        "  features = features[0]\n",
        "  filenames = [os.path.basename(f) for f in filenames[0]]\n",
        "  f_final = []\n",
        "\n",
        "  # Read CSVs:\n",
        "\n",
        "  csv_files = glob.glob(folder_name + os.sep + \"*.csv\")\n",
        "  regression_labels = []\n",
        "  regression_names = []\n",
        "  f_final = []\n",
        "  for c in csv_files:\n",
        "      cur_regression_labels = []\n",
        "      f_temp = []\n",
        "      # open the csv file that contains the current target value's annotations\n",
        "      with open(c, 'rt') as csvfile:\n",
        "          csv_reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "          for row in csv_reader:\n",
        "              if len(row) == 2:\n",
        "                  # ... and if the current filename exists\n",
        "                  # in the list of filenames\n",
        "                  if row[0] in filenames:\n",
        "                      index = filenames.index(row[0])\n",
        "                      cur_regression_labels.append(float(row[1]))\n",
        "                      f_temp.append(features[index, :])\n",
        "                  else:\n",
        "                      print(\"Warning: {} not found \"\n",
        "                            \"in list of files.\".format(row[0]))\n",
        "              else:\n",
        "                  print(\"Warning: Row with unknown format in regression file\")\n",
        "      f_final.append(np.array(f_temp))\n",
        "      # cur_regression_labels is the list of values\n",
        "      # for the current regression problem\n",
        "      regression_labels.append(np.array(cur_regression_labels))\n",
        "      # regression task name\n",
        "      regression_names.append(os.path.basename(c).replace(\".csv\", \"\"))\n",
        "      if len(features) == 0:\n",
        "          print(\"ERROR: No data found in any input folder!\")\n",
        "\n",
        "  #TRAIN SPLIT THE DATA SET\n",
        "\n",
        "  features_norm, mean, std = aT.normalize_features([f_final[0]])\n",
        "\n",
        "  first8 = []\n",
        "  for feature in features_norm[0]:\n",
        "    temp = []\n",
        "    temp.extend(feature[0:7])\n",
        "    temp.extend(feature[34:41])\n",
        "    temp.extend(feature[68:75])\n",
        "    temp.extend(feature[102:109])\n",
        "    print(len(temp))\n",
        "    first8.append(temp)\n",
        "  print(len(first8))\n",
        "  first8 = np.array([first8])\n",
        "  mfccFeatures = []\n",
        "  for feature in features_norm[0]:\n",
        "    temp = []\n",
        "    temp.extend(feature[8:20])\n",
        "    temp.extend(feature[42:54])\n",
        "    temp.extend(feature[76:88])\n",
        "    temp.extend(feature[110:122])\n",
        "    #,42:54,76:88,110:122]\n",
        "    #print(len(temp))\n",
        "    mfccFeatures.append(temp)\n",
        "  #print(len(mfccFeatures))\n",
        "  mfccFeatures = np.array([mfccFeatures])\n",
        "  chroma = []\n",
        "  for feature in features_norm[0]:\n",
        "    temp = []\n",
        "    temp.extend(feature[21:33])\n",
        "    temp.extend(feature[55:67])\n",
        "    temp.extend(feature[89:101])\n",
        "    temp.extend(feature[123:135])\n",
        "    chroma.append(temp)\n",
        "  chroma = np.array([chroma])\n",
        "  return [features_norm[0], first8,mfccFeatures,chroma], regression_labels[0]\n",
        "\n",
        "\n",
        "def train4Models(f_train, l_train, f_test, l_test,shortTermWindow, shortTermStep, _):\n",
        "  actual = l_test\n",
        "  classifiers = []\n",
        "  rmses = []\n",
        "  \"\"\"\n",
        "  f_train = StandardScaler().fit_transform(f_train)\n",
        "  #l_train = StandardScaler().fit_transform(l_train)\n",
        "\n",
        "  f_test = StandardScaler().fit_transform(f_test)\n",
        "  #l_test = StandardScaler().fit_transform(l_test)\n",
        "\n",
        "  f_train2 = f_train.copy()\n",
        "  f_test2 = f_test.copy()\n",
        "\n",
        "  pca = PCA(n_components=2)\n",
        "  f_train = pca.fit_transform(f_train2)\n",
        "  f_test = pca.fit_transform(f_test2)\n",
        "  \"\"\"\n",
        "\n",
        "  #RMSE SCORE\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "  for neighVal in range(1,30,5):\n",
        "    regr = RandomForestClassifier(max_depth=neighVal, random_state=0)\n",
        "    regr.fit(f_train, l_train)\n",
        "\n",
        "    predicted = []\n",
        "    for itest, fTest in enumerate(f_test):\n",
        "      R = regr.predict(fTest.reshape(1,-1))[0]\n",
        "      predicted.append(R)\n",
        "    mse = sklearn.metrics.mean_squared_error(actual, predicted)\n",
        "    rmse = math.sqrt(mse)\n",
        "\n",
        "    rmses.append([rmse,neighVal])\n",
        "    classifiers.append(regr)\n",
        "\n",
        "  regr = RandomForestClassifier(random_state=0)\n",
        "  regr.fit(f_train, l_train)\n",
        "\n",
        "  predicted = []\n",
        "  for itest, fTest in enumerate(f_test):\n",
        "    R = regr.predict(fTest.reshape(1,-1))[0]\n",
        "    predicted.append(R)\n",
        "  mse = sklearn.metrics.mean_squared_error(actual, predicted)\n",
        "  rmse = math.sqrt(mse)\n",
        "\n",
        "  rmses.append([rmse,neighVal])\n",
        "  classifiers.append(regr)\n",
        "\n",
        "  neigh = KNeighborsRegressor(n_neighbors=5)\n",
        "  neigh.fit(f_train, l_train)\n",
        "\n",
        "  predicted = []\n",
        "  for itest, fTest in enumerate(f_test):\n",
        "    R = neigh.predict(fTest.reshape(1,-1))[0]\n",
        "    predicted.append(R)\n",
        "  mse = sklearn.metrics.mean_squared_error(actual, predicted)\n",
        "  rmse = math.sqrt(mse)\n",
        "\n",
        "  rmses.append(rmse)\n",
        "  classifiers.append(neigh)\n",
        "\n",
        "  disNeigh = KNeighborsRegressor(n_neighbors=5, weights='distance')\n",
        "  disNeigh.fit(f_train, l_train)\n",
        "\n",
        "  predicted = []\n",
        "  for itest, fTest in enumerate(f_test):\n",
        "    R = disNeigh.predict(fTest.reshape(1,-1))[0]\n",
        "    predicted.append(R)\n",
        "  mse = sklearn.metrics.mean_squared_error(actual, predicted)\n",
        "  rmse = math.sqrt(mse)\n",
        "\n",
        "  rmses.append(rmse)\n",
        "  classifiers.append(disNeigh)\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "  \"\"\"\n",
        "  pca3 = PCA(n_components=3)\n",
        "  f_train = pca3.fit_transform(f_train2)\n",
        "  f_test = pca3.fit_transform(f_test2)\n",
        "\n",
        "  for neighVal in range(1,10):\n",
        "    regr = RandomForestRegressor(max_depth=neighVal, random_state=0)\n",
        "    regr.fit(f_train, l_train)\n",
        "\n",
        "    predicted = []\n",
        "    for itest, fTest in enumerate(f_test):\n",
        "      R = regr.predict(fTest.reshape(1,-1))[0]\n",
        "      predicted.append(R)\n",
        "    mse = sklearn.metrics.mean_squared_error(actual, predicted)\n",
        "    rmse = math.sqrt(mse)\n",
        "\n",
        "    rmses.append([rmse,neighVal])\n",
        "    classifiers.append(regr)\n",
        "\n",
        "  regr = RandomForestRegressor(random_state=0)\n",
        "  regr.fit(f_train, l_train)\n",
        "\n",
        "  predicted = []\n",
        "  for itest, fTest in enumerate(f_test):\n",
        "    R = regr.predict(fTest.reshape(1,-1))[0]\n",
        "    predicted.append(R)\n",
        "  mse = sklearn.metrics.mean_squared_error(actual, predicted)\n",
        "  rmse = math.sqrt(mse)\n",
        "\n",
        "  rmses.append([rmse,neighVal])\n",
        "  classifiers.append(regr)\n",
        "  \"\"\"\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "  return [shortTermWindow, shortTermStep, classifiers, rmses]\n",
        "\n",
        "def grapghPCA(f_train, l_train, f_test, l_test,shortTermWindow, shortTermStep):\n",
        "  actual = l_test\n",
        "  classifiers = []\n",
        "  rmses = []\n",
        "\n",
        "  f_train = StandardScaler().fit_transform(f_train)\n",
        "  #l_train = StandardScaler().fit_transform(l_train)\n",
        "\n",
        "  f_test = StandardScaler().fit_transform(f_test)\n",
        "  #l_test = StandardScaler().fit_transform(l_test)\n",
        "\n",
        "  pca = PCA(n_components=2)\n",
        "  principalComponents = pca.fit_transform(f_train)\n",
        "\n",
        "  principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
        "  labelsDf = pd.DataFrame(data = l_train, columns = ['target'])\n",
        "  finalDf = pd.concat([principalDf, labelsDf], axis = 1)\n",
        "\n",
        "  listOFRgb = []\n",
        "  for i in range(1,31):\n",
        "    rgb = (random.random(), random.random(), random.random())\n",
        "    listOFRgb.append(rgb)\n",
        "\n",
        "  print(len(listOFRgb))\n",
        "\n",
        "  fig = plt.figure(figsize = (8,8))\n",
        "  ax = fig.add_subplot(1,1,1) \n",
        "  ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "  ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "  ax.set_title('PCA: SW-{} SS-{}'.format(shortTermWindow, shortTermStep), fontsize = 20)\n",
        "\n",
        "  targets = list(range(1, 31))\n",
        "  colors = listOFRgb.copy()\n",
        "  for target, color in zip(targets,colors):\n",
        "      indicesToKeep = finalDf['target'] == target\n",
        "      ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
        "                , finalDf.loc[indicesToKeep, 'principal component 2']\n",
        "                , c = color\n",
        "                , s = 50)\n",
        "  ax.legend(targets)\n",
        "  ax.grid()\n",
        "  return [shortTermWindow, shortTermStep, classifiers, rmses]\n",
        "\n",
        "\n",
        "\n",
        "def threeGrapghPCA(f_train, l_train, f_test, l_test,shortTermWindow, shortTermStep):\n",
        "  actual = l_test\n",
        "  classifiers = []\n",
        "  rmses = []\n",
        "\n",
        "  \n",
        "  f_train = StandardScaler().fit_transform(f_train)\n",
        "  #l_train = StandardScaler().fit_transform(l_train)\n",
        "\n",
        "  f_test = StandardScaler().fit_transform(f_test)\n",
        "  #l_test = StandardScaler().fit_transform(l_test)\n",
        "\n",
        "  pca = PCA(n_components=3)\n",
        "  principalComponents = pca.fit_transform(f_train)\n",
        "  \n",
        "  principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2','principal component 3'])\n",
        "  labelsDf = pd.DataFrame(data = l_train, columns = ['target'])\n",
        "  finalDf = pd.concat([principalDf, labelsDf], axis = 1)\n",
        "\n",
        "  listOFRgb = []\n",
        "  for i in range(1,31):\n",
        "    rgb = (random.random(), random.random(), random.random())\n",
        "    listOFRgb.append(rgb)\n",
        "\n",
        "  print(len(listOFRgb))\n",
        "\n",
        "  fig = plt.figure(figsize = (8,8))\n",
        "  ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
        "  ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "  ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "  ax.set_zlabel('Principal Component 3', fontsize = 15)\n",
        "  ax.set_title('PCA: SW-{} SS-{}'.format(shortTermWindow, shortTermStep), fontsize = 20)\n",
        "\n",
        "  targets = list(range(1, 31))\n",
        "  colors = listOFRgb.copy()\n",
        "  for target, color in zip(targets,colors):\n",
        "      indicesToKeep = finalDf['target'] == target\n",
        "      ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
        "                , finalDf.loc[indicesToKeep, 'principal component 2'],\n",
        "                 finalDf.loc[indicesToKeep, 'principal component 3']\n",
        "                , c = color\n",
        "                , s = 50)\n",
        "  plt.show()\n",
        "  return [shortTermWindow, shortTermStep, classifiers, rmses]\n",
        "\n",
        "\n",
        "def autokerasTraining(trainFeats,trainLabels,testFeats,testLabels,shortTermWindow, shortTermStep,which4):\n",
        "  rmses = []\n",
        "  #train_set = tf.data.Dataset.from_tensor_slices((trainFeats.astype(np.unicode), trainLabels))\n",
        "  #test_set = tf.data.Dataset.from_tensor_slices((testFeats.astype(np.unicode), testLabels))\n",
        "\n",
        "  train_set = tf.data.Dataset.from_tensor_slices((trainFeats, trainLabels))\n",
        "  test_set = tf.data.Dataset.from_tensor_slices((testFeats, testLabels))\n",
        "\n",
        "  \n",
        "  batch_size = 400\n",
        "  reg = ak.StructuredDataRegressor(max_trials=10, overwrite=True)\n",
        "  # Feed the tensorflow Dataset to the regressor.\n",
        "  #reg.fit(train_set)\n",
        "  reg.fit(np.matrix(trainFeats),trainLabels,batch_size=batch_size,validation_split=0.1,epochs=30)\n",
        "  # Predict with the best model.\n",
        "  print(\"55555555555555555555555555555555555555555555555\")\n",
        "  rmse1 = math.sqrt(reg.evaluate(test_set,batch_size=batch_size)[0])\n",
        "  rmse2 = math.sqrt(reg.evaluate(test_set,batch_size=batch_size)[1])\n",
        "  print(rmse1)\n",
        "  print(rmse2)\n",
        "  rmses.extend([rmse1,rmse2,which4])\n",
        "  model = reg.export_model()\n",
        "\n",
        "  print(type(model))  # <class 'tensorflow.python.keras.engine.training.Model'>\n",
        "\n",
        "  try:\n",
        "      model.save(\"./classifierModelSaves/100-SS{}-SW{}-{}\".format(shortTermStep,shortTermWindow,which4), save_format=\"tf\")\n",
        "  except Exception:\n",
        "      model.save(\"./classifierModelSaves/100-SS{}-SW{}-{}.h5\".format(shortTermStep,shortTermWindow,which4))\n",
        "\n",
        "  return [shortTermWindow, shortTermStep, rmses]\n",
        "\n",
        "def setupTrainData(shortTermWindow, shortTermStep):\n",
        "  temp = pickle.load(open(\"./pickles/combineTrain/SS{}-SW{}.pickle\".format(shortTermWindow, shortTermStep), 'rb'))\n",
        "  trainFeats = temp[0]\n",
        "  trainLabels = temp[1]\n",
        "\n",
        "  temp = pickle.load(open(\"./pickles/combineTest/SS{}-SW{}.pickle\".format(shortTermWindow, shortTermStep), 'rb'))\n",
        "  testFeats = temp[0]\n",
        "  testLabels = temp[1]\n",
        "  testFeats0 = testFeats[0][:569]\n",
        "  testFeats1 = testFeats[1][0][:569]\n",
        "  testFeats2 = testFeats[2][0][:569]\n",
        "  testFeats3 = testFeats[3][0][:569]\n",
        "  testFeats = []\n",
        "  testFeats.append(testFeats0)\n",
        "  testFeats.append(testFeats1)\n",
        "  testFeats.append(testFeats2)\n",
        "  testFeats.append(testFeats3)\n",
        "\n",
        "  testLabels = testLabels[:569]\n",
        "\n",
        "  testDict = {}\n",
        "  testWavFolder = []\n",
        "  for waveee in os.listdir('./combinedTestWavs'):\n",
        "    if '.wav' in waveee:\n",
        "      testWavFolder.append(waveee)\n",
        "  for i in range(len(testWavFolder)):\n",
        "    testDict[testWavFolder[i].split('.')[0]] = [testFeats[0][i],testFeats[1][i],testFeats[2][i],testFeats[3][i], testLabels[i]]\n",
        "\n",
        "\n",
        "  listOFPickles = os.listdir('./trainTokenPickles')\n",
        "  listOFJson = os.listdir('./combinedTrainWavs/jsons')\n",
        "  temp = []\n",
        "\n",
        "  test = os.listdir('./combinedTrainWavs')\n",
        "  wavs = []\n",
        "  for i in test:\n",
        "    if '.wav' in i:\n",
        "      wavs.append(i)\n",
        "  count = 0\n",
        "  trainDict = {}\n",
        "  for i in range(len(wavs)):\n",
        "    count += 1\n",
        "    trainDict[wavs[i].split('.')[0]] = [trainFeats[0][i],trainFeats[1][0][i],trainFeats[2][0][i],trainFeats[3][0][i], trainLabels[i]]\n",
        "\n",
        "  should = []\n",
        "  for i in os.listdir('./combinedTrainWavs'):\n",
        "    if '.wav' in i:\n",
        "      should.append(i.split('.')[0])\n",
        "\n",
        "  for jsonFile in listOFJson:\n",
        "    jsonFileName = jsonFile.split('.')[0]\n",
        "    if jsonFileName in should:\n",
        "      with open('./trainTokenPickles/{}.pickle'.format(jsonFileName), 'rb') as handle:\n",
        "        temp.append(pickle.load(handle))\n",
        "  trainFeatsTok = []\n",
        "  trainLabelsTok = []\n",
        "  trainNamesTok = []\n",
        "  for i in temp:\n",
        "    if i[1] == 'NA':\n",
        "      i[1] = 30\n",
        "    trainNamesTok.append(i[0])\n",
        "    trainFeatsTok.append(i[2])\n",
        "    trainLabelsTok.append(i[1])\n",
        "  count = 0\n",
        "  for name in range(len(trainNamesTok)):\n",
        "    if trainNamesTok[name] in trainDict.keys():\n",
        "      trainDict[trainNamesTok[name].split('.')[0]].append(trainFeatsTok[name])\n",
        "      trainDict[trainNamesTok[name].split('.')[0]].append(trainLabelsTok[name])\n",
        "      count += 1\n",
        "    else:\n",
        "      None\n",
        "  count = 0\n",
        "  finalTrainDict = {}\n",
        "  for trainJawn in trainDict.keys():\n",
        "    if len(trainDict[trainJawn]) == 7:\n",
        "      if int(trainDict[trainJawn][4]) == int(trainDict[trainJawn][6]):\n",
        "        count +=1\n",
        "        finalTrainDict[trainJawn] = trainDict[trainJawn]\n",
        "      else:\n",
        "        None\n",
        "  listOFPickles = os.listdir('./testTokenPickles')\n",
        "  listOFJson = os.listdir('./combinedTestWavs/jsons')\n",
        "  temp = []\n",
        "  for jsonFile in listOFJson:\n",
        "    jsonFileName = jsonFile.split('.')[0]\n",
        "    with open('./testTokenPickles/{}.pickle'.format(jsonFileName), 'rb') as handle:\n",
        "      temp.append(pickle.load(handle))\n",
        "  testFeatsTok = []\n",
        "  testLabelsTok = []\n",
        "  testNamesTok = []\n",
        "  for i in temp:\n",
        "    testNamesTok.append(i[0])\n",
        "    testFeatsTok.append(i[2])\n",
        "    testLabelsTok.append(i[1])\n",
        "  for name in range(len(testNamesTok)):\n",
        "    if testNamesTok[name] in testDict.keys():\n",
        "      testDict[testNamesTok[name].split('.')[0]].append(testFeatsTok[name])\n",
        "      testDict[testNamesTok[name].split('.')[0]].append(testLabelsTok[name])\n",
        "      count += 1\n",
        "    else:\n",
        "      None\n",
        "  count = 0\n",
        "  finalTestDict = {}\n",
        "  for testJawn in testDict.keys():\n",
        "    if len(testDict[testJawn]) == 7:\n",
        "      count +=1\n",
        "      if int(testDict[testJawn][4]) == int(testDict[testJawn][6]):\n",
        "        finalTestDict[testJawn] = testDict[testJawn]\n",
        "      else:\n",
        "        print('4: {}'.format(testDict[testJawn][4]))\n",
        "        print('6: {}'.format(testDict[testJawn][6]))\n",
        "        print()\n",
        "  print('Count56: {}'.format(count))\n",
        "\n",
        "  listOFPickles = os.listdir('./testTokenPickles')\n",
        "  listOfWavs = os.listdir('./combinedTestWavs')\n",
        "\n",
        "  namePick = []\n",
        "  for picklee in listOFPickles:\n",
        "    if '.pickle' in picklee:\n",
        "      namePick.append(picklee.split('.')[0])\n",
        "  nameWav = []\n",
        "  for wav in listOfWavs:\n",
        "    if '.wav' in wav:\n",
        "      nameWav.append(wav.split('.')[0])\n",
        "  \n",
        "  trainFeats0 = []\n",
        "  trainFeats1 = []\n",
        "  trainFeats2 = []\n",
        "  trainFeats3 = []\n",
        "  trainLabels = []\n",
        "  count = 0\n",
        "  for trainKey in finalTrainDict.keys():\n",
        "    if count == 0:\n",
        "      print(\"TESTKEY ONE: {}\".format(len(finalTrainDict[trainKey][0])))\n",
        "      print(\"TESTKEY ONE: {}\".format(len(finalTrainDict[trainKey][5][0])))\n",
        "      count = 1\n",
        "    trainLabels.append(finalTrainDict[trainKey][4])\n",
        "    #\"\"\"\n",
        "    trainFeats0.append(np.concatenate([finalTrainDict[trainKey][0],finalTrainDict[trainKey][5][0]], axis=0))\n",
        "    trainFeats1.append(np.concatenate([finalTrainDict[trainKey][1],finalTrainDict[trainKey][5][0]], axis=0))\n",
        "    trainFeats2.append(np.concatenate([finalTrainDict[trainKey][2],finalTrainDict[trainKey][5][0]], axis=0))\n",
        "    trainFeats3.append(np.concatenate([finalTrainDict[trainKey][3],finalTrainDict[trainKey][5][0]], axis=0))\n",
        "    #\"\"\"\n",
        "    \"\"\"\n",
        "    trainFeats0.append(np.append(finalTrainDict[trainKey][0].reshape(-1),finalTrainDict[trainKey][5][0].reshape(-1)))\n",
        "    trainFeats1.append(np.append(finalTrainDict[trainKey][1].reshape(-1),finalTrainDict[trainKey][5][0].reshape(-1)))\n",
        "    trainFeats2.append(np.append(finalTrainDict[trainKey][2].reshape(-1),finalTrainDict[trainKey][5][0].reshape(-1)))\n",
        "    trainFeats3.append(np.append(finalTrainDict[trainKey][3].reshape(-1),finalTrainDict[trainKey][5][0].reshape(-1)))\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    trainFeats0.append(np.append(finalTrainDict[trainKey][0],finalTrainDict[trainKey][5][0]))\n",
        "    trainFeats1.append(np.append(finalTrainDict[trainKey][1],finalTrainDict[trainKey][5][0]))\n",
        "    trainFeats2.append(np.append(finalTrainDict[trainKey][2],finalTrainDict[trainKey][5][0]))\n",
        "    trainFeats3.append(np.append(finalTrainDict[trainKey][3],finalTrainDict[trainKey][5][0]))\n",
        "    \"\"\"\n",
        "  trainFeats = [trainFeats0,trainFeats1,trainFeats2,trainFeats3]\n",
        "  \n",
        "  print(len(testFeats0[0]))\n",
        "  print(type(testFeats0[0]))\n",
        "  print((testFeats0.shape[1]))\n",
        "\n",
        "\n",
        "\n",
        "  testFeats0 = []\n",
        "  testFeats1 = []\n",
        "  testFeats2 = []\n",
        "  testFeats3 = []\n",
        "  testLabels = []\n",
        "  count = 0\n",
        "  for testKey in finalTestDict.keys():\n",
        "    if count == 0:\n",
        "      print(\"TESTKEY ONE: {}\".format(len(finalTestDict[testKey][0])))\n",
        "      print(\"TESTKEY ONE: {}\".format(len(finalTestDict[testKey][5][0])))\n",
        "      count = 1\n",
        "    testLabels.append(finalTestDict[testKey][4])\n",
        "    #\"\"\"\n",
        "    testFeats0.append(np.concatenate([finalTestDict[testKey][0],finalTestDict[testKey][5][0]], axis=0))\n",
        "    testFeats1.append(np.concatenate([finalTestDict[testKey][1],finalTestDict[testKey][5][0]], axis=0))\n",
        "    testFeats2.append(np.concatenate([finalTestDict[testKey][2],finalTestDict[testKey][5][0]], axis=0))\n",
        "    testFeats3.append(np.concatenate([finalTestDict[testKey][3],finalTestDict[testKey][5][0]], axis=0))\n",
        "    #\"\"\"\n",
        "    \"\"\"\n",
        "    testFeats0.append(np.append(finalTestDict[testKey][0].reshape(-1),finalTestDict[testKey][5][0].reshape(-1)))\n",
        "    testFeats1.append(np.append(finalTestDict[testKey][1].reshape(-1),finalTestDict[testKey][5][0].reshape(-1)))\n",
        "    testFeats2.append(np.append(finalTestDict[testKey][2].reshape(-1),finalTestDict[testKey][5][0].reshape(-1)))\n",
        "    testFeats3.append(np.append(finalTestDict[testKey][3].reshape(-1),finalTestDict[testKey][5][0].reshape(-1)))\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    testFeats0.append(np.append(finalTestDict[testKey][0],finalTestDict[testKey][5][0]))\n",
        "    testFeats1.append(np.append(finalTestDict[testKey][1],finalTestDict[testKey][5][0]))\n",
        "    testFeats2.append(np.append(finalTestDict[testKey][2],finalTestDict[testKey][5][0]))\n",
        "    testFeats3.append(np.append(finalTestDict[testKey][3],finalTestDict[testKey][5][0]))\n",
        "    \"\"\"\n",
        "  print(\"TESTONE: {}\".format(len(testFeats0[0])))\n",
        "\n",
        "  testFeats = [testFeats0,testFeats1,testFeats2,testFeats3]\n",
        "\n",
        "  return trainFeats, trainLabels, testFeats, testLabels\n",
        "\n",
        "\n",
        "def tryDifferentWindows():\n",
        "  try:\n",
        "    listOFEverything22 = pickle.load(open(\"test-CombinedToken-autokerasTraining.pickle\",'rb'))\n",
        "  except:\n",
        "    listOFEverything22 = []\n",
        "  #print(listOFEverything22[len(listOFEverything22) - 1][0])\n",
        "  #print(listOFEverything22[len(listOFEverything22) - 1][1])\n",
        "  notFirstRun = False\n",
        "  firstRun = True\n",
        "  valMet = True\n",
        "  print(len(listOFEverything22))\n",
        "  listOFEverything = listOFEverything22.copy()\n",
        "  shortTermStepList = [.001, 0.005, 0.01,0.015, 0.02]\n",
        "  for shortTermStep in shortTermStepList:\n",
        "    for shortTermWindow in range(25,105,5):\n",
        "      if listOFEverything != []:\n",
        "        try:\n",
        "          if ((listOFEverything22[len(listOFEverything22) - 1][0] == (float(shortTermWindow)/1000)) and (listOFEverything22[len(listOFEverything22) - 1][1] == shortTermStep)):\n",
        "            print('in')\n",
        "            notFirstRun = True\n",
        "            continue\n",
        "        except:\n",
        "          None\n",
        "      if (notFirstRun) or (listOFEverything == []):\n",
        "        lof = len(listOFEverything22) - 1\n",
        "        print(shortTermStep)\n",
        "        print(str(shortTermWindow) + \"\\n\")\n",
        "        trainDir = './real mmse scores'\n",
        "        testDir = './test-mmse'\n",
        "        shortTermWindow = float(shortTermWindow)/1000\n",
        "\n",
        "        trainFeats, trainLabels, testFeats, testLabels = setupTrainData(shortTermWindow, shortTermStep)\n",
        "        \n",
        "        for i in range(4):\n",
        "          #np.array(trainFeats[i], dtype=np.float32)\n",
        "          #listOFEverything.append(autokerasTraining(np.matrix(trainFeats[i]), trainLabels, np.matrix(testFeats[i]), testLabels,shortTermWindow, shortTermStep, i))\n",
        "          #listOFEverything.append(autokerasTraining(trainFeats[i], np.array(trainLabels),testFeats[i], np.array(testLabels),shortTermWindow, shortTermStep, i))\n",
        "          listOFEverything.append(autokerasTraining(np.asarray(trainFeats[i]), np.asarray(trainLabels), np.asarray(testFeats[i]), np.asarray(testLabels),shortTermWindow, shortTermStep, i))\n",
        "\n",
        "        with open(\"test-CombinedToken-autokerasTraining.pickle\", 'wb') as handle:\n",
        "          pickle.dump(listOFEverything, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "      \n",
        "        \n",
        "\n",
        "  return listOFEverything\n",
        "\n",
        "listOFEverything = tryDifferentWindows()\n",
        "\"\"\"\n",
        "with open(\"ak-classifier-1-100.pickle\", 'wb') as handle:\n",
        "  pickle.dump(listOFEverything, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 20s]\n",
            "val_loss: 264.5651550292969\n",
            "\n",
            "Best val_loss So Far: 24.485628128051758\n",
            "Total elapsed time: 00h 03m 32s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 3s 116ms/step - loss: 653.8290 - mean_squared_error: 653.8290\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 111ms/step - loss: 630.3519 - mean_squared_error: 630.3519\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 609.5400 - mean_squared_error: 609.5400\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 107ms/step - loss: 599.0682 - mean_squared_error: 599.0682\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 109ms/step - loss: 584.1058 - mean_squared_error: 584.1058\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 108ms/step - loss: 569.3148 - mean_squared_error: 569.3148\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 110ms/step - loss: 552.6928 - mean_squared_error: 552.6928\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 108ms/step - loss: 536.4221 - mean_squared_error: 536.4221\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 110ms/step - loss: 513.5547 - mean_squared_error: 513.5547\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 108ms/step - loss: 483.1112 - mean_squared_error: 483.1112\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 459.2573 - mean_squared_error: 459.2573\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 108ms/step - loss: 436.6394 - mean_squared_error: 436.6394\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 111ms/step - loss: 403.6988 - mean_squared_error: 403.6988\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 109ms/step - loss: 370.6034 - mean_squared_error: 370.6034\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 108ms/step - loss: 354.6204 - mean_squared_error: 354.6204\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 315.8534 - mean_squared_error: 315.8534\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 109ms/step - loss: 304.6381 - mean_squared_error: 304.6382\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 109ms/step - loss: 283.5146 - mean_squared_error: 283.5146\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 109ms/step - loss: 273.5180 - mean_squared_error: 273.5180\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 112ms/step - loss: 256.1124 - mean_squared_error: 256.1124\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 107ms/step - loss: 239.0345 - mean_squared_error: 239.0345\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 67ms/step - loss: 231.7040 - mean_squared_error: 231.7040\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 69ms/step - loss: 217.7275 - mean_squared_error: 217.7275\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 68ms/step - loss: 204.6683 - mean_squared_error: 204.6683\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 67ms/step - loss: 207.8148 - mean_squared_error: 207.8148\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 68ms/step - loss: 186.8344 - mean_squared_error: 186.8344\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 68ms/step - loss: 189.7362 - mean_squared_error: 189.7362\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 68ms/step - loss: 179.1470 - mean_squared_error: 179.1470\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 70ms/step - loss: 175.5056 - mean_squared_error: 175.5056\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 72ms/step - loss: 173.4498 - mean_squared_error: 173.4498\n",
            "INFO:tensorflow:Assets written to: .\\structured_data_regressor\\best_model\\assets\n",
            "55555555555555555555555555555555555555555555555\n",
            "2/2 [==============================] - 2s 125ms/step - loss: 98.6897 - mean_squared_error: 98.6897\n",
            "2/2 [==============================] - 2s 123ms/step - loss: 98.6897 - mean_squared_error: 98.6897\n",
            "9.934269217965634\n",
            "9.934269217965634\n",
            "<class 'tensorflow.python.keras.engine.functional.Functional'>\n",
            "INFO:tensorflow:Assets written to: ./classifierModelSaves/100-SS0.02-SW0.1-3\\assets\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nwith open(\"ak-classifier-1-100.pickle\", \\'wb\\') as handle:\\n  pickle.dump(listOFEverything, handle, protocol=pickle.HIGHEST_PROTOCOL)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLEBVzlsCvQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30436b38-9d95-4742-be68-33dd72c19306"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.2\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "johnBItu0k8h",
        "outputId": "1bb21fc8-af69-4e42-d70e-2a5bf68774c4"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "\n",
        "# Create Function Transformer to use Feature Union\n",
        "def get_numeric_data(x):\n",
        "    return [record[:-2].astype(float) for record in x]\n",
        "\n",
        "def get_text_data(x):\n",
        "    return [record[-1] for record in x]\n",
        "\n",
        "transfomer_numeric = FunctionTransformer(get_numeric_data)\n",
        "transformer_text = FunctionTransformer(get_text_data)\n",
        "\n",
        "# Create a pipeline to concatenate Tfidf Vector and Numeric data\n",
        "# Use RandomForestClassifier as an example\n",
        "pipeline = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "            ('numeric_features', Pipeline([\n",
        "                ('selector', transfomer_numeric)\n",
        "            ])),\n",
        "             ('text_features', Pipeline([\n",
        "                ('selector', transformer_text),\n",
        "                ('vec', TfidfVectorizer(analyzer='word'))\n",
        "            ]))\n",
        "         ])),\n",
        "    ('clf', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Grid Search Parameters for RandomForest\n",
        "param_grid = {'clf__n_estimators': np.linspace(1, 100, 10, dtype=int),\n",
        "              'clf__min_samples_split': [3, 10],\n",
        "              'clf__min_samples_leaf': [3],\n",
        "              'clf__max_features': [7],\n",
        "              'clf__max_depth': [None],\n",
        "              'clf__criterion': ['gini'],\n",
        "              'clf__bootstrap': [False]}\n",
        "\n",
        "# Training config\n",
        "kfold = StratifiedKFold(n_splits=7)\n",
        "scoring = {'Accuracy': 'accuracy', 'F1': 'f1_macro'}\n",
        "refit = 'F1'\n",
        "\n",
        "# Perform GridSearch\n",
        "rf_model = GridSearchCV(pipeline, param_grid=param_grid, cv=kfold, scoring=scoring, \n",
        "                         refit=refit, n_jobs=-1, return_train_score=True, verbose=1)\n",
        "rf_model.fit(X_train, Y_train)\n",
        "rf_best = rf_model.best_estimator_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mC:\\Users\\THEGAM~1\\AppData\\Local\\Temp/ipykernel_7132/1567423979.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m rf_model = GridSearchCV(pipeline, param_grid=param_grid, cv=kfold, scoring=scoring, \n\u001b[0;32m     49\u001b[0m                          refit=refit, n_jobs=-1, return_train_score=True, verbose=1)\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mrf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[0mrf_best\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIA-Z7Yquttv",
        "outputId": "447186fc-bbe9-4334-87c3-dccba9ce2273"
      },
      "source": [
        "import pickle\n",
        "\n",
        "listOFEverything22 = pickle.load(open(\"test-CombinedToken-Real2.pickle\",'rb'))\n",
        "temp = []\n",
        "for i in listOFEverything22:\n",
        "\tfor x in i[3]:\n",
        "\t\ttry:\n",
        "\t\t\ttemp.append(x[0])\n",
        "\t\texcept:\n",
        "\t\t\ttemp.append(x)\n",
        "temp.sort()\n",
        "print(temp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6.206972079913672, 6.210936285083477, 6.21336522935113, 6.215268715104014, 6.216225158532272, 6.220486186671948, 6.2447317560060265, 6.254763309425826, 6.261795844908935, 6.267750594024686, 6.271374723910049, 6.27461132907536, 6.2951085283075985, 6.297993665945454, 6.302826570341335, 6.304589812279623, 6.305045147027087, 6.307477654118308, 6.309779109389335, 6.312586985412445, 6.313139435919861, 6.313460736938841, 6.321303050852968, 6.326040628670289, 6.3262748431694, 6.327631078849924, 6.328422469783161, 6.329597679406418, 6.330368450543505, 6.331613052249511, 6.332304342955647, 6.3327426833275355, 6.332861390113135, 6.3329480072198185, 6.33318661252289, 6.333265280273698, 6.33397388930028, 6.334591246745539, 6.3352829528162165, 6.336028289229444, 6.336051736012221, 6.336946643791523, 6.339650966514182, 6.341693186643956, 6.3434332902201565, 6.3436296400937655, 6.345653595166311, 6.347656277203668, 6.349186626529837, 6.352072545459143, 6.355734293275748, 6.358573666690754, 6.36083454357051, 6.363277200850999, 6.367024702223077, 6.37247752062647, 6.374791153472998, 6.378768287361576, 6.379884056327086, 6.380070150072533, 6.3815682094127855, 6.381854931015594, 6.383140605200948, 6.383213082192152, 6.383540072703271, 6.384158064017484, 6.384717743168477, 6.385416804484852, 6.386846178832291, 6.3870258855027755, 6.3883542310047945, 6.3886576539377256, 6.391280759823471, 6.392650361758167, 6.392783965749211, 6.393440811429528, 6.394676389075247, 6.395255136132394, 6.395772447932215, 6.401434447787765, 6.401762435265781, 6.4072735937574175, 6.407407856913735, 6.415196429298918, 6.415293040551495, 6.4172033021122665, 6.418473116951174, 6.419437786493606, 6.421511504096236, 6.421741684566859, 6.423126873727117, 6.425782143566932, 6.426344940195006, 6.426813295922288, 6.427194226314917, 6.4285485487941925, 6.431423081012094, 6.431960602741682, 6.4320293944952605, 6.433010950809771, 6.434244460646442, 6.434764344621993, 6.435026273142936, 6.436305022088363, 6.438619118624407, 6.439916667969506, 6.440651588353641, 6.440772908362081, 6.447251062533954, 6.447543602437914, 6.448123123432633, 6.449453378392732, 6.450221205142805, 6.45101443097224, 6.4525092310588965, 6.452597492769349, 6.455167038478704, 6.455762412558422, 6.457420777857267, 6.459770520421716, 6.46121194409198, 6.464350638810882, 6.467020083040442, 6.470480464209936, 6.47196112267289, 6.478483412879966, 6.478489489620097, 6.480457353870021, 6.484656990514451, 6.486202437540921, 6.489449417065735, 6.489537158125408, 6.492371652925863, 6.49239905979303, 6.492947903201129, 6.493270299339293, 6.495374692612886, 6.497533065945913, 6.501673168639647, 6.502903813460371, 6.503794456654794, 6.506106874913656, 6.506913924866594, 6.511566328883556, 6.511943584880711, 6.5141619606686225, 6.515975440642828, 6.5189983123981445, 6.519688222574217, 6.523906215847011, 6.52552735001435, 6.530672225376382, 6.531588957067745, 6.532421196646115, 6.533330912435701, 6.533525051710357, 6.534142586618729, 6.534754197351467, 6.535754075506771, 6.535841506591806, 6.536495383492984, 6.536911964470769, 6.537186265547482, 6.53830759387862, 6.5384978704552505, 6.538748451861869, 6.538879070741358, 6.541376722873115, 6.5451348370822116, 6.54755937795368, 6.547628289646011, 6.550939189953044, 6.551015989094157, 6.5528293853675645, 6.559885990340403, 6.568582430404557, 6.568646545348777, 6.571103303829072, 6.571230187158058, 6.572210448390704, 6.5728926774644805, 6.573352871368633, 6.575216093902966, 6.576053830814125, 6.576253890052905, 6.576502980997936, 6.576532814177741, 6.57872663424419, 6.579245740604174, 6.57931989805039, 6.579629947733408, 6.580176202899886, 6.581390992667795, 6.583458187623361, 6.5835309361728696, 6.584323018241202, 6.585413398539885, 6.590461933291397, 6.592452671126905, 6.592662592651471, 6.5928807366927265, 6.593164798794047, 6.594924177807659, 6.595042892000852, 6.595339668135243, 6.595706572894941, 6.596106074847162, 6.596116618768848, 6.596204712011624, 6.597300795455838, 6.597438303676549, 6.59758332797937, 6.599018583345951, 6.600039271539375, 6.600636225131064, 6.603694869276929, 6.606478002795702, 6.606568002864456, 6.6070166519501, 6.607708836033686, 6.6081799842790545, 6.608492325674998, 6.609257051052674, 6.610436049814209, 6.611318882533806, 6.611633520077304, 6.611968935345748, 6.612470694322511, 6.614456291220749, 6.618737555813813, 6.619062707899647, 6.62064005582057, 6.621812219882332, 6.623735115966387, 6.6256645406315435, 6.626212371442608, 6.62684070984012, 6.6276268339568984, 6.628316604486429, 6.634005998428748, 6.6351699621822995, 6.635936098454675, 6.636091379660822, 6.639389630223679, 6.639435964231267, 6.640286093257594, 6.641152839080612, 6.641924421966598, 6.642106598901766, 6.6432251186152085, 6.64410848952263, 6.645149141850596, 6.645449035434873, 6.645626390327726, 6.645770334972141, 6.645909032705428, 6.646991131572454, 6.649003885473308, 6.649287549131262, 6.6499701288765785, 6.651946976375598, 6.652825931436314, 6.652936632517925, 6.6529395411903, 6.653401987488614, 6.654088039461788, 6.654166810582711, 6.654712007812117, 6.6552256513022074, 6.658460537210214, 6.659489373794397, 6.659560863126015, 6.660111249110919, 6.660633811595108, 6.660696613471472, 6.660934071481742, 6.661173987915668, 6.6635034013701215, 6.66725815732123, 6.66818547349253, 6.67183249961088, 6.673248070835658, 6.675317044998353, 6.675487639964344, 6.680802947752704, 6.681535613125638, 6.681919721843237, 6.682568815855748, 6.685180800454768, 6.686230551719822, 6.687045956914999, 6.687551509762713, 6.687825361821007, 6.6891796623472555, 6.690748911477794, 6.694129643358663, 6.69423402972443, 6.694726240153885, 6.695286607126025, 6.696150697505326, 6.696961289790305, 6.698331287687194, 6.698594955453285, 6.699471482845955, 6.7007799046293055, 6.702567628259182, 6.7042292913845865, 6.704512520829555, 6.7049665819826965, 6.706263772719135, 6.708172102274707, 6.708736063721039, 6.709117143509685, 6.709689175038049, 6.710171808079893, 6.710320303570647, 6.7134909588151315, 6.713702989348091, 6.71448117777912, 6.715118121664008, 6.71572276113373, 6.715939503626437, 6.7161655225971035, 6.717946425478436, 6.720446157479468, 6.7227716280555985, 6.723734981365947, 6.723947602282025, 6.724520272278235, 6.724776058369569, 6.724793452225054, 6.725740642383714, 6.72761292952735, 6.727791580453562, 6.728158282844092, 6.729234002753743, 6.729329709392679, 6.729972265202247, 6.7303688434352, 6.730613213793406, 6.7319066501118945, 6.733123095504572, 6.733223778716884, 6.733317543185688, 6.734528862775682, 6.735189366358732, 6.736272452034783, 6.737408184097643, 6.73792691407233, 6.73952594508047, 6.741223848372464, 6.741991823935524, 6.743173693378714, 6.746608479721325, 6.746810694537644, 6.746828990621194, 6.746866941264788, 6.746879579902258, 6.747141216155724, 6.747792139555529, 6.74820128927475, 6.749288461130614, 6.750927180291186, 6.753741476801614, 6.754510816491951, 6.7561439740145035, 6.756380930203784, 6.756634418058821, 6.757897712651615, 6.758361200770646, 6.758935037931278, 6.761246068482977, 6.76689099970752, 6.768264957055308, 6.768616084626725, 6.768947310094291, 6.77689732241984, 6.779240228689431, 6.780220343053452, 6.7820518840544946, 6.785577126486868, 6.786933753961638, 6.788095439344377, 6.788205682587359, 6.790043014571521, 6.791568983134092, 6.792051037453537, 6.79453316475826, 6.7947304343744, 6.795534434923258, 6.79588005817676, 6.79985869645943, 6.801126253995999, 6.801177415335925, 6.803409196319081, 6.804908735072768, 6.806104649373284, 6.807174633224468, 6.807281008824014, 6.807469207471757, 6.808625876376787, 6.81003440585717, 6.81282023635981, 6.815812680165382, 6.815889776313295, 6.817013466164254, 6.81809399274828, 6.818673335028015, 6.819814230605245, 6.820379760040813, 6.820581150156903, 6.821329328498162, 6.822104043573926, 6.82289947862151, 6.824086710672512, 6.825441579678166, 6.825503053467211, 6.828045577300576, 6.828848164938687, 6.830943391445693, 6.832547187368387, 6.8344599454699635, 6.836234046826115, 6.838117990444853, 6.8409831821806435, 6.841151387973173, 6.841783565026511, 6.842128031934429, 6.844022979227911, 6.844775611847061, 6.844954002267645, 6.845218088282986, 6.846226588810662, 6.846782760077982, 6.847525983860457, 6.8485965001124365, 6.848664051350178, 6.850123772319843, 6.850403188205877, 6.854329928842191, 6.857440046093377, 6.857726567069558, 6.8582132489425796, 6.861875696796891, 6.868302400042873, 6.877302914200121, 6.883339006026517, 6.88551261869696, 6.886437234511975, 6.892148299699694, 6.8930263517578485, 6.893361601113137, 6.899829809737711, 6.903163440077821, 6.905811867023247, 6.907418939438805, 6.909228446246363, 6.909603647279479, 6.911973210703325, 6.919969555401731, 6.933840888439262, 6.934710466514171, 6.93686476258239, 6.9375451444740035, 6.9387046900556815, 6.943149930748586, 6.947915020469997, 6.947990875230604, 6.963798077903506, 6.963826207254161, 6.9662429562330725, 6.97137599096651, 6.975717925016393, 6.976723861522792, 6.9769646447862845, 6.978065074382086, 6.982586372936322, 6.983366103479848, 6.984419404945689, 6.986672620500393, 6.988580806467945, 6.9906088504252715, 6.9931437211776615, 6.99736096669456, 7.002444921271772, 7.002580876864091, 7.003054747801972, 7.003277710353062, 7.0065223094926745, 7.012977294545293, 7.013409841092975, 7.014230578258902, 7.017232348626282, 7.018399813565803, 7.019039234153948, 7.021712048460172, 7.023448736141901, 7.024583167937726, 7.025064430246196, 7.026811895109522, 7.028018678912226, 7.029692996845115, 7.030054196292323, 7.031661998605554, 7.034431800021767, 7.035679279085244, 7.036609769693481, 7.037039777180656, 7.0391741413900695, 7.045591236502876, 7.046964970999296, 7.047652197602695, 7.051965689460952, 7.052894843944406, 7.054827674815966, 7.05566245183316, 7.056800561423078, 7.056820733248788, 7.057052705100216, 7.062310369702476, 7.06238965435897, 7.0657461519130464, 7.082280549981628, 7.085472079596908, 7.087400479132065, 7.088309997738649, 7.088672035492198, 7.090789379775439, 7.090964628273829, 7.091161381248671, 7.091531714944025, 7.091585392707005, 7.091901496684907, 7.09223423730973, 7.097776705680523, 7.0988947062075844, 7.099535396538142, 7.100195046771108, 7.101355692333623, 7.102317801215755, 7.103657244046048, 7.1042811192671795, 7.106660702082207, 7.107932514193879, 7.110983374435117, 7.111543491976283, 7.113222613067898, 7.115110321248924, 7.118143826184542, 7.1191836470702, 7.12145381977035, 7.12159118075028, 7.121825723581832, 7.1236661359921865, 7.124345509054453, 7.1293039694606914, 7.131883828061178, 7.135670507689242, 7.149973281514569, 7.152079557225476, 7.154152534558036, 7.154174054752509, 7.15958852242779, 7.160984817806292, 7.163266317933251, 7.163594199180591, 7.167333964354084, 7.168919229748923, 7.169615780367244, 7.171295092910941, 7.171617554943339, 7.173572329947498, 7.176677330693975, 7.177158311162403, 7.177158311162403, 7.177961893297998, 7.190633542613086, 7.192071884131065, 7.193230308948829, 7.195075421445193, 7.197698542706633, 7.199722298519034, 7.199970343948415, 7.201681318254564, 7.203513377665541, 7.20526695453548, 7.208575048592803, 7.209227220080956, 7.209477929826544, 7.2128592165861525, 7.21382618699867, 7.214822622935857, 7.230136613151916, 7.231518700826151, 7.237702088160703, 7.241384622125219, 7.241933619144441, 7.246233528935697, 7.253680015778661, 7.255160492268646, 7.257482140995235, 7.2575693704752, 7.258035184062977, 7.260362131233274, 7.264882597579863, 7.264915936108198, 7.2655037322945315, 7.266641817352054, 7.267379463772525, 7.267452915989273, 7.277661285915521, 7.281449153798949, 7.281860261701222, 7.285445931439532, 7.290681775957092, 7.2909021685071425, 7.295716250100494, 7.2972621980567896, 7.298139966805855, 7.299897654475032, 7.3000316871539646, 7.300436295644361, 7.3076715892915916, 7.308162089064461, 7.314190238193448, 7.3173026553140605, 7.321643793340629, 7.330257092295324, 7.331409406326619, 7.3338390853315945, 7.342645871794208, 7.344913332848107, 7.354128822540791, 7.357766371572808, 7.359768429612905, 7.361015905440115, 7.365766395385943, 7.366964830817732, 7.422945420298324, 7.4313321490091395, 7.711663302406279, 7.751822194230261, 7.763519948636866, 7.763519948636866, 7.7794326884250795, 7.818788394456994, 7.840491668801193, 7.840832078729447, 7.843441397354649, 7.844575612997985, 7.853643441262858, 7.858286647809808, 7.859305521101807, 7.861682378604725, 7.86326655112826, 7.872425862970799, 7.87954241133961, 7.887103758284797, 7.8875549526879745, 7.88845726407372, 7.888682825794091, 7.890487087428021, 7.890712591124425, 7.893305420584804, 7.894432472124865, 7.895559362783966, 7.8980379562723595, 7.8986011645019705, 7.901191405274005, 7.902880235346786, 7.904343596202833, 7.904906355168653, 7.905919220360223, 7.905919220360223, 7.906706914694349, 7.906931955805403, 7.907382018813754, 7.910081859108261, 7.9109816011186895, 7.911656340477768, 7.914354722664166, 7.916490289600441, 7.918512927001443, 7.919074679041864, 7.919748728894473, 7.920422721383578, 7.920984337982388, 7.92255665265896, 7.922893536636887, 7.923679543551518, 7.924465472504188, 7.924914539768454, 7.926037096616411, 7.926261588908147, 7.92648607484186, 7.927383955007273, 7.927720633851736, 7.9283939486495125, 7.9285061622232975, 7.929403813645616, 7.929516012928422, 7.92974040673145, 7.929964794184819, 7.929964794184819, 7.930301363460439, 7.931983995619354, 7.9323204792200235, 7.934226950079867, 7.9354603048033905, 7.936132962954562, 7.936581370049473, 7.936693467864749, 7.937478108244533, 7.938262671068238, 7.939271280775767, 7.939271280775767, 7.939943616067718, 7.940503851991229, 7.940615894432758, 7.941064048390529, 7.942408358540112, 7.9436404429923995, 7.944088426324754, 7.9442004182106025, 7.946551883222069, 7.947111653292235, 7.947335550280638, 7.947447496409464, 7.948343008679853, 7.949014576676377, 7.949238420067342, 7.951476507397027, 7.952371565962676, 7.9524834412001315, 7.953378386447303, 7.953378386447303, 7.953713964953664, 7.954161380938996, 7.955279810812905, 7.955391645152236, 7.955838966789136, 7.9575161989743695, 7.957851602991307, 7.958745944599323, 7.958745944599323, 7.9588577302340795, 7.959416634858348, 7.959528411073761, 7.959640185719515, 7.959640185719515, 7.960981359059212, 7.960981359059212, 7.962098830936118, 7.962657508065215, 7.963216145999074, 7.964221595534356, 7.964221595534356, 7.964445011526981, 7.965115221904869, 7.965673687477643, 7.966120431746942, 7.967460514258188, 7.967572177626881, 7.967683839430664, 7.967907158343762, 7.96813047099801, 7.968353777393935, 7.968577077532061, 7.968688725254614, 7.9688003714129145, 7.969135300502961, 7.96924694040491, 7.969470215517104, 7.970809734850124, 7.971367801471371, 7.971479410107332, 7.972483817521399, 7.973376517892869, 7.973376517892869, 7.9744922528266, 7.975496280813307, 7.976054019527446, 7.976165562590244, 7.977057850943252, 7.977169379969688, 7.977503957693647, 7.978953632376594, 7.980180074494078, 7.981294858333349, 7.98185219186696, 7.981963653903843, 7.982298030675992, 7.983189633597504, 7.983301076961162, 7.983858270445579, 7.9846382760007675, 7.984861120735771, 7.984972540771091, 7.985418205366, 7.986643654773099, 7.986755049940996, 7.988759897350082, 7.989093989671214, 7.989428068021673, 7.9895394243675355, 7.9897621324032135, 7.9897621324032135, 7.990318875335162, 7.990652902474815, 7.991098250275215, 7.99120958334733, 7.99132091486838, 7.99132091486838, 7.992211511206247, 7.992656772161562, 7.993547219665679, 7.993547219665679, 7.99410369898827, 7.994771423046538, 7.995327817163884, 7.995772904581333, 7.996329229014401, 7.99911027080857, 7.9996663631496645, 8.000222416836998, 8.000889630250178, 8.00111202235702, 8.001667975582283, 8.001779161592108, 8.002112710352279, 8.002557420406548, 8.002890936730035, 8.00366908743796, 8.004224863055947, 8.004447162498119, 8.0048917428624, 8.005336298536685, 8.005558567116367, 8.005891958415505, 8.006003085763375, 8.006669817460708, 8.006669817460708, 8.00744760094317, 8.007892014729778, 8.008114212374482, 8.008225308884906, 8.008336403854136, 8.008780768320344, 8.009114025490561, 8.00944726879461, 8.009669423294737, 8.010113713811124, 8.010113713811124, 8.010446915527051, 8.010446915527051, 8.010557979685776, 8.010669042304649, 8.01100222092279, 8.011335385684568, 8.012667906202589, 8.012667906202589, 8.01277893957643, 8.01300100170846, 8.013334083368333, 8.014000205156988, 8.015554274052713, 8.015665267444449, 8.015776259299278, 8.016109225642932, 8.016331195521984, 8.016442178156687, 8.016442178156687, 8.016664138816765, 8.016775116842268, 8.016997068284518, 8.016997068284518, 8.017440952735791, 8.017551920008724, 8.017551920008724, 8.018661508278152, 8.018883407507733, 8.019105300597072, 8.019327187546676, 8.019881878062687, 8.01999281156217, 8.020325602854331, 8.020325602854331, 8.020547456044502, 8.020991144016078, 8.021102062174185, 8.02154571946937, 8.021656629959173, 8.021656629959173, 8.021878446338437, 8.021989352228024, 8.022432960454216, 8.022543858678045, 8.022543858678045, 8.022765650526882, 8.022765650526882, 8.023098326804043, 8.023320103325496, 8.023763637978718, 8.023763637978718, 8.024650633740656, 8.024650633740656, 8.024872367362333, 8.025204956307803, 8.028197636790818, 8.028640901989332, 8.028751714465125, 8.029638159218106, 8.029748957931508, 8.03019213749737, 8.030413718108537, 8.030856860989877, 8.031078423261063, 8.031632302198751, 8.031964611227705, 8.032075377848793, 8.032407668547249, 8.032629188042845, 8.0338474360878, 8.03395817674981, 8.03395817674981, 8.034068915885383, 8.034179653494583, 8.034622588668913, 8.034733318647195, 8.036061959377031, 8.036504790807323, 8.036504790807323, 8.036615494852272, 8.037058295783481, 8.037390380472624, 8.037611762642875, 8.037833138715701, 8.037833138715701, 8.038497230354684, 8.038497230354684, 8.038829255601325, 8.03905059814733, 8.03905059814733, 8.03905059814733, 8.039161267134997, 8.039271934599187, 8.039382600539968, 8.039382600539968, 8.039935907394897, 8.040157219475189, 8.040267873230942, 8.040599825361378, 8.040710473026126, 8.041374327042341, 8.041374327042341, 8.04148496405025, 8.04148496405025, 8.042038126258747, 8.042038126258747, 8.042148754134674, 8.042701870688912, 8.042701870688912, 8.042701870688912, 8.042812489435118, 8.043144336545586, 8.043144336545586, 8.043144336545586, 8.0433655603464, 8.044029195244766, 8.044471588095305, 8.044913956618492, 8.045024544948612, 8.045024544948612, 8.045135131758585, 8.045798620698871, 8.045798620698871, 8.04601977152064, 8.046240916264082, 8.0464620549297, 8.04657262198348, 8.046683187517992, 8.046793751533297, 8.046793751533297, 8.04701487500654, 8.04701487500654, 8.047678208971837, 8.047899308143794, 8.048230945513005, 8.048230945513005, 8.048230945513005, 8.048783644095932, 8.049004712902546, 8.049004712902546, 8.049336304728435, 8.049336304728435, 8.049446832301573, 8.049667882894985, 8.049778405915385, 8.050330998256822, 8.050441512173434, 8.050441512173434, 8.050441512173434, 8.050552024572966, 8.050662535455482, 8.050773044821042, 8.051104563816626, 8.05132556889672, 8.051546567910497, 8.051657065142674, 8.051988547741095, 8.052209528558913, 8.05243050331241, 8.05243050331241, 8.052540988415195, 8.052651472002086, 8.052761954073146, 8.05309339119197, 8.05309339119197, 8.05309339119197, 8.053203867200333, 8.053203867200333, 8.053203867200333, 8.053424814670564, 8.053535286132558, 8.053645756079218, 8.053645756079218, 8.053756224510611, 8.053977156827838, 8.054087620713798, 8.054198083084737, 8.054308543940717, 8.054529461108057, 8.05463991741954, 8.054750372216315, 8.05497127726599, 8.055081727519012, 8.055192176257577, 8.055192176257577, 8.05552351338714, 8.055633956068492, 8.055854836888814, 8.056407012445309, 8.056738299614166, 8.057290414625923, 8.057511250037694, 8.057621665473913, 8.058173719960847, 8.058284126319835, 8.058946532710205, 8.059388106724, 8.059498496447212, 8.060271182177999, 8.060381559807185, 8.0604919359249, 8.0604919359249, 8.0606023105312, 8.060712683626152, 8.060712683626152, 8.061043793843533, 8.061043793843533, 8.061264526432845, 8.061816331465112, 8.061816331465112, 8.061926687939316, 8.061926687939316, 8.062368098730706, 8.06247844765251, 8.062588795064016, 8.062809485356393, 8.062809485356393, 8.062919828237387, 8.06325084782034, 8.063692186126518, 8.063692186126518, 8.063802516928876, 8.063912846221681, 8.064023174005001, 8.064243825043418, 8.064243825043418, 8.064685109009112, 8.06490574193738, 8.065126368829922, 8.065346989687235, 8.065346989687235, 8.065457297852834, 8.065457297852834, 8.065567604509813, 8.06567790965823, 8.06578821329815, 8.065898515429634, 8.065898515429634, 8.065898515429634, 8.066008816052742, 8.066339708872444, 8.066670588119004, 8.06733230589938, 8.06733230589938, 8.067442586919254, 8.067663144436537, 8.067663144436537, 8.067883695924282, 8.067993969407231, 8.068104241382983, 8.068104241382983, 8.068214511851595, 8.068214511851595, 8.068545314215227, 8.068545314215227, 8.068545314215227, 8.069096621350962, 8.069206878258116, 8.069206878258116, 8.069317133658753, 8.069317133658753, 8.069868388066299, 8.070088879285837, 8.070309364481275, 8.070529843653103, 8.070640080980317, 8.070860551117656, 8.070860551117656, 8.071191245031864, 8.071301473325699, 8.071301473325699, 8.071411700114188, 8.071521925397391, 8.071742371448188, 8.071742371448188, 8.071742371448188, 8.071852592215906, 8.071852592215906, 8.07218324548907, 8.07218324548907, 8.07240367348014, 8.072624095452287, 8.072844511406005, 8.072844511406005, 8.073064921341787, 8.073064921341787, 8.073175124053106, 8.073285325260127, 8.073285325260127, 8.073505723161514, 8.073836308732888, 8.073836308732888, 8.074166880768896, 8.074277068439994, 8.074277068439994, 8.074827984241459, 8.074938162891378, 8.075158515681336, 8.075268689821497, 8.075489033592492, 8.075489033592492, 8.075709371351461, 8.075819537976589, 8.075929703098895, 8.075929703098895, 8.07603986671844, 8.07603986671844, 8.076260189449497, 8.076370348561131, 8.076370348561131, 8.076480506170249, 8.07670081688119, 8.076810969983132, 8.077141420275597, 8.077251567368833, 8.077361712960048, 8.077471857049302, 8.077581999636655, 8.077692140722169, 8.077912418387928, 8.077912418387928, 8.077912418387928, 8.078022554968296, 8.078022554968296, 8.078242823624313, 8.0786833429192, 8.078793468989707, 8.079674423517051, 8.0797845360793, 8.07989464714094, 8.07989464714094, 8.08011486476263, 8.08055528200138, 8.08055528200138, 8.080665382560463, 8.080775481619424, 8.080885579178327, 8.080885579178327, 8.080885579178327, 8.081105769796201, 8.081325954414577, 8.081436044474104, 8.081546133033942, 8.081656220094152, 8.081656220094152, 8.081656220094152, 8.08176630565479, 8.082316710966609, 8.082426787531126, 8.082426787531126, 8.082536862596502, 8.08297714786784, 8.083527470733724, 8.083747589389958, 8.083747589389958, 8.08418780872206, 8.08418780872206, 8.084738049179046, 8.085068175478499, 8.085068175478499, 8.085178214582895, 8.08528825218968, 8.08561835602499, 8.085728387641948, 8.085728387641948, 8.086058473509246, 8.086168499137356, 8.086278523268406, 8.086388545902459, 8.086388545902459, 8.086498567039575, 8.086718604823238, 8.086718604823238, 8.086938636619887, 8.088038705814999, 8.088038705814999, 8.088258701699814, 8.088588684307949, 8.08880866523469, 8.089028640179054, 8.089028640179054, 8.089358591379716, 8.08946857212261, 8.08946857212261, 8.08946857212261, 8.089578551370277, 8.090018453409884, 8.090238395459906, 8.090238395459906, 8.090458331530723, 8.090458331530723, 8.090568297324081, 8.091008145551944, 8.091228060699384, 8.091338016031694, 8.091338016031694, 8.09188777028159, 8.09188777028159, 8.091997716649661, 8.091997716649661, 8.091997716649661, 8.092107661523908, 8.092657363489879, 8.092657363489879, 8.092767299402448, 8.092877233821614, 8.093207028119323, 8.093316956565497, 8.09353680897862, 8.093756655419844, 8.093976495889658, 8.093976495889658, 8.093976495889658, 8.094196330388549, 8.09430624539905, 8.09430624539905, 8.094416158917001, 8.09518551175641, 8.09518551175641, 8.095405313423912, 8.09551521201972, 8.09562510912365, 8.09573500473576, 8.095844898856111, 8.095954791484765, 8.095954791484765, 8.096064682621781, 8.096174572267222, 8.096504232254693, 8.096723998122904, 8.096943758026269, 8.097273386698289, 8.097273386698289, 8.097383259940399, 8.097383259940399, 8.097493131691659, 8.097493131691659, 8.097493131691659, 8.097493131691659, 8.097712870721882, 8.097822738000962, 8.09837205203851, 8.09837205203851, 8.099470568348556, 8.100349274135612, 8.100459105656908, 8.100459105656908, 8.100678764232114, 8.100898416851205, 8.101118063514663, 8.101337704222974, 8.101337704222974, 8.1016671541206, 8.1016671541206, 8.1016671541206, 8.10199659062187, 8.102106399812273, 8.102545621692075, 8.102655423441876, 8.102984819763877, 8.102984819763877, 8.102984819763877, 8.103094615562282, 8.103094615562282, 8.103423994031552, 8.103753359113176, 8.103753359113176, 8.103753359113176, 8.103863144498968, 8.104412049120016, 8.104412049120016, 8.104851146050995, 8.105180453138347, 8.105619508443409, 8.105729268553754, 8.10583902717785, 8.105948784315762, 8.106387798006743, 8.106387798006743, 8.106497547714623, 8.10660729593668, 8.10660729593668, 8.106717042672972, 8.10682678792356, 8.10682678792356, 8.106936531688502, 8.107046273967864, 8.107046273967864, 8.107265754070076, 8.107375491893048, 8.107375491893048, 8.107594963083026, 8.107594963083026, 8.107704696450153, 8.108033887640806, 8.108033887640806, 8.108253341009572, 8.108253341009572, 8.108253341009572, 8.108253341009572, 8.108363065466635, 8.108363065466635, 8.108472788438899, 8.108472788438899, 8.108582509926423, 8.108692229929268, 8.108801948447496, 8.108911665481164, 8.109131095095067, 8.10946022838324, 8.109569936510823, 8.109899051988993, 8.11000875418039, 8.110557242880144, 8.11088631829638, 8.11088631829638, 8.111105694490032, 8.111105694490032, 8.111215380361886, 8.111434747655956, 8.11154442907829, 8.111654109017572, 8.111983139937697, 8.112202486470284, 8.112641161743634, 8.112641161743634, 8.11297015263269, 8.11297015263269, 8.113079813297752, 8.113079813297752, 8.113518441136492, 8.113518441136492, 8.113628094391098, 8.113957045263694, 8.114066692591019, 8.114176338436671, 8.114285982800714, 8.114505267084208, 8.11461490700378, 8.11461490700378, 8.114943817874513, 8.114943817874513, 8.115053451868965, 8.115601599625466, 8.115601599625466, 8.115820848361835, 8.116040091175366, 8.116040091175366, 8.116149710361215, 8.116478559035825, 8.116478559035825, 8.116588172299915, 8.116588172299915, 8.116807394387283, 8.117355423703774, 8.117355423703774, 8.11757462507086, 8.117684223534745, 8.117793820518942, 8.118232193660017, 8.118341783246658, 8.118560957982009, 8.118780126800514, 8.119328022963723, 8.119875882157128, 8.120095015484623, 8.120095015484623, 8.120204579930782, 8.120314142898625, 8.120642822932878, 8.120642822932878, 8.120752379988067, 8.120861935565243, 8.120971489664464, 8.121300143095, 8.121409691283002, 8.12151923799335, 8.12151923799335, 8.121628783226102, 8.121628783226102, 8.122395558487714, 8.122505091901854, 8.122724154299021, 8.122833683282167, 8.123052736817886, 8.123162261370577, 8.123381306045923, 8.123709861984743, 8.12392889189519, 8.12392889189519, 8.12403840463596, 8.124366934001184, 8.124476440837428, 8.124476440837428, 8.124914453423177, 8.125352442397022, 8.125461935951572, 8.125680918634393, 8.125899895415909, 8.126118866296597, 8.126118866296597, 8.126228349524279, 8.126775743538463, 8.12688521791687, 8.127104162249733, 8.127323100684391, 8.127870420969515, 8.127979880603899, 8.128089338764209, 8.128198795450503, 8.128198795450503, 8.12830825066284, 8.12830825066284, 8.12830825066284, 8.12830825066284, 8.128527156665887, 8.128746056773824, 8.128855504617277, 8.129074395883446, 8.129183839306283, 8.129183839306283, 8.129183839306283, 8.129293281255698, 8.129621598264022, 8.129840468903561, 8.130059333650848, 8.13027819250636, 8.130387619724848, 8.130606469743588, 8.130715892543959, 8.130715892543959, 8.131044152109784, 8.131153569020162, 8.13137239842393, 8.13137239842393, 8.131591221938775, 8.131700631488, 8.131810039565174, 8.132357057872335, 8.132575854892913, 8.132575854892913, 8.1329040393871, 8.1329040393871, 8.133122821692798, 8.133341598113374, 8.133341598113374, 8.133341598113374, 8.133341598113374, 8.133341598113374, 8.133779133301067, 8.13388851342053, 8.134216644953973, 8.134326019190334, 8.13443539195607, 8.134654133075891, 8.134654133075891, 8.134763501430099, 8.13509159767062, 8.135747750457528, 8.135747750457528, 8.135747750457528, 8.13596645629368, 8.136731880426574, 8.137059897299638, 8.137169233318955, 8.137169233318955, 8.137278567869187, 8.13738790095039, 8.137497232562627, 8.137606562705955, 8.138153191391032, 8.13826251272215, 8.138590467904775, 8.13869978336227, 8.138918409872547, 8.138918409872547, 8.139355645275902, 8.139574254169926, 8.13979285719281, 8.13979285719281, 8.13979285719281, 8.14012075071979, 8.14012075071979, 8.1403393390669, 8.14044863103937, 8.140776498153143, 8.141760020275592, 8.141978564613101, 8.142415635690776, 8.142415635690776, 8.142524899794427, 8.142524899794427, 8.142743423603218, 8.142852683308476, 8.143289707469979, 8.143398959845765, 8.143617460200245, 8.143835954692328, 8.144054443322485, 8.14438216527764, 8.144600639255048, 8.14481910737218, 8.145037569629507, 8.145037569629507, 8.145037569629507, 8.145146798560893, 8.145146798560893, 8.145692921247386, 8.145911360070215, 8.146348220144006, 8.146675849825819, 8.14678505679178, 8.146894262293848, 8.147221870017304, 8.1474402678479, 8.1474402678479, 8.147658659824348, 8.147767853617411, 8.147767853617411, 8.147986236813516, 8.148095426216674, 8.148095426216674, 8.148204614156649, 8.148313800633497, 8.148313800633497, 8.148641351285876, 8.148641351285876, 8.14875053191081, 8.148968888772243, 8.14918723978282, 8.14918723978282, 8.149296413094184, 8.149405584943013, 8.149405584943013, 8.14951475532936, 8.149623924253287, 8.14984225771412, 8.14995142225114, 8.150169746938685, 8.150169746938685, 8.150278907089326, 8.150278907089326, 8.150715533072395, 8.150715533072395, 8.15082468591358, 8.15082468591358, 8.150933837293051, 8.151479572266734, 8.151479572266734, 8.151697856026257, 8.151916133940798, 8.152134406010825, 8.152352672236812, 8.152570932619225, 8.152680060619238, 8.152680060619238, 8.152680060619238, 8.152898312237172, 8.153334797946306, 8.153334797946306, 8.15344391572253, 8.153880372224942, 8.153989482700219, 8.154207699270787, 8.154316805366197, 8.154971411284114, 8.154971411284114, 8.15508050716189, 8.15508050716189, 8.15508050716189, 8.15508050716189, 8.155189601580236, 8.155298694539216, 8.155298694539216, 8.155625964660521, 8.155625964660521, 8.155625964660521, 8.156062304394643, 8.156280465508067, 8.156389543876571, 8.156498620786353, 8.156607696237467, 8.15682584276393, 8.15682584276393, 8.157043983456427, 8.157043983456427, 8.157262118315428, 8.157262118315428, 8.157371183557515, 8.1574802473414, 8.157698370534813, 8.15791648789613, 8.158025544389902, 8.158025544389902, 8.158025544389902, 8.158134599425825, 8.158243653003957, 8.158243653003957, 8.15835270512436, 8.15835270512436, 8.158570804992205, 8.15878889902983, 8.15878889902983, 8.15878889902983, 8.1590069872377, 8.1590069872377, 8.1590069872377, 8.159225069616282, 8.159661216887452, 8.160097340847082, 8.160206368195002, 8.16031539408624, 8.16031539408624, 8.160533441498911, 8.160642463020462, 8.160642463020462, 8.160751483085567, 8.160969518846672, 8.160969518846672, 8.16107853454279, 8.161296561566447, 8.161405572894102, 8.161405572894102, 8.161732598141079, 8.161950607692987, 8.162059610285295, 8.162604601412687, 8.163149556155185, 8.163149556155185, 8.163149556155185, 8.163476511538965, 8.163585493756957, 8.163585493756957, 8.163694474520074, 8.163803453828383, 8.163803453828383, 8.163912431681933, 8.164021408080789, 8.164130383025006, 8.164239356514644, 8.164457299130412, 8.164566268256658, 8.164566268256658, 8.165220052474892, 8.165329011421933, 8.165655879539653, 8.16587378434921, 8.16598273457349, 8.166200630661331, 8.166309576525013, 8.166418520935277, 8.166527463892185, 8.166527463892185, 8.166527463892185, 8.166636405395794, 8.166636405395794, 8.167072156878397, 8.167290023901415, 8.167398955233555, 8.167398955233555, 8.167507885112864, 8.16794359010291, 8.168161433882439, 8.168270353593533, 8.168379271852258, 8.168488188658673, 8.168597104012832, 8.169141659001872, 8.169468374571975, 8.169468374571975, 8.169795077076504, 8.169903975008157, 8.170012871488312, 8.170012871488312, 8.170012871488312, 8.170339552220367, 8.170666219891027, 8.170775106212318, 8.170883991082578, 8.170883991082578, 8.170992874501858, 8.171319516054426, 8.171428393670384, 8.171972759991041, 8.172299362378217, 8.172299362378217, 8.172517090051793, 8.172734811924942, 8.17317023827182, 8.173605641422558, 8.173932178565375, 8.174041021380862, 8.174149862747054, 8.174258702664009, 8.174258702664009, 8.174476378150437, 8.174585213720027, 8.17469404784061, 8.175020541508903, 8.175129369834043, 8.175129369834043, 8.175129369834043, 8.175347022138224, 8.175347022138224, 8.176217573415546, 8.176217573415546, 8.176326385807979, 8.176326385807979, 8.176435196752335, 8.176544006248667, 8.176979229754924, 8.177088032012009, 8.177088032012009, 8.177088032012009, 8.177196832821418, 8.17741443009744, 8.177523226564167, 8.177740815155348, 8.177849607279914, 8.177849607279914, 8.178175974970218, 8.178502329636636, 8.178611111631515, 8.17882867128073, 8.179154999904057, 8.179263773218597, 8.179263773218597, 8.179263773218597, 8.179263773218597, 8.179372545086617, 8.179372545086617, 8.179372545086617, 8.179807618094642, 8.180242667962947, 8.180351426814877, 8.180351426814877, 8.180460184220859, 8.180460184220859, 8.180460184220859, 8.180568940180956, 8.180677694695225, 8.180786447763722, 8.181112698295166, 8.181221445581155, 8.181765160330015, 8.181982636112792, 8.181982636112792, 8.182091371836533, 8.182308838949007, 8.182417570337856, 8.182417570337856, 8.182417570337856, 8.182526300281856, 8.182635028781064, 8.182635028781064, 8.182635028781064, 8.182743755835537, 8.182852481445336, 8.183069928331134, 8.183613520267823, 8.183722234322286, 8.183722234322286, 8.18393965809879, 8.184157076099122, 8.184157076099122, 8.184483192270307, 8.184809295447694, 8.184917993619582, 8.185026690347941, 8.185135385632835, 8.18535277187245, 8.185461462827286, 8.185896212214825, 8.18633093851424, 8.186439616482007, 8.186656968089432, 8.18676564172921, 8.18676564172921, 8.187200321863408, 8.187200321863408, 8.187526316819513, 8.187743639579423, 8.18796095657121, 8.18817826779533, 8.188286921244659, 8.18861287294241, 8.188830166866287, 8.188938811666013, 8.189047455024337, 8.189156096941318, 8.189156096941318, 8.189156096941318, 8.189264737417016, 8.189373376451485, 8.189373376451485, 8.189482014044783, 8.189482014044783, 8.189916550007418, 8.190133809343203, 8.190133809343203, 8.190785552773937, 8.190894171637236, 8.19100278906017, 8.191220019585158, 8.191220019585158, 8.191437244349363, 8.191437244349363, 8.191654463353242, 8.191871676597254, 8.191871676597254, 8.191980281059452, 8.191980281059452, 8.192197485664522, 8.192197485664522, 8.192197485664522, 8.192523281774665, 8.192523281774665, 8.192631877598952, 8.192849064929229, 8.193066246502157, 8.19317483512976, 8.1938263366749, 8.1938263366749, 8.194586356345354, 8.194694924829847, 8.194803491875973, 8.195129184384736, 8.195454863950278, 8.195454863950278, 8.195563420929451, 8.19567197647072, 8.195780530574137, 8.195889083239761, 8.196106184257857, 8.196106184257857, 8.196214732610445, 8.19675745281105, 8.196865992539108, 8.19697453082994, 8.19697453082994, 8.197300137079681, 8.197300137079681, 8.197517200727777, 8.197734258628355, 8.197734258628355, 8.197842785423466, 8.19795131078187, 8.198059834703622, 8.198276878237401, 8.19849391602526, 8.19849391602526, 8.198602432764611, 8.198710948067655, 8.198710948067655, 8.19892797436504, 8.199036485359498, 8.199036485359498, 8.199144994917875, 8.199470514977088, 8.199579018791713, 8.199796022113627, 8.199796022113627, 8.199904521621034, 8.199904521621034, 8.200013019692815, 8.200121516329027, 8.20033850529498, 8.200555488519342, 8.200772466002572, 8.200880952591405, 8.201640318533284, 8.201857267317687, 8.20196573955796, 8.20207421036369, 8.20207421036369, 8.202182679734936, 8.202941925172823, 8.202941925172823, 8.203158839533659, 8.203267294563174, 8.203375748158829, 8.20348420032068, 8.203592651048785, 8.203701100343201, 8.203809548203983, 8.204026439624878, 8.204351766005397, 8.204460205265578, 8.20467707948629, 8.204785514446938, 8.204893947974519, 8.205002380069095, 8.20511081073072, 8.20511081073072, 8.20511081073072, 8.205544519048845, 8.205652942546568, 8.205652942546568, 8.205652942546568, 8.205652942546568, 8.206195038547168, 8.206303453450083, 8.206411866920728, 8.20662868956544, 8.20662868956544, 8.20673709873962, 8.206845506481756, 8.206953912791906, 8.207062317670129, 8.20717072111648, 8.207387523713791, 8.207387523713791, 8.207495922864867, 8.20771271687214, 8.208037897146701, 8.208146287708757, 8.208471450807322, 8.209121738365347, 8.209121738365347, 8.209230114616906, 8.209230114616906, 8.209338489437727, 8.209338489437727, 8.209880342082746, 8.209988708320331, 8.209988708320331, 8.21020543650453, 8.21031379845126, 8.21031379845126, 8.210530518054261, 8.2112889916209, 8.2112889916209, 8.21150568548642, 8.211722373633744, 8.211722373633744, 8.211722373633744, 8.212372403771072, 8.212697399546173, 8.212914056251511, 8.212914056251511, 8.213563992077818, 8.213563992077818, 8.213563992077818, 8.214538799399072, 8.214538799399072, 8.214755407539165, 8.21508030904055, 8.215188606685569, 8.215838362577491, 8.215838362577491, 8.215946650230308, 8.21605493645589, 8.21605493645589, 8.216488067087035, 8.216704623840682, 8.216812900077223, 8.217137720226384, 8.218003844524052, 8.21832861760248, 8.218545125858052, 8.218761628410107, 8.218761628410107, 8.218978125259097, 8.219086371545085, 8.219086371545085, 8.21951934243363, 8.219735819325447, 8.21984405563344, 8.219952290516229, 8.219952290516229, 8.220385215796485, 8.220493443554105, 8.221683854859213, 8.221683854859213, 8.222116688966114, 8.222549500288673, 8.222657699559681, 8.22276589740695, 8.22276589740695, 8.22298228883049, 8.223090482406874, 8.223090482406874, 8.223198674559745, 8.224064160542888, 8.224172339886644, 8.22428051780745, 8.22428051780745, 8.224388694305357, 8.224388694305357, 8.224605043032705, 8.224713215262257, 8.224821386069136, 8.224929555453397, 8.225037723415099, 8.22579485932002, 8.225903015903254, 8.226443777489717, 8.22655192554146, 8.226660072171482, 8.226660072171482, 8.226768217379842, 8.226876361166594, 8.226984503531796, 8.22720078399777, 8.22720078399777, 8.22720078399777, 8.227417058778212, 8.227417058778212, 8.227741460289485, 8.227849591284297, 8.227849591284297, 8.22795772085806, 8.228173975742672, 8.228282101053631, 8.22849834741314, 8.2286064684618, 8.228822706297214, 8.228822706297214, 8.228930823084077, 8.228930823084077, 8.229038938450456, 8.229038938450456, 8.229579493977006, 8.230011912835197, 8.230228113744506, 8.230228113744506, 8.230336212069412, 8.23055240446, 8.230768591171998, 8.230876682398668, 8.231200947562021, 8.231309033111108, 8.231417117240937, 8.231417117240937, 8.23152519995157, 8.23152519995157, 8.231633281243056, 8.231633281243056, 8.231957516603213, 8.232065592218683, 8.23217366641529, 8.232605949014186, 8.232605949014186, 8.233902660641006, 8.23411875939206, 8.234658981459114, 8.234658981459114, 8.234875060362876, 8.234983097688609, 8.235091133596985, 8.23519916808806, 8.235307201161891, 8.235307201161891, 8.23541523281853, 8.235523263058038, 8.235631291880466, 8.235631291880466, 8.235739319285871, 8.23595536984584, 8.236063393000514, 8.236387453963962, 8.236387453963962, 8.236603487523006, 8.237035537643296, 8.23800756756596, 8.238115563810975, 8.238871497888463, 8.239087466314462, 8.239303429079504, 8.239303429079504, 8.239411408339306, 8.239519386184032, 8.239519386184032, 8.239519386184032, 8.239519386184032, 8.240059254183523, 8.240275191479755, 8.240922969419163, 8.241138884084018, 8.241246839295142, 8.241462745475063, 8.241570696443972, 8.241570696443972, 8.242434253297313, 8.242865997797807, 8.242973930389557, 8.242973930389557, 8.243081861568067, 8.243189791333393, 8.243405646624717, 8.243837340251607, 8.243837340251607, 8.24405317858806, 8.24405317858806, 8.244161095637233, 8.244376925497749, 8.244592749708195, 8.244808568269013, 8.245024381180649, 8.245779681890463, 8.246103361016608, 8.246211251235321, 8.246427027438077, 8.246534913422233, 8.246858562906462, 8.247397950489393, 8.247397950489393, 8.247613695645418, 8.247721566107119, 8.247829435158021, 8.24815303384648, 8.248476619839732, 8.248908048085221, 8.249123753746597, 8.24933945376765, 8.249662993224593, 8.249878679146777, 8.249878679146777, 8.250094359430186, 8.250202197457488, 8.250417869283567, 8.25095702418493, 8.251280500215614, 8.251280500215614, 8.251280500215614, 8.251388322741152, 8.251603963565524, 8.252250852228977, 8.252358662076224, 8.252574277545504, 8.252574277545504, 8.252574277545504, 8.252682083167647, 8.252897690187217, 8.252897690187217, 8.253436683094536, 8.253652270401979, 8.253867852078379, 8.253975640805075, 8.254622343607682, 8.25473012248237, 8.25494567601017, 8.25494567601017, 8.255268995748764, 8.255484535206401, 8.255484535206401, 8.25570006903674, 8.255807833841812, 8.255915597240227, 8.256023359232037, 8.256131119817299, 8.256238878996069, 8.256238878996069, 8.256238878996069, 8.256454393134348, 8.25688540453541, 8.256993153870269, 8.257208648321884, 8.257208648321884, 8.257208648321884, 8.25731639343875, 8.257531879454872, 8.257531879454872, 8.257855097935863, 8.258501496947796, 8.258932401516033, 8.259040124145299, 8.259578716217614, 8.26000956458995, 8.260117273171613, 8.260224980348818, 8.260548093454238, 8.260655795014163, 8.260655795014163, 8.260655795014163, 8.260871193921515, 8.261086587212574, 8.261517356947566, 8.261517356947566, 8.26173273339238, 8.26173273339238, 8.262055787532478, 8.262055787532478, 8.262055787532478, 8.262271149941773, 8.262594183030652, 8.262917203490773, 8.263024874171347, 8.263347877795427, 8.263563206530668, 8.263563206530668, 8.2637785296551, 8.264101503822447, 8.2642091590733, 8.264962706565909, 8.265177993230786, 8.265177993230786, 8.265285634460378, 8.265608549738422, 8.265608549738422, 8.265716185361041, 8.26636196966496, 8.267007703522887, 8.267115320928902, 8.267115320928902, 8.267653386946648, 8.26797620974941, 8.268083814549342, 8.268944602538927, 8.268944602538927, 8.269159785535582, 8.269482549531784, 8.26991288193129, 8.270128039733986, 8.270343191939228, 8.270343191939228, 8.270450765942938, 8.270450765942938, 8.270558338547447, 8.270881047966325, 8.270988614974582, 8.270988614974582, 8.271203744794366, 8.271311307606005, 8.271418869018879, 8.271418869018879, 8.271526429033047, 8.271741544865469, 8.272171759748211, 8.27227930997294, 8.2723868587994, 8.2723868587994, 8.272709496889687, 8.272817040123602, 8.272817040123602, 8.272924581959517, 8.273139661437563, 8.273354735324267, 8.274214974966345, 8.274537541775318, 8.274752579328903, 8.275290148765984, 8.275290148765984, 8.275397658462943, 8.275827683284396, 8.276042687316057, 8.276257685762255, 8.27690264759242, 8.277010136344707, 8.277010136344707, 8.277117623701118, 8.277117623701118, 8.277225109661705, 8.277655039546929, 8.277762518529222, 8.278192420503988, 8.278407363119484, 8.278622300154302, 8.278729766579092, 8.278729766579092, 8.27905215748364, 8.27915961832873, 8.27915961832873, 8.27926707777903, 8.27926707777903, 8.279374535834597, 8.280234150075175, 8.28034159558057, 8.28044903969177, 8.28044903969177, 8.280771363660767, 8.280986239336798, 8.281201109437363, 8.281523404135163, 8.28173826029896, 8.28195311088881, 8.282167955905148, 8.282167955905148, 8.282275376323385, 8.282382795348406, 8.282490212980266, 8.282597629219017, 8.283349503887827, 8.283456908983672, 8.283671714997555, 8.283779115915703, 8.283779115915703, 8.284101310315618, 8.284208705664263, 8.284423492184883, 8.284423492184883, 8.285389962623379, 8.285712094384648, 8.285819468855367, 8.285819468855367, 8.286141583919168, 8.286141583919168, 8.286248952824495, 8.286356320338612, 8.286678414534226, 8.287000496210876, 8.28721521037438, 8.287429918974984, 8.287537271189333, 8.288074011403694, 8.288288697757007, 8.288288697757007, 8.288396038848358, 8.288503378549578, 8.288932723454218, 8.289040056205591, 8.289147387567155, 8.289147387567155, 8.289576699116415, 8.289791346553605, 8.290542568819562, 8.290757191250414, 8.291079114479748, 8.291079114479748, 8.291186419445223, 8.291937515322164, 8.292581257641721, 8.292581257641721, 8.292903110062396, 8.293117671403426, 8.293117671403426, 8.293224949992275, 8.293332227193421, 8.293439503006915, 8.29408312875023, 8.294190394851677, 8.294726704551683, 8.295262979577915, 8.296228187268676, 8.296228187268676, 8.296657132421089, 8.297300508571288, 8.2979438348377, 8.2979438348377, 8.298158265843224, 8.298587111231924, 8.298908730730592, 8.29955193233828, 8.299980705718689, 8.300195084102764, 8.30030227121846, 8.30030227121846, 8.300409456949994, 8.300516641297428, 8.300731005840191, 8.300731005840191, 8.300838186035632, 8.301909911884938, 8.301909911884938, 8.302124240452837, 8.302874346871018, 8.302874346871018, 8.303088650543847, 8.303302948685616, 8.303517241296754, 8.303517241296754, 8.303945809928848, 8.303945809928848, 8.305124259634622, 8.305552745344162, 8.305659863317578, 8.30576697990951, 8.305874095120018, 8.305874095120018, 8.306302542148835, 8.306409650453004, 8.306623862918075, 8.306838069859145, 8.30790902171944, 8.308123195525477, 8.308123195525477, 8.308444445882772, 8.30940812243542, 8.309943450009076, 8.309943450009076, 8.310264630000693, 8.310692850681162, 8.310906952747347, 8.310906952747347, 8.311656266547507, 8.312405512801494, 8.312405512801494, 8.312833623198236, 8.314010813122133, 8.314866846568055, 8.315401822720872, 8.315829778863444, 8.315936764457906, 8.316043748676003, 8.316685625083881, 8.31689957287799, 8.31689957287799, 8.317006544711186, 8.317434418286153, 8.317969229303756, 8.318504005937436, 8.318504005937436, 8.318824855416116, 8.319252635472214, 8.319680393532783, 8.319787329611522, 8.319894264315804, 8.320215060182448, 8.321498119981406, 8.321498119981406, 8.321925762621085, 8.321925762621085, 8.322246480179995, 8.322674084366799, 8.32310166658519, 8.32320855870762, 8.32406364627209, 8.325025514799583, 8.325239248269401, 8.326521533875507, 8.326842074423192, 8.327589954386477, 8.327696788898427, 8.327803622039834, 8.328124113241325, 8.32833776719057, 8.328551415658909, 8.328551415658909, 8.328978696154556, 8.329405954731637, 8.32972638428242, 8.32972638428242, 8.33036720640779, 8.330474005302754, 8.330580802828548, 8.33068759898522, 8.332716465899198, 8.333036768151178, 8.333036768151178, 8.334104353418276, 8.334317854061403, 8.335065063245812, 8.336239113789718, 8.336879435307987, 8.337199577625153, 8.337199577625153, 8.337199577625153, 8.3383733276275, 8.338586718968083, 8.339013485267506, 8.339120173429803, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.339973629595525, 8.341253650112554, 8.342746759164184, 8.3429600386437, 8.343386581246737, 8.346265173642307, 8.346691547342388, 8.346904726024976, 8.347331067057333, 8.348183683802443, 8.348716525050923, 8.34882308921976, 8.349462445671135, 8.349569000319908, 8.35042138856166, 8.352551978596729, 8.352871520228954, 8.353510566823722, 8.353936570728678, 8.354043068310613, 8.354256059401688, 8.355533891931568, 8.356172734918822, 8.356279204001702, 8.35691799001372, 8.357343820230048, 8.358301858895311, 8.358408300857493, 8.359046924167076, 8.360217606866684, 8.361707329618287, 8.363196787007983, 8.363303166671123, 8.365111413960458, 8.365962218622338, 8.368833045774226, 8.369045660462364, 8.369045660462364, 8.369683472120528, 8.37042752429658, 8.371490341259902, 8.374784216692188, 8.376058917493587, 8.377864411629263, 8.378289177234606, 8.379669516750909, 8.380518843449329, 8.381898815781966, 8.386355636037177, 8.391764307048511, 8.393248430559044, 8.396745683792728, 8.396957591720529, 8.400665115169405, 8.401088727953457, 8.40765199682076, 8.435436184922107]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gXuVxld_kGl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fkCadnCMsHNu",
        "outputId": "694a4808-f354-4253-eeb0-11e3c9200b3c"
      },
      "source": [
        "#from pyAudioAnalysis import MidTermFeatures as aF\n",
        "#from pyAudioAnalysis import audioTrainTest as aT\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import random\n",
        "import pandas as pd\n",
        "import glob\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import math\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import autokeras as ak\n",
        "import pickle\n",
        "import os\n",
        "import math\n",
        "\n",
        "def createProperData(baseDir, shortTermWindow, shortTermStep):\n",
        "  print('shortTermWindow: {} \\nshortTermStep: {}'.format(shortTermWindow,shortTermStep))\n",
        "  folder_name = baseDir \n",
        "  features2, _, filenames2 = aF.directory_feature_extraction(baseDir, 1.0, 1.0,shortTermWindow, shortTermStep)\n",
        "\n",
        "  f = features2\n",
        "  fn = _\n",
        "  feature_names = filenames2\n",
        "  features_Real = []\n",
        "  class_names = []\n",
        "  file_names = []\n",
        "\n",
        "  for i, d in enumerate([baseDir]):\n",
        "    if f.shape[0] > 0:\n",
        "        # if at least one audio file has been found in the provided folder:\n",
        "        features_Real.append(f)\n",
        "        file_names.append(fn)\n",
        "        if d[-1] == os.sep:\n",
        "            class_names.append(d.split(os.sep)[-2])\n",
        "        else:\n",
        "            class_names.append(d.split(os.sep)[-1])\n",
        "\n",
        "  features = features_Real\n",
        "  filenames = file_names\n",
        "\n",
        "  features = features[0]\n",
        "  filenames = [os.path.basename(f) for f in filenames[0]]\n",
        "  f_final = []\n",
        "\n",
        "  # Read CSVs:\n",
        "\n",
        "  csv_files = glob.glob(folder_name + os.sep + \"*.csv\")\n",
        "  regression_labels = []\n",
        "  regression_names = []\n",
        "  f_final = []\n",
        "  for c in csv_files:\n",
        "      cur_regression_labels = []\n",
        "      f_temp = []\n",
        "      # open the csv file that contains the current target value's annotations\n",
        "      with open(c, 'rt') as csvfile:\n",
        "          csv_reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "          for row in csv_reader:\n",
        "              if len(row) == 2:\n",
        "                  # ... and if the current filename exists\n",
        "                  # in the list of filenames\n",
        "                  if row[0] in filenames:\n",
        "                      index = filenames.index(row[0])\n",
        "                      cur_regression_labels.append(float(row[1]))\n",
        "                      f_temp.append(features[index, :])\n",
        "                  else:\n",
        "                      print(\"Warning: {} not found \"\n",
        "                            \"in list of files.\".format(row[0]))\n",
        "              else:\n",
        "                  print(\"Warning: Row with unknown format in regression file\")\n",
        "      f_final.append(np.array(f_temp))\n",
        "      # cur_regression_labels is the list of values\n",
        "      # for the current regression problem\n",
        "      regression_labels.append(np.array(cur_regression_labels))\n",
        "      # regression task name\n",
        "      regression_names.append(os.path.basename(c).replace(\".csv\", \"\"))\n",
        "      if len(features) == 0:\n",
        "          print(\"ERROR: No data found in any input folder!\")\n",
        "\n",
        "  #TRAIN SPLIT THE DATA SET\n",
        "\n",
        "  features_norm, mean, std = aT.normalize_features([f_final[0]])\n",
        "\n",
        "  first8 = []\n",
        "  for feature in features_norm[0]:\n",
        "    temp = []\n",
        "    temp.extend(feature[0:7])\n",
        "    temp.extend(feature[34:41])\n",
        "    temp.extend(feature[68:75])\n",
        "    temp.extend(feature[102:109])\n",
        "    print(len(temp))\n",
        "    first8.append(temp)\n",
        "  print(len(first8))\n",
        "  first8 = np.array([first8])\n",
        "  mfccFeatures = []\n",
        "  for feature in features_norm[0]:\n",
        "    temp = []\n",
        "    temp.extend(feature[8:20])\n",
        "    temp.extend(feature[42:54])\n",
        "    temp.extend(feature[76:88])\n",
        "    temp.extend(feature[110:122])\n",
        "    #,42:54,76:88,110:122]\n",
        "    #print(len(temp))\n",
        "    mfccFeatures.append(temp)\n",
        "  #print(len(mfccFeatures))\n",
        "  mfccFeatures = np.array([mfccFeatures])\n",
        "  chroma = []\n",
        "  for feature in features_norm[0]:\n",
        "    temp = []\n",
        "    temp.extend(feature[21:33])\n",
        "    temp.extend(feature[55:67])\n",
        "    temp.extend(feature[89:101])\n",
        "    temp.extend(feature[123:135])\n",
        "    chroma.append(temp)\n",
        "  chroma = np.array([chroma])\n",
        "  return [features_norm[0], first8,mfccFeatures,chroma], regression_labels[0]\n",
        "\n",
        "\n",
        "def train4Models(f_train, l_train, f_test, l_test,shortTermWindow, shortTermStep, _):\n",
        "  actual = l_test\n",
        "  classifiers = []\n",
        "  rmses = []\n",
        "  \"\"\"\n",
        "  f_train = StandardScaler().fit_transform(f_train)\n",
        "  #l_train = StandardScaler().fit_transform(l_train)\n",
        "\n",
        "  f_test = StandardScaler().fit_transform(f_test)\n",
        "  #l_test = StandardScaler().fit_transform(l_test)\n",
        "\n",
        "  f_train2 = f_train.copy()\n",
        "  f_test2 = f_test.copy()\n",
        "\n",
        "  pca = PCA(n_components=2)\n",
        "  f_train = pca.fit_transform(f_train2)\n",
        "  f_test = pca.fit_transform(f_test2)\n",
        "  \"\"\"\n",
        "\n",
        "  #RMSE SCORE\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "  for neighVal in range(1,10):\n",
        "    regr = RandomForestClassifier(max_depth=neighVal, random_state=0)\n",
        "    regr.fit(f_train, l_train)\n",
        "\n",
        "    predicted = []\n",
        "    for itest, fTest in enumerate(f_test):\n",
        "      R = regr.predict(fTest.reshape(1,-1))[0]\n",
        "      predicted.append(R)\n",
        "    mse = sklearn.metrics.mean_squared_error(actual, predicted)\n",
        "    rmse = math.sqrt(mse)\n",
        "\n",
        "    rmses.append([rmse,neighVal])\n",
        "    classifiers.append(regr)\n",
        "\n",
        "  regr = RandomForestClassifier(random_state=0)\n",
        "  regr.fit(f_train, l_train)\n",
        "\n",
        "  predicted = []\n",
        "  for itest, fTest in enumerate(f_test):\n",
        "    R = regr.predict(fTest.reshape(1,-1))[0]\n",
        "    predicted.append(R)\n",
        "  mse = sklearn.metrics.mean_squared_error(actual, predicted)\n",
        "  rmse = math.sqrt(mse)\n",
        "\n",
        "  rmses.append([rmse,neighVal])\n",
        "  classifiers.append(regr)\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "  \"\"\"\n",
        "  pca3 = PCA(n_components=3)\n",
        "  f_train = pca3.fit_transform(f_train2)\n",
        "  f_test = pca3.fit_transform(f_test2)\n",
        "\n",
        "  for neighVal in range(1,10):\n",
        "    regr = RandomForestRegressor(max_depth=neighVal, random_state=0)\n",
        "    regr.fit(f_train, l_train)\n",
        "\n",
        "    predicted = []\n",
        "    for itest, fTest in enumerate(f_test):\n",
        "      R = regr.predict(fTest.reshape(1,-1))[0]\n",
        "      predicted.append(R)\n",
        "    mse = sklearn.metrics.mean_squared_error(actual, predicted)\n",
        "    rmse = math.sqrt(mse)\n",
        "\n",
        "    rmses.append([rmse,neighVal])\n",
        "    classifiers.append(regr)\n",
        "\n",
        "  regr = RandomForestRegressor(random_state=0)\n",
        "  regr.fit(f_train, l_train)\n",
        "\n",
        "  predicted = []\n",
        "  for itest, fTest in enumerate(f_test):\n",
        "    R = regr.predict(fTest.reshape(1,-1))[0]\n",
        "    predicted.append(R)\n",
        "  mse = sklearn.metrics.mean_squared_error(actual, predicted)\n",
        "  rmse = math.sqrt(mse)\n",
        "\n",
        "  rmses.append([rmse,neighVal])\n",
        "  classifiers.append(regr)\n",
        "  \"\"\"\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "  return [shortTermWindow, shortTermStep, classifiers, rmses]\n",
        "\n",
        "def grapghPCA(f_train, l_train, f_test, l_test,shortTermWindow, shortTermStep):\n",
        "  actual = l_test\n",
        "  classifiers = []\n",
        "  rmses = []\n",
        "\n",
        "  f_train = StandardScaler().fit_transform(f_train)\n",
        "  #l_train = StandardScaler().fit_transform(l_train)\n",
        "\n",
        "  f_test = StandardScaler().fit_transform(f_test)\n",
        "  #l_test = StandardScaler().fit_transform(l_test)\n",
        "\n",
        "  pca = PCA(n_components=2)\n",
        "  principalComponents = pca.fit_transform(f_train)\n",
        "\n",
        "  principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
        "  labelsDf = pd.DataFrame(data = l_train, columns = ['target'])\n",
        "  finalDf = pd.concat([principalDf, labelsDf], axis = 1)\n",
        "\n",
        "  listOFRgb = []\n",
        "  for i in range(1,31):\n",
        "    rgb = (random.random(), random.random(), random.random())\n",
        "    listOFRgb.append(rgb)\n",
        "\n",
        "  print(len(listOFRgb))\n",
        "\n",
        "  fig = plt.figure(figsize = (8,8))\n",
        "  ax = fig.add_subplot(1,1,1) \n",
        "  ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "  ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "  ax.set_title('PCA: SW-{} SS-{}'.format(shortTermWindow, shortTermStep), fontsize = 20)\n",
        "\n",
        "  targets = list(range(1, 31))\n",
        "  colors = listOFRgb.copy()\n",
        "  for target, color in zip(targets,colors):\n",
        "      indicesToKeep = finalDf['target'] == target\n",
        "      ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
        "                , finalDf.loc[indicesToKeep, 'principal component 2']\n",
        "                , c = color\n",
        "                , s = 50)\n",
        "  ax.legend(targets)\n",
        "  ax.grid()\n",
        "  return [shortTermWindow, shortTermStep, classifiers, rmses]\n",
        "\n",
        "\n",
        "\n",
        "def threeGrapghPCA(f_train, l_train, f_test, l_test,shortTermWindow, shortTermStep):\n",
        "  actual = l_test\n",
        "  classifiers = []\n",
        "  rmses = []\n",
        "\n",
        "  \n",
        "  f_train = StandardScaler().fit_transform(f_train)\n",
        "  #l_train = StandardScaler().fit_transform(l_train)\n",
        "\n",
        "  f_test = StandardScaler().fit_transform(f_test)\n",
        "  #l_test = StandardScaler().fit_transform(l_test)\n",
        "\n",
        "  pca = PCA(n_components=3)\n",
        "  principalComponents = pca.fit_transform(f_train)\n",
        "  \n",
        "  principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2','principal component 3'])\n",
        "  labelsDf = pd.DataFrame(data = l_train, columns = ['target'])\n",
        "  finalDf = pd.concat([principalDf, labelsDf], axis = 1)\n",
        "\n",
        "  listOFRgb = []\n",
        "  for i in range(1,31):\n",
        "    rgb = (random.random(), random.random(), random.random())\n",
        "    listOFRgb.append(rgb)\n",
        "\n",
        "  print(len(listOFRgb))\n",
        "\n",
        "  fig = plt.figure(figsize = (8,8))\n",
        "  ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
        "  ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "  ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "  ax.set_zlabel('Principal Component 3', fontsize = 15)\n",
        "  ax.set_title('PCA: SW-{} SS-{}'.format(shortTermWindow, shortTermStep), fontsize = 20)\n",
        "\n",
        "  targets = list(range(1, 31))\n",
        "  colors = listOFRgb.copy()\n",
        "  for target, color in zip(targets,colors):\n",
        "      indicesToKeep = finalDf['target'] == target\n",
        "      ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
        "                , finalDf.loc[indicesToKeep, 'principal component 2'],\n",
        "                 finalDf.loc[indicesToKeep, 'principal component 3']\n",
        "                , c = color\n",
        "                , s = 50)\n",
        "  plt.show()\n",
        "  return [shortTermWindow, shortTermStep, classifiers, rmses]\n",
        "\n",
        "\n",
        "def autokerasTraining(trainFeats,trainLabels,testFeats,testLabels,shortTermWindow, shortTermStep,which4):\n",
        "  rmses = []\n",
        "  train_set = tf.data.Dataset.from_tensor_slices((trainFeats.astype(np.unicode), trainLabels))\n",
        "  test_set = tf.data.Dataset.from_tensor_slices((testFeats.astype(np.unicode), testLabels))\n",
        "\n",
        "  reg = ak.StructuredDataClassifier(max_trials=100, overwrite=True)\n",
        "  # Feed the tensorflow Dataset to the regressor.\n",
        "  #reg.fit(train_set)\n",
        "  reg.fit(np.matrix(trainFeats),trainLabels)\n",
        "  # Predict with the best model.\n",
        "  print(\"55555555555555555555555555555555555555555555555\")\n",
        "  rmse1 = math.sqrt(reg.evaluate(test_set)[0])\n",
        "  rmse2 = math.sqrt(reg.evaluate(test_set)[1])\n",
        "  print(rmse1)\n",
        "  print(rmse2)\n",
        "  rmses.extend([rmse1,rmse2,which4])\n",
        "  model = reg.export_model()\n",
        "\n",
        "  print(type(model))  # <class 'tensorflow.python.keras.engine.training.Model'>\n",
        "\n",
        "  try:\n",
        "      model.save(\"./classifierModelSaves/100-SS{}-SW{}-{}\".format(shortTermStep,shortTermWindow,which4), save_format=\"tf\")\n",
        "  except Exception:\n",
        "      model.save(\"./classifierModelSaves/100-SS{}-SW{}-{}.h5\".format(shortTermStep,shortTermWindow,which4))\n",
        "\n",
        "  return [shortTermWindow, shortTermStep, rmses]\n",
        "\n",
        "def tryDifferentWindows():\n",
        "  listOFEverything22 = []\n",
        "  #listOFEverything22 = pickle.load(open(\"ak-100.pickle\",'rb'))\n",
        "  #print(listOFEverything22[len(listOFEverything22) - 1][0])\n",
        "  #print(listOFEverything22[len(listOFEverything22) - 1][1])\n",
        "  notFirstRun = False\n",
        "  valMet = True\n",
        "  listOFEverything = listOFEverything22.copy()\n",
        "  shortTermStepList = [.001, 0.005, 0.01,0.015, 0.02]\n",
        "  for shortTermStep in shortTermStepList:\n",
        "    for shortTermWindow in range(25,105,5):\n",
        "      \"\"\"\n",
        "      if ((listOFEverything22[len(listOFEverything22) - 1][0] == (float(shortTermWindow)/1000)) and (listOFEverything22[len(listOFEverything22) - 1][1] == shortTermStep)):\n",
        "        print('in')\n",
        "        notFirstRun = True\n",
        "      \n",
        "      if (notFirstRun):\n",
        "      \"\"\"\n",
        "      lof = len(listOFEverything22) - 1\n",
        "      print(shortTermStep)\n",
        "      print(str(shortTermWindow) + \"\\n\")\n",
        "      trainDir = './real mmse scores'\n",
        "      testDir = './test-mmse'\n",
        "      shortTermWindow = float(shortTermWindow)/1000\n",
        "      \n",
        "      temp = pickle.load(open(\"./pickles/train/SS{}-SW{}.pickle\".format(shortTermStep, shortTermWindow), 'rb'))\n",
        "      trainFeats = temp[0]\n",
        "      trainLabels = temp[1]\n",
        "\n",
        "      temp = pickle.load(open(\"./pickles/test/SS{}-SW{}.pickle\".format(shortTermStep, shortTermWindow), 'rb'))\n",
        "      testFeats = temp[0]\n",
        "      testLabels = temp[1]\n",
        "\n",
        "      print(trainFeats[0].shape)\n",
        "      for i in range(4):\n",
        "        #listOFEverything.append(autokerasTraining(np.matrix(trainFeats[i]), trainLabels,np.matrix(testFeats[i]), testLabels,shortTermWindow, shortTermStep, i))\n",
        "        listOFEverything.append(autokerasTraining(trainFeats[i], trainLabels,testFeats[i], testLabels,shortTermWindow, shortTermStep, i))\n",
        "      \n",
        "      \n",
        "      with open(\"ak-classifier-1-100.pickle\", 'wb') as handle:\n",
        "        pickle.dump(listOFEverything, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        \n",
        "\n",
        "  return listOFEverything\n",
        "\n",
        "listOFEverything = tryDifferentWindows()\n",
        "\n",
        "with open(\"ak-classifier-1-100.pickle\", 'wb') as handle:\n",
        "  pickle.dump(listOFEverything, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.001\n",
            "25\n",
            "\n",
            "(2798, 138)\n",
            "\n",
            "Search: Running Trial #1\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "structured_data...|True              |?                 \n",
            "structured_data...|2                 |?                 \n",
            "structured_data...|False             |?                 \n",
            "structured_data...|0                 |?                 \n",
            "structured_data...|32                |?                 \n",
            "structured_data...|32                |?                 \n",
            "classification_...|0                 |?                 \n",
            "optimizer         |adam              |?                 \n",
            "learning_rate     |0.001             |?                 \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[1;32mC:\\Users\\THEGAM~1\\AppData\\Local\\Temp/ipykernel_54876/2926295655.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    380\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mlistOFEverything\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m \u001b[0mlistOFEverything\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtryDifferentWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ak-classifier-1-100.pickle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Users\\THEGAM~1\\AppData\\Local\\Temp/ipykernel_54876/2926295655.py\u001b[0m in \u001b[0;36mtryDifferentWindows\u001b[1;34m()\u001b[0m\n\u001b[0;32m    371\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;31m#listOFEverything.append(autokerasTraining(np.matrix(trainFeats[i]), trainLabels,np.matrix(testFeats[i]), testLabels,shortTermWindow, shortTermStep, i))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m         \u001b[0mlistOFEverything\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mautokerasTraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainFeats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestFeats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestLabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshortTermWindow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshortTermStep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Users\\THEGAM~1\\AppData\\Local\\Temp/ipykernel_54876/2926295655.py\u001b[0m in \u001b[0;36mautokerasTraining\u001b[1;34m(trainFeats, trainLabels, testFeats, testLabels, shortTermWindow, shortTermStep, which4)\u001b[0m\n\u001b[0;32m    316\u001b[0m   \u001b[1;31m# Feed the tensorflow Dataset to the regressor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m   \u001b[1;31m#reg.fit(train_set)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m   \u001b[0mreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainFeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainLabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m   \u001b[1;31m# Predict with the best model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"55555555555555555555555555555555555555555555555\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\autokeras\\tasks\\structured_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, epochs, callbacks, validation_split, validation_data, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m                 \u001b[0mvalidation\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mvalidation\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mif\u001b[0m \u001b[0mapplicable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \"\"\"\n\u001b[1;32m--> 326\u001b[1;33m         history = super().fit(\n\u001b[0m\u001b[0;32m    327\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\autokeras\\tasks\\structured_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, epochs, callbacks, validation_split, validation_data, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_in_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m         history = super().fit(\n\u001b[0m\u001b[0;32m    140\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\autokeras\\auto_model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, callbacks, validation_split, validation_data, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m             )\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m         history = self.tuner.search(\n\u001b[0m\u001b[0;32m    285\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\autokeras\\engine\\tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, epochs, callbacks, validation_split, verbose, **fit_kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         super().search(\n\u001b[0m\u001b[0;32m    188\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         )\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"callbacks\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\autokeras\\engine\\tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[1;34m(self, trial, fit_args, fit_kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         _, history = utils.fit_with_adaptive_batch_size(\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\autokeras\\engine\\tuner.py\u001b[0m in \u001b[0;36madapt\u001b[1;34m(model, dataset)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPreprocessingLayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                     \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m                 \u001b[0mtemp_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                 \u001b[0mlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_output_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\autokeras\\keras_layers.py\u001b[0m in \u001b[0;36madapt\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mdata_column\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mencoding_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_column\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast_to_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\preprocessing\\index_lookup.py\u001b[0m in \u001b[0;36madapt\u001b[1;34m(self, data, reset_state)\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mreset_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"IndexLookup does not support streaming adapts.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIndexLookup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_preprocessing_layer.py\u001b[0m in \u001b[0;36madapt\u001b[1;34m(self, data, batch_size, steps, reset_state)\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mreset_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adapt_accumulator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_combiner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restore_updates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m     super(CombinerPreprocessingLayer, self).adapt(\n\u001b[0m\u001b[0;32m    331\u001b[0m         data, batch_size=batch_size, steps=steps, reset_state=reset_state)\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_preprocessing_layer.py\u001b[0m in \u001b[0;36madapt\u001b[1;34m(self, data, batch_size, steps, reset_state)\u001b[0m\n\u001b[0;32m    240\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adapt_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_preprocessing_layer.py\u001b[0m in \u001b[0;36madapt_step\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madapt_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m       \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adapt_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    759\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[0;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2725\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2726\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2727\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2728\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2729\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6896\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6897\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6898\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\thegamer1123\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
            "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[0] = -1 is not in [0, 24)\n\t [[{{node embedding_lookup}}]] [Op:IteratorGetNext]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "zloA53sEjIp3",
        "outputId": "c592e6ca-7ac2-4d7f-824f-8fdbc255200e"
      },
      "source": [
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "listOFWavs = os.listdir('./test-mmse')\n",
        "dictOfWavs = {}\n",
        "for wav in listOFWavs:\n",
        "  if \".wav\" in wav:\n",
        "    num = wav.split('-')[0]\n",
        "    x = wav.split('-')[2:4]\n",
        "    try:\n",
        "      currentList = dictOfWavs[num + x[0] + x[1]]\n",
        "      currentList.append(wav)\n",
        "    except:\n",
        "      temp = [wav]\n",
        "      dictOfWavs[num + x[0] + x[1]] = temp\n",
        "\n",
        "print(dictOfWavs)\n",
        "\n",
        "for wav in listOFWavs:\n",
        "  if \".wav\" in wav:\n",
        "    num = wav.split('-')[0]\n",
        "    x = wav.split('-')[2:4]\n",
        "    currentList = dictOfWavs[num + x[0] + x[1]]\n",
        "    temp = []\n",
        "    for i in currentList:\n",
        "      sound1 = AudioSegment.from_wav('./test-mmse/{}'.format(i))\n",
        "      temp.append(sound1)\n",
        "    \n",
        "    combined_sounds = None\n",
        "\n",
        "    for i in temp:\n",
        "      if combined_sounds == None:\n",
        "        combined_sounds = i\n",
        "      else:\n",
        "        combined_sounds += i\n",
        "\n",
        "    combined_sounds.export(\"./combinedTestWavs/{}-{}-{}.wav\".format(num,x[0],x[1]), format=\"wav\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "sound1 = AudioSegment.from_wav(\"/path/to/file1.wav\")\n",
        "sound2 = AudioSegment.from_wav(\"/path/to/file2.wav\")\n",
        "\n",
        "combined_sounds = sound1 + sound2\n",
        "combined_sounds.export(\"/output/path.wav\", format=\"wav\")\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'S16003957': ['S160-211-0-3957-1-380-1010.wav', 'S160-211-0-3957-2-1810-3960.wav'], 'S1601282615114': ['S160-211-12826-15114-1-310-2160.wav'], 'S1601511420622': ['S160-211-15114-20622-1-340-2670.wav', 'S160-211-15114-20622-2-2820-3690.wav', 'S160-211-15114-20622-3-3740-5510.wav'], 'S1602062224498': ['S160-211-20622-24498-1-2960-3710.wav'], 'S160395712857': ['S160-211-3957-12857-1-320-1010.wav', 'S160-211-3957-12857-2-1050-1900.wav', 'S160-211-3957-12857-3-2000-4820.wav', 'S160-211-3957-12857-4-4950-5920.wav', 'S160-211-3957-12857-5-7840-8900.wav'], 'S16108530': ['S161-42-0-8530-1-250-810.wav', 'S161-42-0-8530-2-1060-1620.wav', 'S161-42-0-8530-3-1690-2000.wav', 'S161-42-0-8530-4-2130-2460.wav', 'S161-42-0-8530-5-2540-3080.wav', 'S161-42-0-8530-6-3180-4500.wav', 'S161-42-0-8530-7-4610-6100.wav', 'S161-42-0-8530-8-6240-8530.wav'], 'S1611193815645': ['S161-42-11938-15645-1-170-1570.wav', 'S161-42-11938-15645-2-2220-3710.wav'], 'S1611564518365': ['S161-42-15645-18365-1-0-2680.wav'], 'S1611836520234': ['S161-42-18365-20234-1-130-1870.wav'], 'S1614983355717': ['S161-42-49833-55717-1-0-5890.wav'], 'S1615571758690': ['S161-42-55717-58690-1-0-2980.wav'], 'S1615869063629': ['S161-42-58690-63629-1-60-1050.wav', 'S161-42-58690-63629-2-1070-1450.wav', 'S161-42-58690-63629-3-1910-2590.wav', 'S161-42-58690-63629-4-2680-4940.wav'], 'S1616362966500': ['S161-42-63629-66500-1-0-2880.wav'], 'S1616650070518': ['S161-42-66500-70518-1-0-4019.wav'], 'S1617051872604': ['S161-42-70518-72604-1-0-2090.wav'], 'S161853011938': ['S161-42-8530-11938-1-360-690.wav', 'S161-42-8530-11938-2-720-2430.wav', 'S161-42-8530-11938-3-2480-3410.wav'], 'S1621378016730': ['S162-214-13780-16730-1-610-2870.wav'], 'S1621673020296': ['S162-214-16730-20296-1-1400-3570.wav'], 'S1622029624145': ['S162-214-20296-24145-1-90-1510.wav', 'S162-214-20296-24145-2-1900-2210.wav', 'S162-214-20296-24145-3-2260-3790.wav'], 'S1622414532160': ['S162-214-24145-32160-1-1090-2630.wav', 'S162-214-24145-32160-2-3920-4630.wav', 'S162-214-24145-32160-3-5140-6290.wav', 'S162-214-24145-32160-4-6880-8020.wav'], 'S1623216036236': ['S162-214-32160-36236-1-270-1870.wav', 'S162-214-32160-36236-2-2310-3900.wav'], 'S1623623640368': ['S162-214-36236-40368-1-380-1250.wav', 'S162-214-36236-40368-2-1790-2230.wav', 'S162-214-36236-40368-3-2550-4140.wav'], 'S1624036844916': ['S162-214-40368-44916-1-2800-4350.wav'], 'S1624491651331': ['S162-214-44916-51331-1-480-920.wav', 'S162-214-44916-51331-2-1010-1330.wav', 'S162-214-44916-51331-3-2820-4590.wav', 'S162-214-44916-51331-4-4610-5060.wav', 'S162-214-44916-51331-5-5130-6360.wav'], 'S1625133152963': ['S162-214-51331-52963-1-300-1640.wav'], 'S1625296357013': ['S162-214-52963-57013-1-0-1400.wav', 'S162-214-52963-57013-2-2090-2410.wav', 'S162-214-52963-57013-3-3150-4050.wav'], 'S1625701360913': ['S162-214-57013-60913-1-1180-1970.wav', 'S162-214-57013-60913-2-2320-3900.wav'], 'S1626091364843': ['S162-214-60913-64843-1-2260-3930.wav'], 'S1626484368606': ['S162-214-64843-68606-1-1190-3630.wav'], 'S1626860671638': ['S162-214-68606-71638-1-1310-3040.wav'], 'S1627163872967': ['S162-214-71638-72967-1-570-1330.wav'], 'S1631140016529': ['S163-179-11400-16529-1-160-5130.wav'], 'S1631652921200': ['S163-179-16529-21200-1-340-4650.wav'], 'S1632120027004': ['S163-179-21200-27004-1-220-1620.wav', 'S163-179-21200-27004-2-1780-5810.wav'], 'S1632700437689': ['S163-179-27004-37689-1-20-330.wav', 'S163-179-27004-37689-2-420-1790.wav', 'S163-179-27004-37689-3-2220-3580.wav', 'S163-179-27004-37689-4-4070-5210.wav', 'S163-179-27004-37689-5-5280-7520.wav', 'S163-179-27004-37689-6-8070-10690.wav'], 'S16339728850': ['S163-179-3972-8850-1-1110-3010.wav', 'S163-179-3972-8850-2-3070-4880.wav'], 'S1634210046800': ['S163-179-42100-46800-1-0-2540.wav', 'S163-179-42100-46800-2-2800-4700.wav'], 'S1634680056300': ['S163-179-46800-56300-1-0-320.wav', 'S163-179-46800-56300-2-550-3080.wav', 'S163-179-46800-56300-3-3380-5310.wav', 'S163-179-46800-56300-4-5760-7560.wav', 'S163-179-46800-56300-5-7580-9500.wav'], 'S1635630061000': ['S163-179-56300-61000-1-340-4550.wav'], 'S1636100064109': ['S163-179-61000-64109-1-150-1570.wav', 'S163-179-61000-64109-2-2080-3110.wav'], 'S1636410966303': ['S163-179-64109-66303-1-930-2200.wav'], 'S1636630368698': ['S163-179-66303-68698-1-590-2400.wav'], 'S1636869870977': ['S163-179-68698-70977-1-150-730.wav', 'S163-179-68698-70977-2-790-1120.wav', 'S163-179-68698-70977-3-1260-2280.wav'], 'S1637097772750': ['S163-179-70977-72750-1-0-1780.wav'], 'S1637275074735': ['S163-179-72750-74735-1-0-340.wav', 'S163-179-72750-74735-2-1210-1990.wav'], 'S163895011400': ['S163-179-8950-11400-1-320-2150.wav'], 'S16406759': ['S164-176-0-6759-1-2510-3000.wav', 'S164-176-0-6759-2-5220-5650.wav'], 'S1641192314648': ['S164-176-11923-14648-1-0-2730.wav'], 'S1641464816664': ['S164-176-14648-16664-1-0-2020.wav'], 'S1641666418111': ['S164-176-16664-18111-1-0-1450.wav'], 'S1641811118727': ['S164-176-18111-18727-1-0-620.wav'], 'S1641872724016': ['S164-176-18727-24016-1-60-3070.wav', 'S164-176-18727-24016-2-3080-5290.wav'], 'S1642401627600': ['S164-176-24016-27600-1-0-3590.wav'], 'S164675911923': ['S164-176-6759-11923-1-2470-3180.wav', 'S164-176-6759-11923-2-4150-4540.wav'], 'S1651255515783': ['S165-132-12555-15783-1-210-920.wav', 'S165-132-12555-15783-2-1040-1730.wav', 'S165-132-12555-15783-3-2100-2490.wav'], 'S1651578321000': ['S165-132-15783-21000-1-0-1450.wav', 'S165-132-15783-21000-2-1490-2400.wav', 'S165-132-15783-21000-3-2660-3790.wav'], 'S1652171828077': ['S165-132-21718-28077-1-1370-2660.wav', 'S165-132-21718-28077-2-3970-4880.wav'], 'S1652807734660': ['S165-132-28077-34660-1-5090-5590.wav', 'S165-132-28077-34660-2-5610-6590.wav'], 'S1653466038483': ['S165-132-34660-38483-1-2020-2420.wav', 'S165-132-34660-38483-2-2480-3340.wav'], 'S1653848344669': ['S165-132-38483-44669-1-5120-5800.wav', 'S165-132-38483-44669-2-5830-6180.wav'], 'S1654466947360': ['S165-132-44669-47360-1-770-2280.wav', 'S165-132-44669-47360-2-2340-2700.wav'], 'S1654859153535': ['S165-132-48591-53535-1-1340-2180.wav'], 'S1655353556323': ['S165-132-53535-56323-1-350-2790.wav'], 'S16558707815': ['S165-132-5870-7815-1-80-1950.wav'], 'S1656711368717': ['S165-132-67113-68717-1-0-840.wav'], 'S1657181277055': ['S165-132-71812-77055-1-3640-3970.wav', 'S165-132-71812-77055-2-4310-5250.wav'], 'S1657705577896': ['S165-132-77055-77896-1-440-850.wav'], 'S16578158246': ['S165-132-7815-8246-1-30-440.wav'], 'S1658015386423': ['S165-132-80153-86423-1-4090-5620.wav', 'S165-132-80153-86423-2-5830-6270.wav'], 'S1659017496000': ['S165-132-90174-96000-1-4380-5110.wav'], 'S166102050106922': ['S166-105-102050-106922-1-360-1270.wav', 'S166-105-102050-106922-2-1900-4880.wav'], 'S166106922108228': ['S166-105-106922-108228-1-0-1310.wav'], 'S166108228109999': ['S166-105-108228-109999-1-0-1780.wav'], 'S166109999115857': ['S166-105-109999-115857-1-1200-5730.wav'], 'S166115857119668': ['S166-105-115857-119668-1-360-3690.wav'], 'S1661181312444': ['S166-105-11813-12444-1-0-640.wav'], 'S166119668122656': ['S166-105-119668-122656-1-330-2990.wav'], 'S166122656128770': ['S166-105-122656-128770-1-750-5820.wav'], 'S1661244418909': ['S166-105-12444-18909-1-860-6470.wav'], 'S166134016140834': ['S166-105-134016-140834-1-2020-2930.wav', 'S166-105-134016-140834-2-3260-5570.wav', 'S166-105-134016-140834-3-5700-6040.wav'], 'S166140834149371': ['S166-105-140834-149371-1-1880-2640.wav', 'S166-105-140834-149371-2-7590-8540.wav'], 'S166149371160002': ['S166-105-149371-160002-1-0-2370.wav', 'S166-105-149371-160002-2-2730-3360.wav', 'S166-105-149371-160002-3-3800-4800.wav', 'S166-105-149371-160002-4-5640-9080.wav', 'S166-105-149371-160002-5-9100-10440.wav'], 'S166160002165760': ['S166-105-160002-165760-1-470-5180.wav'], 'S16617383144': ['S166-105-1738-3144-1-0-360.wav', 'S166-105-1738-3144-2-400-1410.wav'], 'S1661890925333': ['S166-105-18909-25333-1-0-380.wav', 'S166-105-18909-25333-2-740-6430.wav'], 'S1662533328900': ['S166-105-25333-28900-1-10-2009.wav', 'S166-105-25333-28900-2-2470-3570.wav'], 'S1662890035783': ['S166-105-28900-35783-1-0-1470.wav', 'S166-105-28900-35783-2-1700-2080.wav', 'S166-105-28900-35783-3-2100-2630.wav', 'S166-105-28900-35783-4-3170-6890.wav'], 'S1663578340372': ['S166-105-35783-40372-1-350-4590.wav'], 'S1664373345222': ['S166-105-43733-45222-1-120-1490.wav'], 'S1664522246250': ['S166-105-45222-46250-1-0-1030.wav'], 'S16650806693': ['S166-105-5080-6693-1-0-1620.wav'], 'S1665501738': ['S166-105-550-1738-1-100-1190.wav'], 'S1665545562888': ['S166-105-55455-62888-1-690-3660.wav', 'S166-105-55455-62888-2-4070-7440.wav'], 'S1666288867392': ['S166-105-62888-67392-1-0-690.wav', 'S166-105-62888-67392-2-960-4510.wav'], 'S16666939884': ['S166-105-6693-9884-1-0-3200.wav'], 'S1666739278690': ['S166-105-67392-78690-1-450-3320.wav', 'S166-105-67392-78690-2-3690-11300.wav'], 'S1667869080619': ['S166-105-78690-80619-1-470-1930.wav'], 'S1668061981782': ['S166-105-80619-81782-1-380-1170.wav'], 'S1668178287414': ['S166-105-81782-87414-1-110-870.wav', 'S166-105-81782-87414-2-1260-5640.wav'], 'S1668741488836': ['S166-105-87414-88836-1-150-1430.wav'], 'S1668883691659': ['S166-105-88836-91659-1-0-320.wav', 'S166-105-88836-91659-2-680-2830.wav'], 'S16691659102050': ['S166-105-91659-102050-1-760-3640.wav', 'S166-105-91659-102050-2-3810-10050.wav'], 'S167110828148000': ['S167-167-110828-148000-1-7480-7790.wav', 'S167-167-110828-148000-2-15110-17640.wav', 'S167-167-110828-148000-3-18480-18800.wav', 'S167-167-110828-148000-4-21320-21850.wav', 'S167-167-110828-148000-5-25910-26710.wav', 'S167-167-110828-148000-6-34080-34400.wav'], 'S167148000151227': ['S167-167-148000-151227-1-1540-2590.wav'], 'S167172005172560': ['S167-167-172005-172560-1-340-560.wav'], 'S167174388176861': ['S167-167-174388-176861-1-860-1370.wav', 'S167-167-174388-176861-2-1490-1980.wav'], 'S167178513180412': ['S167-167-178513-180412-1-440-1020.wav', 'S167-167-178513-180412-2-1220-1530.wav'], 'S167180412185876': ['S167-167-180412-185876-1-1810-2510.wav', 'S167-167-180412-185876-2-2520-3800.wav', 'S167-167-180412-185876-3-4420-5470.wav'], 'S167185876190327': ['S167-167-185876-190327-1-1800-2390.wav', 'S167-167-185876-190327-2-2410-2960.wav', 'S167-167-185876-190327-3-3580-4190.wav'], 'S16718859809': ['S167-167-1885-9809-1-0-430.wav', 'S167-167-1885-9809-2-650-5460.wav', 'S167-167-1885-9809-3-5610-7710.wav'], 'S167190327193394': ['S167-167-190327-193394-1-1770-2100.wav', 'S167-167-190327-193394-2-2450-3070.wav'], 'S167193394199962': ['S167-167-193394-199962-1-1540-2830.wav', 'S167-167-193394-199962-2-3110-4410.wav', 'S167-167-193394-199962-3-4810-6320.wav'], 'S167201248209314': ['S167-167-201248-209314-1-690-1210.wav', 'S167-167-201248-209314-2-5760-6130.wav', 'S167-167-201248-209314-3-6240-7550.wav'], 'S167213064214803': ['S167-167-213064-214803-1-1100-1740.wav'], 'S167215510219367': ['S167-167-215510-219367-1-990-2460.wav', 'S167-167-215510-219367-2-2530-3860.wav'], 'S167219367220686': ['S167-167-219367-220686-1-0-650.wav', 'S167-167-219367-220686-2-840-1320.wav'], 'S167221358226445': ['S167-167-221358-226445-1-1150-1550.wav', 'S167-167-221358-226445-2-3580-4620.wav', 'S167-167-221358-226445-3-4630-5090.wav'], 'S167226444229071': ['S167-167-226444-229071-1-310-1280.wav'], 'S167229071234991': ['S167-167-229071-234991-1-780-1570.wav', 'S167-167-229071-234991-2-1740-2230.wav', 'S167-167-229071-234991-3-2700-3620.wav', 'S167-167-229071-234991-4-3770-4280.wav', 'S167-167-229071-234991-5-4910-5260.wav', 'S167-167-229071-234991-6-5470-5860.wav'], 'S167234991246500': ['S167-167-234991-246500-1-5770-6740.wav', 'S167-167-234991-246500-2-8530-8850.wav', 'S167-167-234991-246500-3-10410-10730.wav'], 'S1672408846364': ['S167-167-24088-46364-1-7400-7750.wav', 'S167-167-24088-46364-2-11020-11460.wav', 'S167-167-24088-46364-3-13150-13460.wav', 'S167-167-24088-46364-4-14300-14610.wav', 'S167-167-24088-46364-5-17360-17680.wav', 'S167-167-24088-46364-6-21230-21590.wav', 'S167-167-24088-46364-7-21670-22210.wav'], 'S167246500250053': ['S167-167-246500-250053-1-700-2009.wav', 'S167-167-246500-250053-2-2160-3000.wav'], 'S167250053253976': ['S167-167-250053-253976-1-340-2300.wav', 'S167-167-250053-253976-2-2390-2730.wav', 'S167-167-250053-253976-3-2840-3570.wav'], 'S167253976259645': ['S167-167-253976-259645-1-3400-3790.wav', 'S167-167-253976-259645-2-4070-4440.wav', 'S167-167-253976-259645-3-4540-5100.wav'], 'S167259645265513': ['S167-167-259645-265513-1-580-1870.wav', 'S167-167-259645-265513-2-3330-4290.wav'], 'S167265513267007': ['S167-167-265513-267007-1-1200-1500.wav'], 'S1674636450784': ['S167-167-46364-50784-1-300-680.wav', 'S167-167-46364-50784-2-1310-2370.wav', 'S167-167-46364-50784-3-2500-3270.wav'], 'S1677547076285': ['S167-167-75470-76285-1-510-820.wav'], 'S1677711578036': ['S167-167-77115-78036-1-0-830.wav'], 'S1677803687045': ['S167-167-78036-87045-1-2120-2890.wav', 'S167-167-78036-87045-2-5330-6480.wav', 'S167-167-78036-87045-3-6640-7130.wav', 'S167-167-78036-87045-4-7200-7560.wav', 'S167-167-78036-87045-5-8470-8970.wav'], 'S16787045110828': ['S167-167-87045-110828-1-14040-14480.wav', 'S167-167-87045-110828-2-14620-15010.wav', 'S167-167-87045-110828-3-15060-15380.wav', 'S167-167-87045-110828-4-15700-16030.wav', 'S167-167-87045-110828-5-16430-18480.wav', 'S167-167-87045-110828-6-21490-22900.wav'], 'S167980924088': ['S167-167-9809-24088-1-390-780.wav', 'S167-167-9809-24088-10-13590-14100.wav', 'S167-167-9809-24088-2-3710-4050.wav', 'S167-167-9809-24088-3-5640-5950.wav', 'S167-167-9809-24088-4-6400-7370.wav', 'S167-167-9809-24088-5-8200-9340.wav', 'S167-167-9809-24088-6-9830-11560.wav', 'S167-167-9809-24088-7-11650-12040.wav', 'S167-167-9809-24088-8-12460-12820.wav', 'S167-167-9809-24088-9-13200-13530.wav'], 'S168011394': ['S168-51-0-11394-1-1140-2470.wav', 'S168-51-0-11394-2-4420-5540.wav', 'S168-51-0-11394-3-7040-9120.wav', 'S168-51-0-11394-4-10160-11380.wav'], 'S1681139416202': ['S168-51-11394-16202-1-1460-2410.wav', 'S168-51-11394-16202-2-2540-4810.wav'], 'S1681620221324': ['S168-51-16202-21324-1-2850-5030.wav'], 'S1682132425281': ['S168-51-21324-25281-1-2510-3960.wav'], 'S1682528135967': ['S168-51-25281-35967-1-150-940.wav', 'S168-51-25281-35967-2-2340-6590.wav', 'S168-51-25281-35967-3-8990-10430.wav'], 'S1691220415223': ['S169-285-12204-15223-1-1170-2570.wav'], 'S1691522319034': ['S169-285-15223-19034-1-260-3640.wav'], 'S1691903423098': ['S169-285-19034-23098-1-1530-3750.wav'], 'S1692309827456': ['S169-285-23098-27456-1-2220-4300.wav'], 'S1692745640512': ['S169-285-27456-40512-1-6660-7830.wav', 'S169-285-27456-40512-2-8250-9160.wav', 'S169-285-27456-40512-3-9260-10890.wav'], 'S1696360064857': ['S169-285-63600-64857-1-270-1260.wav'], 'S169740912204': ['S169-285-7409-12204-1-1390-2430.wav', 'S169-285-7409-12204-2-3260-4730.wav'], 'S17007099': ['S170-192-0-7099-1-2890-3700.wav', 'S170-192-0-7099-2-3950-5260.wav', 'S170-192-0-7099-3-5310-6110.wav', 'S170-192-0-7099-4-6470-6780.wav'], 'S1701775840528': ['S170-192-17758-40528-1-1220-1580.wav', 'S170-192-17758-40528-2-19960-20270.wav', 'S170-192-17758-40528-3-20740-21140.wav'], 'S170709917758': ['S170-192-7099-17758-1-3300-3910.wav', 'S170-192-7099-17758-2-5750-7250.wav', 'S170-192-7099-17758-3-8280-9120.wav', 'S170-192-7099-17758-4-9400-10000.wav', 'S170-192-7099-17758-5-10070-10540.wav'], 'S1711783626000': ['S171-298-17836-26000-1-720-1830.wav', 'S171-298-17836-26000-2-2410-3240.wav', 'S171-298-17836-26000-3-4400-5730.wav', 'S171-298-17836-26000-4-5830-7270.wav'], 'S1712600042894': ['S171-298-26000-42894-1-150-1500.wav', 'S171-298-26000-42894-2-6060-6490.wav', 'S171-298-26000-42894-3-9520-10020.wav', 'S171-298-26000-42894-4-10790-13290.wav', 'S171-298-26000-42894-5-13830-14510.wav', 'S171-298-26000-42894-6-14890-15470.wav', 'S171-298-26000-42894-7-15680-16620.wav'], 'S1714289451982': ['S171-298-42894-51982-1-4230-6200.wav', 'S171-298-42894-51982-2-6580-8060.wav'], 'S1715198266666': ['S171-298-51982-66666-1-12940-14690.wav'], 'S1716666667526': ['S171-298-66666-67526-1-0-860.wav'], 'S1721175018750': ['S172-238-11750-18750-1-0-4010.wav', 'S172-238-11750-18750-2-4300-7000.wav'], 'S1721875025510': ['S172-238-18750-25510-1-30-1140.wav', 'S172-238-18750-25510-2-1580-5100.wav', 'S172-238-18750-25510-3-5150-6730.wav'], 'S1722551028354': ['S172-238-25510-28354-1-530-1640.wav', 'S172-238-25510-28354-2-1720-2850.wav'], 'S1722835431111': ['S172-238-28354-31111-1-710-2760.wav'], 'S1723111136133': ['S172-238-31111-36133-1-40-710.wav', 'S172-238-31111-36133-2-1300-3730.wav', 'S172-238-31111-36133-3-4430-5030.wav'], 'S1723613338692': ['S172-238-36133-38692-1-90-400.wav', 'S172-238-36133-38692-2-1120-2560.wav'], 'S1723869241600': ['S172-238-38692-41600-1-60-2910.wav'], 'S1724160046568': ['S172-238-41600-46568-1-0-2060.wav', 'S172-238-41600-46568-2-2470-4750.wav'], 'S1724656852153': ['S172-238-46568-52153-1-700-5370.wav'], 'S1725215358500': ['S172-238-52153-58500-1-510-1540.wav', 'S172-238-52153-58500-2-4160-4890.wav', 'S172-238-52153-58500-3-5010-6350.wav'], 'S1725850070564': ['S172-238-58500-70564-1-0-2790.wav', 'S172-238-58500-70564-2-3060-4540.wav', 'S172-238-58500-70564-3-5140-7690.wav', 'S172-238-58500-70564-4-8430-11410.wav', 'S172-238-58500-70564-5-11720-12070.wav'], 'S172940011750': ['S172-238-9400-11750-1-430-2350.wav'], 'S1731960223094': ['S173-111-19602-23094-1-470-2880.wav', 'S173-111-19602-23094-2-2930-3500.wav'], 'S1732309427000': ['S173-111-23094-27000-1-140-640.wav', 'S173-111-23094-27000-2-900-2460.wav', 'S173-111-23094-27000-3-2970-3910.wav'], 'S1732700030574': ['S173-111-27000-30574-1-940-3580.wav'], 'S1733057433018': ['S173-111-30574-33018-1-1830-2450.wav'], 'S1733562939369': ['S173-111-35629-39369-1-2110-3740.wav'], 'S1733936942878': ['S173-111-39369-42878-1-1200-3510.wav'], 'S1734287844739': ['S173-111-42878-44739-1-0-1870.wav'], 'S1734473947017': ['S173-111-44739-47017-1-770-2280.wav'], 'S1734701752137': ['S173-111-47017-52137-1-560-2150.wav', 'S173-111-47017-52137-2-2710-5120.wav'], 'S1735368157706': ['S173-111-53681-57706-1-0-640.wav', 'S173-111-53681-57706-2-990-1360.wav', 'S173-111-53681-57706-3-1440-2970.wav', 'S173-111-53681-57706-4-3390-4030.wav'], 'S1735770658803': ['S173-111-57706-58803-1-120-1100.wav'], 'S1736164666684': ['S173-111-61646-66684-1-3840-5040.wav'], 'S1737245173500': ['S173-111-72451-73500-1-140-1050.wav'], 'S1737350074114': ['S173-111-73500-74114-1-60-620.wav'], 'S174103232106130': ['S174-172-103232-106130-1-0-1050.wav', 'S174-172-103232-106130-2-1130-2620.wav'], 'S174111231113053': ['S174-172-111231-113053-1-0-350.wav', 'S174-172-111231-113053-2-450-1800.wav'], 'S1741193519846': ['S174-172-11935-19846-1-690-4580.wav', 'S174-172-11935-19846-2-5120-5830.wav', 'S174-172-11935-19846-3-5930-7920.wav'], 'S1741984624219': ['S174-172-19846-24219-1-640-1570.wav', 'S174-172-19846-24219-2-1810-3110.wav', 'S174-172-19846-24219-3-3240-4230.wav'], 'S1742421933196': ['S174-172-24219-33196-1-1000-3800.wav', 'S174-172-24219-33196-2-4840-5370.wav', 'S174-172-24219-33196-3-5890-6230.wav', 'S174-172-24219-33196-4-6250-8160.wav', 'S174-172-24219-33196-5-8320-8980.wav'], 'S1743319642631': ['S174-172-33196-42631-1-20-1200.wav', 'S174-172-33196-42631-2-1400-3410.wav', 'S174-172-33196-42631-3-4270-5950.wav', 'S174-172-33196-42631-4-6860-7730.wav', 'S174-172-33196-42631-5-8330-9380.wav'], 'S1745039765944': ['S174-172-50397-65944-1-230-540.wav', 'S174-172-50397-65944-2-1160-2200.wav', 'S174-172-50397-65944-3-2500-3440.wav', 'S174-172-50397-65944-4-4480-5230.wav', 'S174-172-50397-65944-5-6400-9590.wav', 'S174-172-50397-65944-6-11320-11860.wav', 'S174-172-50397-65944-7-12280-12620.wav', 'S174-172-50397-65944-8-12980-13300.wav', 'S174-172-50397-65944-9-13580-15390.wav'], 'S1746594471860': ['S174-172-65944-71860-1-660-2420.wav', 'S174-172-65944-71860-2-2990-3810.wav', 'S174-172-65944-71860-3-3960-5840.wav'], 'S1747186081648': ['S174-172-71860-81648-1-3330-4740.wav', 'S174-172-71860-81648-2-4970-6160.wav', 'S174-172-71860-81648-3-6320-8680.wav', 'S174-172-71860-81648-4-8720-9050.wav', 'S174-172-71860-81648-5-9080-9790.wav'], 'S1748164886454': ['S174-172-81648-86454-1-0-320.wav', 'S174-172-81648-86454-2-720-4810.wav'], 'S1748645492897': ['S174-172-86454-92897-1-2610-3350.wav', 'S174-172-86454-92897-2-3360-3680.wav', 'S174-172-86454-92897-3-3880-5220.wav', 'S174-172-86454-92897-4-5880-6450.wav'], 'S17492897103232': ['S174-172-92897-103232-1-5070-6370.wav', 'S174-172-92897-103232-2-6380-7240.wav', 'S174-172-92897-103232-3-7990-8810.wav', 'S174-172-92897-103232-4-8840-10340.wav'], 'S17502522': ['S175-167-0-2522-1-150-2300.wav'], 'S175100518107326': ['S175-167-100518-107326-1-60-1180.wav', 'S175-167-100518-107326-2-2120-4000.wav', 'S175-167-100518-107326-3-4870-6310.wav'], 'S175107326114116': ['S175-167-107326-114116-1-1850-3420.wav', 'S175-167-107326-114116-2-4070-6600.wav'], 'S175114116119936': ['S175-167-114116-119936-1-4460-5740.wav'], 'S1751407318724': ['S175-167-14073-18724-1-1240-2150.wav', 'S175-167-14073-18724-2-2610-4350.wav'], 'S1751872422233': ['S175-167-18724-22233-1-1100-3460.wav'], 'S1752223325473': ['S175-167-22233-25473-1-1350-1790.wav', 'S175-167-22233-25473-2-1800-3240.wav'], 'S17525226133': ['S175-167-2522-6133-1-1480-2770.wav', 'S175-167-2522-6133-2-3050-3420.wav'], 'S1752547329199': ['S175-167-25473-29199-1-0-530.wav', 'S175-167-25473-29199-2-950-2130.wav', 'S175-167-25473-29199-3-2650-3730.wav'], 'S1752919931162': ['S175-167-29199-31162-1-380-1890.wav'], 'S1753116232375': ['S175-167-31162-32375-1-920-1220.wav'], 'S1753237534023': ['S175-167-32375-34023-1-0-1600.wav'], 'S1753402343476': ['S175-167-34023-43476-1-4500-6850.wav', 'S175-167-34023-43476-2-7080-9460.wav'], 'S1754538246099': ['S175-167-45382-46099-1-60-720.wav'], 'S1754756252666': ['S175-167-47562-52666-1-720-1070.wav', 'S175-167-47562-52666-2-1570-2230.wav', 'S175-167-47562-52666-3-2850-3250.wav', 'S175-167-47562-52666-4-4190-4980.wav'], 'S1755266658458': ['S175-167-52666-58458-1-2830-5460.wav'], 'S1755845874691': ['S175-167-58458-74691-1-10050-11030.wav', 'S175-167-58458-74691-2-11960-13220.wav'], 'S175613314073': ['S175-167-6133-14073-1-580-2750.wav', 'S175-167-6133-14073-2-2770-3880.wav', 'S175-167-6133-14073-3-4470-5070.wav', 'S175-167-6133-14073-4-5260-7750.wav'], 'S1757469177289': ['S175-167-74691-77289-1-670-2500.wav'], 'S1757728980294': ['S175-167-77289-80294-1-900-2620.wav'], 'S1758029482393': ['S175-167-80294-82393-1-670-2100.wav'], 'S1758239392928': ['S175-167-82393-92928-1-7430-9500.wav', 'S175-167-82393-92928-2-9780-10120.wav'], 'S17592928100518': ['S175-167-92928-100518-1-1310-2090.wav', 'S175-167-92928-100518-2-3360-4450.wav'], 'S1761387815200': ['S176-186-13878-15200-1-0-1330.wav'], 'S1761520020234': ['S176-186-15200-20234-1-1010-1670.wav', 'S176-186-15200-20234-2-2000-2960.wav', 'S176-186-15200-20234-3-3430-4600.wav'], 'S17619966385': ['S176-186-1996-6385-1-2050-3030.wav'], 'S1762023431238': ['S176-186-20234-31238-1-1310-2240.wav', 'S176-186-20234-31238-2-2460-2940.wav', 'S176-186-20234-31238-3-3580-4240.wav', 'S176-186-20234-31238-4-4880-5260.wav', 'S176-186-20234-31238-5-5300-6070.wav', 'S176-186-20234-31238-6-6390-7080.wav', 'S176-186-20234-31238-7-7990-9250.wav', 'S176-186-20234-31238-8-9700-10760.wav'], 'S1763403839622': ['S176-186-34038-39622-1-980-1960.wav', 'S176-186-34038-39622-2-2330-4059.wav', 'S176-186-34038-39622-3-4400-5550.wav'], 'S1763962244298': ['S176-186-39622-44298-1-150-550.wav', 'S176-186-39622-44298-2-1210-3410.wav', 'S176-186-39622-44298-3-3570-4530.wav'], 'S1764407644990': ['S176-186-44076-44990-1-0-920.wav'], 'S1764499048511': ['S176-186-44990-48511-1-980-1480.wav', 'S176-186-44990-48511-2-1790-3530.wav'], 'S1764851151603': ['S176-186-48511-51603-1-1300-2770.wav'], 'S1765160360915': ['S176-186-51603-60915-1-7370-7880.wav', 'S176-186-51603-60915-2-8340-9320.wav'], 'S1766091568136': ['S176-186-60915-68136-1-30-990.wav', 'S176-186-60915-68136-2-1200-2050.wav', 'S176-186-60915-68136-3-2490-3310.wav', 'S176-186-60915-68136-4-3390-4019.wav', 'S176-186-60915-68136-5-4900-6360.wav'], 'S176638513878': ['S176-186-6385-13878-1-620-1330.wav', 'S176-186-6385-13878-2-1810-3110.wav', 'S176-186-6385-13878-3-3490-3950.wav', 'S176-186-6385-13878-4-5050-6090.wav', 'S176-186-6385-13878-5-6220-6550.wav', 'S176-186-6385-13878-6-6910-7500.wav'], 'S1766813670260': ['S176-186-68136-70260-1-1200-2130.wav'], 'S1767026080000': ['S176-186-70260-80000-1-0-840.wav', 'S176-186-70260-80000-2-1020-1410.wav'], 'S1768108483544': ['S176-186-81084-83544-1-1800-2460.wav'], 'S1768354484710': ['S176-186-83544-84710-1-0-350.wav', 'S176-186-83544-84710-2-580-1120.wav'], 'S1768471093754': ['S176-186-84710-93754-1-1820-2150.wav', 'S176-186-84710-93754-2-4950-5270.wav', 'S176-186-84710-93754-3-7360-8090.wav', 'S176-186-84710-93754-4-8200-9050.wav'], 'S1769383294689': ['S176-186-93832-94689-1-70-840.wav'], 'S177103602104700': ['S177-37-103602-104700-1-370-820.wav'], 'S1771566120964': ['S177-37-15661-20964-1-2330-3580.wav', 'S177-37-15661-20964-2-4170-4690.wav'], 'S1772096423200': ['S177-37-20964-23200-1-140-470.wav', 'S177-37-20964-23200-2-1110-2240.wav'], 'S1772320025011': ['S177-37-23200-25011-1-0-1560.wav'], 'S17723775120': ['S177-37-2377-5120-1-870-1260.wav', 'S177-37-2377-5120-2-1630-2670.wav'], 'S1772501130681': ['S177-37-25011-30681-1-670-1400.wav', 'S177-37-25011-30681-2-1550-3380.wav', 'S177-37-25011-30681-3-3470-4010.wav', 'S177-37-25011-30681-4-4240-4580.wav', 'S177-37-25011-30681-5-5100-5470.wav'], 'S1773068135518': ['S177-37-30681-35518-1-2510-4840.wav'], 'S1773551837633': ['S177-37-35518-37633-1-500-1540.wav'], 'S1773763341944': ['S177-37-37633-41944-1-1430-2420.wav', 'S177-37-37633-41944-2-2490-2880.wav', 'S177-37-37633-41944-3-3220-3710.wav'], 'S1774194449819': ['S177-37-41944-49819-1-770-1650.wav', 'S177-37-41944-49819-2-2980-3660.wav', 'S177-37-41944-49819-3-4310-4690.wav', 'S177-37-41944-49819-4-5000-7290.wav'], 'S1775100255574': ['S177-37-51002-55574-1-0-730.wav', 'S177-37-51002-55574-2-2060-2550.wav', 'S177-37-51002-55574-3-3810-4160.wav'], 'S1775557458583': ['S177-37-55574-58583-1-1320-2680.wav'], 'S1775858365166': ['S177-37-58583-65166-1-1540-2900.wav', 'S177-37-58583-65166-2-2960-3350.wav', 'S177-37-58583-65166-3-3470-4340.wav', 'S177-37-58583-65166-4-4480-5410.wav', 'S177-37-58583-65166-5-5740-6560.wav'], 'S1776516673941': ['S177-37-65166-73941-1-1070-2150.wav', 'S177-37-65166-73941-2-3700-4350.wav', 'S177-37-65166-73941-3-6450-6770.wav', 'S177-37-65166-73941-4-7890-8290.wav'], 'S1778591189521': ['S177-37-85911-89521-1-190-670.wav', 'S177-37-85911-89521-2-1060-1510.wav', 'S177-37-85911-89521-3-1520-2300.wav', 'S177-37-85911-89521-4-2320-2810.wav'], 'S1778952197750': ['S177-37-89521-97750-1-2340-2720.wav', 'S177-37-89521-97750-2-4530-4860.wav', 'S177-37-89521-97750-3-5140-5620.wav', 'S177-37-89521-97750-4-5850-6250.wav', 'S177-37-89521-97750-5-6990-7520.wav', 'S177-37-89521-97750-6-7550-7890.wav'], 'S17807565': ['S178-171-0-7565-1-2000-4070.wav', 'S178-171-0-7565-2-5200-7310.wav'], 'S1781789921481': ['S178-171-17899-21481-1-1380-3590.wav'], 'S1782148126023': ['S178-171-21481-26023-1-290-1690.wav', 'S178-171-21481-26023-2-1830-4360.wav'], 'S1782602331719': ['S178-171-26023-31719-1-740-2170.wav', 'S178-171-26023-31719-2-4300-5570.wav'], 'S1783171941122': ['S178-171-31719-41122-1-5370-7780.wav', 'S178-171-31719-41122-2-7990-8320.wav'], 'S1784112244622': ['S178-171-41122-44622-1-1790-3240.wav'], 'S1784462254942': ['S178-171-44622-54942-1-8090-10060.wav'], 'S1785494265942': ['S178-171-54942-65942-1-6100-6450.wav', 'S178-171-54942-65942-2-8770-10790.wav'], 'S1786594273340': ['S178-171-65942-73340-1-6890-7400.wav'], 'S1787342374521': ['S178-171-73423-74521-1-160-1100.wav'], 'S178756510429': ['S178-171-7565-10429-1-940-2870.wav'], 'S17904016': ['S179-93-0-4016-1-1020-2810.wav', 'S179-93-0-4016-2-3420-4019.wav'], 'S1791349618818': ['S179-93-13496-18818-1-50-360.wav', 'S179-93-13496-18818-2-1960-2270.wav', 'S179-93-13496-18818-3-4150-4690.wav'], 'S1791881824795': ['S179-93-18818-24795-1-4540-5190.wav'], 'S1792479536111': ['S179-93-24795-36111-1-1380-1700.wav', 'S179-93-24795-36111-2-3260-3600.wav', 'S179-93-24795-36111-3-6780-7120.wav', 'S179-93-24795-36111-4-10830-11200.wav'], 'S1793611138102': ['S179-93-36111-38102-1-70-630.wav', 'S179-93-36111-38102-2-750-1460.wav'], 'S17940168825': ['S179-93-4016-8825-1-0-310.wav', 'S179-93-4016-8825-2-1080-1460.wav', 'S179-93-4016-8825-3-2900-3830.wav', 'S179-93-4016-8825-4-3880-4770.wav'], 'S1794677052935': ['S179-93-46770-52935-1-1040-1700.wav', 'S179-93-46770-52935-2-2360-3590.wav', 'S179-93-46770-52935-3-3680-4019.wav', 'S179-93-46770-52935-4-4120-4670.wav', 'S179-93-46770-52935-5-5830-6170.wav'], 'S1795293558299': ['S179-93-52935-58299-1-1650-2020.wav', 'S179-93-52935-58299-2-4260-5170.wav'], 'S179882513496': ['S179-93-8825-13496-1-3190-3780.wav', 'S179-93-8825-13496-2-4340-4680.wav'], 'S1801572122041': ['S180-59-15721-22041-1-570-890.wav', 'S180-59-15721-22041-2-1990-3520.wav', 'S180-59-15721-22041-3-4600-5540.wav'], 'S1802204131562': ['S180-59-22041-31562-1-1140-1910.wav', 'S180-59-22041-31562-2-3710-6560.wav', 'S180-59-22041-31562-3-7470-8960.wav'], 'S180272515721': ['S180-59-2725-15721-1-450-760.wav', 'S180-59-2725-15721-2-2360-3910.wav', 'S180-59-2725-15721-3-4970-7140.wav', 'S180-59-2725-15721-4-7150-7990.wav', 'S180-59-2725-15721-5-8580-9620.wav', 'S180-59-2725-15721-6-10170-11410.wav', 'S180-59-2725-15721-7-11750-12590.wav'], 'S1803156239768': ['S180-59-31562-39768-1-3770-5590.wav', 'S180-59-31562-39768-2-5750-6930.wav'], 'S1803976853416': ['S180-59-39768-53416-1-11370-12720.wav'], 'S1805682366818': ['S180-59-56823-66818-1-170-710.wav', 'S180-59-56823-66818-2-1080-2390.wav', 'S180-59-56823-66818-3-3630-5050.wav', 'S180-59-56823-66818-4-5460-5840.wav', 'S180-59-56823-66818-5-7820-8360.wav', 'S180-59-56823-66818-6-8370-8840.wav'], 'S1811117517355': ['S181-166-11175-17355-1-1570-2340.wav', 'S181-166-11175-17355-2-2450-3530.wav', 'S181-166-11175-17355-3-3620-6180.wav'], 'S1811735521000': ['S181-166-17355-21000-1-1980-3650.wav'], 'S18120254385': ['S181-166-2025-4385-1-1110-2360.wav'], 'S1812100023454': ['S181-166-21000-23454-1-240-970.wav', 'S181-166-21000-23454-2-1750-2460.wav'], 'S1812345427753': ['S181-166-23454-27753-1-3710-4150.wav'], 'S1812775329445': ['S181-166-27753-29445-1-900-1700.wav'], 'S1812944549980': ['S181-166-29445-49980-1-810-1570.wav', 'S181-166-29445-49980-2-13240-15940.wav', 'S181-166-29445-49980-3-16490-17880.wav', 'S181-166-29445-49980-4-17990-18820.wav', 'S181-166-29445-49980-5-19390-19720.wav', 'S181-166-29445-49980-6-19750-20440.wav'], 'S18143858555': ['S181-166-4385-8555-1-340-4170.wav'], 'S1814999052341': ['S181-166-49990-52341-1-580-1530.wav', 'S181-166-49990-52341-2-1560-2130.wav'], 'S1815234155795': ['S181-166-52341-55795-1-830-1860.wav', 'S181-166-52341-55795-2-1870-2610.wav'], 'S1815579565382': ['S181-166-55795-65382-1-1170-1480.wav', 'S181-166-55795-65382-2-5040-6100.wav', 'S181-166-55795-65382-3-6210-6720.wav', 'S181-166-55795-65382-4-6800-7900.wav', 'S181-166-55795-65382-5-8210-9530.wav'], 'S1816538267952': ['S181-166-65382-67952-1-1310-2570.wav'], 'S1816795270953': ['S181-166-67952-70953-1-1210-2730.wav'], 'S1817095377338': ['S181-166-70953-77338-1-320-1150.wav', 'S181-166-70953-77338-2-2950-3870.wav', 'S181-166-70953-77338-3-4600-5780.wav', 'S181-166-70953-77338-4-5850-6260.wav'], 'S1817840993989': ['S181-166-78409-93989-1-13830-14340.wav', 'S181-166-78409-93989-2-14420-15580.wav'], 'S181855511175': ['S181-166-8555-11175-1-660-2620.wav'], 'S1819358994829': ['S181-166-93589-94829-1-0-1240.wav'], 'S1819445095792': ['S181-166-94450-95792-1-0-650.wav', 'S181-166-94450-95792-2-920-1350.wav'], 'S1819509296100': ['S181-166-95092-96100-1-230-990.wav'], 'S1819610597071': ['S181-166-96105-97071-1-230-920.wav'], 'S1819707197521': ['S181-166-97071-97521-1-100-450.wav'], 'S1819752198237': ['S181-166-97521-98237-1-260-720.wav'], 'S182105407106783': ['S182-158-105407-106783-1-0-460.wav', 'S182-158-105407-106783-2-780-1120.wav'], 'S182108415121112': ['S182-158-108415-121112-1-1420-1730.wav', 'S182-158-108415-121112-2-8189-8630.wav', 'S182-158-108415-121112-3-8800-9330.wav', 'S182-158-108415-121112-4-9460-9900.wav', 'S182-158-108415-121112-5-10710-11090.wav', 'S182-158-108415-121112-6-11850-12440.wav'], 'S182121987127871': ['S182-158-121987-127871-1-3370-3770.wav', 'S182-158-121987-127871-2-5010-5750.wav'], 'S182128806141346': ['S182-158-128806-141346-1-4490-5130.wav', 'S182-158-128806-141346-2-5660-5990.wav', 'S182-158-128806-141346-3-6710-7140.wav', 'S182-158-128806-141346-4-7680-8220.wav', 'S182-158-128806-141346-5-11170-11510.wav', 'S182-158-128806-141346-6-11660-12520.wav'], 'S1822899636222': ['S182-158-28996-36222-1-710-1060.wav', 'S182-158-28996-36222-2-3010-3640.wav', 'S182-158-28996-36222-3-4140-4470.wav', 'S182-158-28996-36222-4-4830-5150.wav', 'S182-158-28996-36222-5-5340-5650.wav', 'S182-158-28996-36222-6-6560-7230.wav'], 'S1823678937777': ['S182-158-36789-37777-1-90-990.wav'], 'S1823914142980': ['S182-158-39141-42980-1-10-3840.wav'], 'S1824622660300': ['S182-158-46226-60300-1-500-810.wav', 'S182-158-46226-60300-2-1210-1530.wav', 'S182-158-46226-60300-3-5980-6320.wav', 'S182-158-46226-60300-4-10170-10540.wav', 'S182-158-46226-60300-5-13760-14080.wav'], 'S1826292367361': ['S182-158-62923-67361-1-410-740.wav', 'S182-158-62923-67361-2-1450-1840.wav', 'S182-158-62923-67361-3-2170-2570.wav', 'S182-158-62923-67361-4-2860-3210.wav', 'S182-158-62923-67361-5-3940-4370.wav'], 'S1826736168152': ['S182-158-67361-68152-1-0-800.wav'], 'S1827115877026': ['S182-158-71158-77026-1-2150-2500.wav', 'S182-158-71158-77026-2-3060-3460.wav', 'S182-158-71158-77026-3-5370-5680.wav'], 'S1827764685000': ['S182-158-77646-85000-1-3840-4200.wav', 'S182-158-77646-85000-2-4550-5240.wav', 'S182-158-77646-85000-3-7160-7360.wav'], 'S1828543087249': ['S182-158-85430-87249-1-0-330.wav', 'S182-158-85430-87249-2-730-1160.wav', 'S182-158-85430-87249-3-1180-1820.wav'], 'S182965016996': ['S182-158-9650-16996-1-840-1170.wav', 'S182-158-9650-16996-2-4940-5530.wav', 'S182-158-9650-16996-3-5970-6390.wav', 'S182-158-9650-16996-4-6500-7350.wav'], 'S1831180014400': ['S183-237-11800-14400-1-160-1330.wav', 'S183-237-11800-14400-2-1610-2530.wav'], 'S1831440017676': ['S183-237-14400-17676-1-930-3260.wav'], 'S1831767620600': ['S183-237-17676-20600-1-1410-2930.wav'], 'S1832060023051': ['S183-237-20600-23051-1-0-1630.wav'], 'S1832305130972': ['S183-237-23051-30972-1-5950-7680.wav'], 'S1833097235798': ['S183-237-30972-35798-1-2330-4010.wav'], 'S1833579843813': ['S183-237-35798-43813-1-2480-3560.wav', 'S183-237-35798-43813-2-4890-7240.wav'], 'S1834381349072': ['S183-237-43813-49072-1-880-1960.wav', 'S183-237-43813-49072-2-2210-2830.wav', 'S183-237-43813-49072-3-3360-4670.wav'], 'S1834907251998': ['S183-237-49072-51998-1-610-2580.wav'], 'S1835199855004': ['S183-237-51998-55004-1-1470-3010.wav'], 'S1835700058250': ['S183-237-57000-58250-1-190-1170.wav'], 'S1835825063001': ['S183-237-58250-63001-1-2710-4680.wav'], 'S1836300165195': ['S183-237-63001-65195-1-300-1960.wav'], 'S1836519568671': ['S183-237-65195-68671-1-1280-3040.wav', 'S183-237-65195-68671-2-3060-3480.wav'], 'S1836867172500': ['S183-237-68671-72500-1-290-1500.wav', 'S183-237-68671-72500-2-1950-3320.wav'], 'S1837250082346': ['S183-237-72500-82346-1-530-1800.wav', 'S183-237-72500-82346-2-2480-3670.wav', 'S183-237-72500-82346-3-3920-6170.wav', 'S183-237-72500-82346-4-6680-7090.wav', 'S183-237-72500-82346-5-7610-8870.wav'], 'S183740911800': ['S183-237-7409-11800-1-410-2140.wav', 'S183-237-7409-11800-2-2420-4400.wav'], 'S1838234691809': ['S183-237-82346-91809-1-710-5710.wav', 'S183-237-82346-91809-2-6330-7940.wav'], 'S1839180995500': ['S183-237-91809-95500-1-2510-3700.wav'], 'S1839560097443': ['S183-237-95600-97443-1-240-1850.wav'], 'S1841246614179': ['S184-169-12466-14179-1-70-1720.wav'], 'S1841417917028': ['S184-169-14179-17028-1-1070-2850.wav'], 'S1841702819844': ['S184-169-17028-19844-1-0-350.wav', 'S184-169-17028-19844-2-520-2740.wav'], 'S1841984425883': ['S184-169-19844-25883-1-1000-3660.wav', 'S184-169-19844-25883-2-4140-5700.wav'], 'S1842588333072': ['S184-169-25883-33072-1-1580-2070.wav', 'S184-169-25883-33072-2-2430-6980.wav'], 'S1843307236442': ['S184-169-33072-36442-1-2140-3370.wav'], 'S1843644239422': ['S184-169-36442-39422-1-0-2980.wav'], 'S1843942243610': ['S184-169-39422-43610-1-120-1130.wav', 'S184-169-39422-43610-2-1320-1710.wav', 'S184-169-39422-43610-3-1880-4100.wav'], 'S1844361045417': ['S184-169-43610-45417-1-480-1810.wav'], 'S1844541748371': ['S184-169-45417-48371-1-1250-2960.wav'], 'S184554212466': ['S184-169-5542-12466-1-4590-6930.wav'], 'S1845795863935': ['S184-169-57958-63935-1-1190-2940.wav', 'S184-169-57958-63935-2-4710-5980.wav'], 'S1846393567283': ['S184-169-63935-67283-1-1210-2890.wav'], 'S1846728373448': ['S184-169-67283-73448-1-360-6130.wav'], 'S1847344877077': ['S184-169-73448-77077-1-900-2180.wav', 'S184-169-73448-77077-2-2290-3630.wav'], 'S1847707779283': ['S184-169-77077-79283-1-240-2020.wav'], 'S1847928382832': ['S184-169-79283-82832-1-440-3390.wav'], 'S1848283290006': ['S184-169-82832-90006-1-660-6110.wav', 'S184-169-82832-90006-2-6280-6910.wav'], 'S1849766199840': ['S184-169-97661-99840-1-1050-2140.wav'], 'S1851193618420': ['S185-39-11936-18420-1-0-6490.wav'], 'S1851842023395': ['S185-39-18420-23395-1-2950-4710.wav'], 'S1852339526755': ['S185-39-23395-26755-1-2110-2740.wav', 'S185-39-23395-26755-2-2750-3150.wav'], 'S1852814029183': ['S185-39-28140-29183-1-230-970.wav'], 'S1852918332857': ['S185-39-29183-32857-1-0-3680.wav'], 'S18530205464': ['S185-39-3020-5464-1-670-1220.wav'], 'S1853528435749': ['S185-39-35284-35749-1-0-470.wav'], 'S1853577440590': ['S185-39-35774-40590-1-3580-3950.wav', 'S185-39-35774-40590-2-4010-4350.wav'], 'S1854059042572': ['S185-39-40590-42572-1-1200-1590.wav'], 'S1854257245595': ['S185-39-42572-45595-1-0-3030.wav'], 'S1854559549345': ['S185-39-45595-49345-1-1470-2020.wav'], 'S1854934550797': ['S185-39-49345-50797-1-0-1460.wav'], 'S1855079754600': ['S185-39-50797-54600-1-0-3810.wav'], 'S18554649590': ['S185-39-5464-9590-1-3020-3550.wav'], 'S185959011936': ['S185-39-9590-11936-1-910-1360.wav'], 'S1861288820765': ['S186-161-12888-20765-1-0-450.wav', 'S186-161-12888-20765-2-540-1890.wav', 'S186-161-12888-20765-3-2040-2970.wav', 'S186-161-12888-20765-4-2980-3870.wav', 'S186-161-12888-20765-5-4040-4730.wav', 'S186-161-12888-20765-6-5180-7740.wav'], 'S18619637183': ['S186-161-1963-7183-1-1070-1710.wav', 'S186-161-1963-7183-2-2320-3530.wav', 'S186-161-1963-7183-3-4080-5220.wav'], 'S1862076529623': ['S186-161-20765-29623-1-610-5160.wav', 'S186-161-20765-29623-2-5460-5790.wav', 'S186-161-20765-29623-3-6470-8080.wav'], 'S1862962338585': ['S186-161-29623-38585-1-380-1050.wav', 'S186-161-29623-38585-2-2940-6260.wav', 'S186-161-29623-38585-3-7110-8600.wav'], 'S1863858542250': ['S186-161-38585-42250-1-2350-3290.wav'], 'S1864225043701': ['S186-161-42250-43701-1-0-1460.wav'], 'S1864370155661': ['S186-161-43701-55661-1-9250-11960.wav'], 'S1865613963954': ['S186-161-56139-63954-1-4890-5990.wav', 'S186-161-56139-63954-2-6110-6550.wav', 'S186-161-56139-63954-3-7010-7350.wav'], 'S1866395465417': ['S186-161-63954-65417-1-1110-1470.wav'], 'S1866541766314': ['S186-161-65417-66314-1-110-900.wav'], 'S186718312888': ['S186-161-7183-12888-1-0-410.wav', 'S186-161-7183-12888-2-1940-4490.wav', 'S186-161-7183-12888-3-4520-5710.wav'], 'S187102334104310': ['S187-151-102334-104310-1-50-1980.wav'], 'S187106210109939': ['S187-151-106210-109939-1-1680-3620.wav'], 'S187125088136477': ['S187-151-125088-136477-1-5000-7710.wav', 'S187-151-125088-136477-2-7940-8250.wav', 'S187-151-125088-136477-3-8260-11200.wav'], 'S1871272417727': ['S187-151-12724-17727-1-1640-2860.wav', 'S187-151-12724-17727-2-3090-5010.wav'], 'S187137308146876': ['S187-151-137308-146876-1-0-3020.wav', 'S187-151-137308-146876-2-4220-5710.wav', 'S187-151-137308-146876-3-7520-9060.wav'], 'S187146876152763': ['S187-151-146876-152763-1-3150-3950.wav', 'S187-151-146876-152763-2-4430-5890.wav'], 'S187152763155685': ['S187-151-152763-155685-1-1540-2670.wav'], 'S187158504161678': ['S187-151-158504-161678-1-1580-3180.wav'], 'S1871772721635': ['S187-151-17727-21635-1-2410-3910.wav'], 'S1872163525805': ['S187-151-21635-25805-1-0-340.wav', 'S187-151-21635-25805-2-2510-4120.wav'], 'S1872580538099': ['S187-151-25805-38099-1-4240-6360.wav', 'S187-151-25805-38099-2-10660-11360.wav'], 'S1873809960731': ['S187-151-38099-60731-1-17220-19240.wav', 'S187-151-38099-60731-2-19550-21100.wav', 'S187-151-38099-60731-3-21240-22190.wav'], 'S187383212724': ['S187-151-3832-12724-1-430-1210.wav', 'S187-151-3832-12724-2-1830-2400.wav'], 'S1876073169806': ['S187-151-60731-69806-1-6910-8430.wav'], 'S18782319101353': ['S187-151-82319-101353-1-4270-5550.wav', 'S187-151-82319-101353-2-6280-7820.wav', 'S187-151-82319-101353-3-8700-10930.wav', 'S187-151-82319-101353-4-11310-14230.wav', 'S187-151-82319-101353-5-16530-18590.wav'], 'S1882490137027': ['S188-290-24901-37027-1-1260-2170.wav', 'S188-290-24901-37027-2-3850-4730.wav', 'S188-290-24901-37027-3-5570-6230.wav', 'S188-290-24901-37027-4-6470-7510.wav', 'S188-290-24901-37027-5-8020-8520.wav', 'S188-290-24901-37027-6-9590-9940.wav', 'S188-290-24901-37027-7-10060-11460.wav'], 'S1883702745491': ['S188-290-37027-45491-1-1290-2490.wav', 'S188-290-37027-45491-2-2860-4440.wav', 'S188-290-37027-45491-3-7020-7530.wav'], 'S1884549158192': ['S188-290-45491-58192-1-880-2300.wav', 'S188-290-45491-58192-2-5300-5900.wav'], 'S188731523900': ['S188-290-7315-23900-1-4300-5000.wav', 'S188-290-7315-23900-2-16010-16590.wav'], 'S1889324094285': ['S188-290-93240-94285-1-520-1050.wav'], 'S18914634190': ['S189-33-1463-4190-1-0-2730.wav'], 'S1891600017824': ['S189-33-16000-17824-1-860-1320.wav'], 'S1891782423860': ['S189-33-17824-23860-1-550-1250.wav', 'S189-33-17824-23860-2-4180-4810.wav', 'S189-33-17824-23860-3-5120-5820.wav'], 'S1892386029895': ['S189-33-23860-29895-1-420-810.wav', 'S189-33-23860-29895-2-1000-1870.wav', 'S189-33-23860-29895-3-2380-4680.wav', 'S189-33-23860-29895-4-4850-5480.wav'], 'S1892989531441': ['S189-33-29895-31441-1-350-1310.wav'], 'S1893144136097': ['S189-33-31441-36097-1-0-4660.wav'], 'S1893609738741': ['S189-33-36097-38741-1-0-2650.wav'], 'S1893874145555': ['S189-33-38741-45555-1-5050-5380.wav', 'S189-33-38741-45555-2-5450-5950.wav'], 'S18941908812': ['S189-33-4190-8812-1-270-630.wav', 'S189-33-4190-8812-2-3730-4080.wav'], 'S1894555548125': ['S189-33-45555-48125-1-650-1060.wav', 'S189-33-45555-48125-2-1770-2190.wav'], 'S1894812559936': ['S189-33-48125-59936-1-180-560.wav', 'S189-33-48125-59936-2-620-1110.wav', 'S189-33-48125-59936-3-1380-1750.wav', 'S189-33-48125-59936-4-4930-5510.wav', 'S189-33-48125-59936-5-5730-6270.wav', 'S189-33-48125-59936-6-6950-7450.wav', 'S189-33-48125-59936-7-10390-11020.wav'], 'S1895993674082': ['S189-33-59936-74082-1-3350-4600.wav', 'S189-33-59936-74082-2-4680-5050.wav', 'S189-33-59936-74082-3-5070-5410.wav', 'S189-33-59936-74082-4-5870-6550.wav', 'S189-33-59936-74082-5-7440-7750.wav', 'S189-33-59936-74082-6-9660-10860.wav', 'S189-33-59936-74082-7-11990-13570.wav'], 'S189881216000': ['S189-33-8812-16000-1-700-1090.wav', 'S189-33-8812-16000-2-1170-1560.wav', 'S189-33-8812-16000-3-1920-2530.wav', 'S189-33-8812-16000-4-2930-3290.wav', 'S189-33-8812-16000-5-3300-4310.wav', 'S189-33-8812-16000-6-4950-5510.wav', 'S189-33-8812-16000-7-6530-7120.wav'], 'S190101302104244': ['S190-124-101302-104244-1-780-2130.wav', 'S190-124-101302-104244-2-2350-2700.wav', 'S190-124-101302-104244-3-2710-2950.wav'], 'S1901024120544': ['S190-124-10241-20544-1-2510-4380.wav', 'S190-124-10241-20544-2-4860-5570.wav', 'S190-124-10241-20544-3-7920-8720.wav'], 'S1902054437402': ['S190-124-20544-37402-1-15540-15950.wav', 'S190-124-20544-37402-2-16110-16860.wav'], 'S1905436458052': ['S190-124-54364-58052-1-1570-3450.wav'], 'S1905805261506': ['S190-124-58052-61506-1-2290-3460.wav'], 'S1906150678361': ['S190-124-61506-78361-1-3960-4690.wav', 'S190-124-61506-78361-2-6600-7160.wav', 'S190-124-61506-78361-3-9180-9840.wav', 'S190-124-61506-78361-4-10070-11730.wav', 'S190-124-61506-78361-5-11840-12370.wav', 'S190-124-61506-78361-6-15810-16800.wav'], 'S1907836190268': ['S190-124-78361-90268-1-2250-2700.wav', 'S190-124-78361-90268-2-4110-5570.wav', 'S190-124-78361-90268-3-5810-6310.wav', 'S190-124-78361-90268-4-6850-7160.wav', 'S190-124-78361-90268-5-7220-7530.wav', 'S190-124-78361-90268-6-8029-8970.wav', 'S190-124-78361-90268-7-9410-9770.wav', 'S190-124-78361-90268-8-10810-11530.wav'], 'S1909026898888': ['S190-124-90268-98888-1-3780-4840.wav'], 'S19098888101302': ['S190-124-98888-101302-1-860-2420.wav'], 'S19117132926': ['S191-222-1713-2926-1-500-1140.wav'], 'S1912305126521': ['S191-222-23051-26521-1-1170-1530.wav', 'S191-222-23051-26521-2-1660-3470.wav'], 'S1912652144108': ['S191-222-26521-44108-1-4160-5370.wav', 'S191-222-26521-44108-2-8400-10370.wav', 'S191-222-26521-44108-3-11870-12730.wav', 'S191-222-26521-44108-4-15790-16790.wav', 'S191-222-26521-44108-5-17090-17590.wav'], 'S191292623051': ['S191-222-2926-23051-1-110-420.wav', 'S191-222-2926-23051-2-2520-3970.wav', 'S191-222-2926-23051-3-4700-5110.wav', 'S191-222-2926-23051-4-9770-10090.wav', 'S191-222-2926-23051-5-11110-11440.wav', 'S191-222-2926-23051-6-12890-13540.wav', 'S191-222-2926-23051-7-13990-16149.wav', 'S191-222-2926-23051-8-17110-18680.wav', 'S191-222-2926-23051-9-18820-19800.wav'], 'S1914410857183': ['S191-222-44108-57183-1-0-370.wav', 'S191-222-44108-57183-2-530-2540.wav', 'S191-222-44108-57183-3-4290-5300.wav', 'S191-222-44108-57183-4-5690-6470.wav', 'S191-222-44108-57183-5-6940-7390.wav'], 'S1916877770581': ['S191-222-68777-70581-1-1370-1800.wav'], 'S1917272984730': ['S191-222-72729-84730-1-2290-3440.wav', 'S191-222-72729-84730-2-4840-5250.wav', 'S191-222-72729-84730-3-5260-7920.wav', 'S191-222-72729-84730-4-9000-11860.wav'], 'S19184730103919': ['S191-222-84730-103919-1-1280-1920.wav', 'S191-222-84730-103919-2-1950-2460.wav', 'S191-222-84730-103919-3-3730-4070.wav', 'S191-222-84730-103919-4-9630-10440.wav', 'S191-222-84730-103919-5-12480-12860.wav', 'S191-222-84730-103919-6-13160-18960.wav'], 'S192106220109646': ['S192-146-106220-109646-1-1130-3160.wav'], 'S192109646112040': ['S192-146-109646-112040-1-1030-2400.wav'], 'S192112744116014': ['S192-146-112744-116014-1-110-1510.wav', 'S192-146-112744-116014-2-1870-3270.wav'], 'S192117120118883': ['S192-146-117120-118883-1-10-340.wav', 'S192-146-117120-118883-2-680-1770.wav'], 'S192120090122241': ['S192-146-120090-122241-1-290-2160.wav'], 'S192125526126486': ['S192-146-125526-126486-1-110-960.wav'], 'S1921637429399': ['S192-146-16374-29399-1-4720-5900.wav', 'S192-146-16374-29399-2-7860-9600.wav', 'S192-146-16374-29399-3-9720-10320.wav', 'S192-146-16374-29399-4-10390-12750.wav'], 'S1922939947548': ['S192-146-29399-47548-1-11240-14280.wav', 'S192-146-29399-47548-2-14630-18120.wav'], 'S19229584600': ['S192-146-2958-4600-1-540-1650.wav'], 'S1924754850909': ['S192-146-47548-50909-1-1310-3220.wav'], 'S192516716374': ['S192-146-5167-16374-1-20-430.wav', 'S192-146-5167-16374-2-660-970.wav', 'S192-146-5167-16374-3-5200-8080.wav', 'S192-146-5167-16374-4-9890-11110.wav'], 'S1926757774035': ['S192-146-67577-74035-1-1790-2560.wav', 'S192-146-67577-74035-2-4360-6150.wav'], 'S1927403588386': ['S192-146-74035-88386-1-4000-5510.wav', 'S192-146-74035-88386-2-6580-7670.wav', 'S192-146-74035-88386-3-8480-9460.wav', 'S192-146-74035-88386-4-10230-10630.wav', 'S192-146-74035-88386-5-11290-12500.wav', 'S192-146-74035-88386-6-13340-13950.wav'], 'S1928838694441': ['S192-146-88386-94441-1-870-1200.wav', 'S192-146-88386-94441-2-3540-5510.wav'], 'S19294441106220': ['S192-146-94441-106220-1-160-480.wav', 'S192-146-94441-106220-2-6530-6860.wav', 'S192-146-94441-106220-3-8500-11750.wav'], 'S1931102217522': ['S193-136-11022-17522-1-660-6500.wav'], 'S1931752219081': ['S193-136-17522-19081-1-0-1560.wav'], 'S1931908122076': ['S193-136-19081-22076-1-710-3000.wav'], 'S1932207630209': ['S193-136-22076-30209-1-0-1340.wav', 'S193-136-22076-30209-2-1970-2280.wav', 'S193-136-22076-30209-3-4660-5620.wav', 'S193-136-22076-30209-4-5890-7990.wav'], 'S193250711022': ['S193-136-2507-11022-1-710-2850.wav', 'S193-136-2507-11022-2-3130-8520.wav'], 'S1933020935712': ['S193-136-30209-35712-1-1440-5510.wav'], 'S1933571240000': ['S193-136-35712-40000-1-0-1520.wav', 'S193-136-35712-40000-2-1850-3880.wav'], 'S1934000042560': ['S193-136-40000-42560-1-240-2560.wav'], 'S1934638548002': ['S193-136-46385-48002-1-100-1620.wav'], 'S1934800249586': ['S193-136-48002-49586-1-250-1460.wav'], 'S1934958651496': ['S193-136-49586-51496-1-440-1910.wav'], 'S1935149654863': ['S193-136-51496-54863-1-180-1980.wav', 'S193-136-51496-54863-2-2210-3370.wav'], 'S1935486357229': ['S193-136-54863-57229-1-550-2370.wav'], 'S1935722958410': ['S193-136-57229-58410-1-600-1190.wav'], 'S1935841062146': ['S193-136-58410-62146-1-800-1130.wav', 'S193-136-58410-62146-2-1500-1820.wav', 'S193-136-58410-62146-3-1860-3740.wav'], 'S1936214667673': ['S193-136-62146-67673-1-2950-5230.wav'], 'S1941047812935': ['S194-140-10478-12935-1-170-2460.wav'], 'S1941293516209': ['S194-140-12935-16209-1-1110-3100.wav'], 'S1941620925827': ['S194-140-16209-25827-1-2070-3590.wav', 'S194-140-16209-25827-2-4050-8610.wav', 'S194-140-16209-25827-3-8810-9370.wav'], 'S1942582727890': ['S194-140-25827-27890-1-890-2070.wav'], 'S1942789030057': ['S194-140-27890-30057-1-190-570.wav', 'S194-140-27890-30057-2-630-2170.wav'], 'S1943214234250': ['S194-140-32142-34250-1-300-2110.wav'], 'S19447019731': ['S194-140-4701-9731-1-390-1050.wav', 'S194-140-4701-9731-2-1190-2170.wav', 'S194-140-4701-9731-3-2190-5030.wav'], 'S1951410316745': ['S195-241-14103-16745-1-410-1280.wav', 'S195-241-14103-16745-2-1450-1880.wav', 'S195-241-14103-16745-3-1940-2650.wav'], 'S1951674518042': ['S195-241-16745-18042-1-20-1300.wav'], 'S1951804221965': ['S195-241-18042-21965-1-530-3930.wav'], 'S1952196524626': ['S195-241-21965-24626-1-0-650.wav', 'S195-241-21965-24626-2-730-1210.wav', 'S195-241-21965-24626-3-1390-2670.wav'], 'S1952462627600': ['S195-241-24626-27600-1-0-920.wav', 'S195-241-24626-27600-2-1080-1700.wav', 'S195-241-24626-27600-3-2120-2980.wav'], 'S1952760032389': ['S195-241-27600-32389-1-10-2160.wav', 'S195-241-27600-32389-2-2430-3290.wav', 'S195-241-27600-32389-3-3480-4790.wav'], 'S1953897239770': ['S195-241-38972-39770-1-310-800.wav'], 'S19571708633': ['S195-241-7170-8633-1-480-1390.wav'], 'S195863314103': ['S195-241-8633-14103-1-530-1860.wav', 'S195-241-8633-14103-2-1920-5470.wav'], 'S1961530017816': ['S196-222-15300-17816-1-0-2520.wav'], 'S1961781621099': ['S196-222-17816-21099-1-0-3250.wav'], 'S1962110731222': ['S196-222-21107-31222-1-560-5100.wav', 'S196-222-21107-31222-2-5530-7980.wav', 'S196-222-21107-31222-3-8350-10090.wav'], 'S1963122238555': ['S196-222-31222-38555-1-5090-7340.wav'], 'S1963855538912': ['S196-222-38555-38912-1-0-360.wav'], 'S19639698141': ['S196-222-3969-8141-1-2930-4180.wav'], 'S196814115300': ['S196-222-8141-15300-1-0-7160.wav'], 'S1971335519377': ['S197-122-13355-19377-1-5240-5990.wav'], 'S1972091824639': ['S197-122-20918-24639-1-910-1900.wav', 'S197-122-20918-24639-2-2600-3730.wav'], 'S1972463928855': ['S197-122-24639-28855-1-1210-4220.wav'], 'S1972885537508': ['S197-122-28855-37508-1-2050-4580.wav', 'S197-122-28855-37508-2-4690-5940.wav', 'S197-122-28855-37508-3-6010-8260.wav', 'S197-122-28855-37508-4-8320-8660.wav'], 'S1973750842022': ['S197-122-37508-42022-1-1360-4520.wav'], 'S1975126952016': ['S197-122-51269-52016-1-260-750.wav'], 'S197895013355': ['S197-122-8950-13355-1-3240-4410.wav'], 'S198011078': ['S198-221-0-11078-1-1070-1590.wav', 'S198-221-0-11078-2-2460-2770.wav', 'S198-221-0-11078-3-4380-4690.wav', 'S198-221-0-11078-4-5200-6240.wav', 'S198-221-0-11078-5-6340-6820.wav', 'S198-221-0-11078-6-6970-8260.wav', 'S198-221-0-11078-7-9350-9950.wav', 'S198-221-0-11078-8-10810-11080.wav'], 'S1981107813538': ['S198-221-11078-13538-1-0-2460.wav'], 'S1981353817378': ['S198-221-13538-17378-1-30-540.wav', 'S198-221-13538-17378-2-590-1300.wav', 'S198-221-13538-17378-3-1390-2120.wav', 'S198-221-13538-17378-4-2460-3840.wav'], 'S1981737819655': ['S198-221-17378-19655-1-20-2280.wav'], 'S1981965521767': ['S198-221-19655-21767-1-380-840.wav', 'S198-221-19655-21767-2-890-2120.wav'], 'S1982176725126': ['S198-221-21767-25126-1-40-3290.wav'], 'S1982512628335': ['S198-221-25126-28335-1-0-410.wav', 'S198-221-25126-28335-2-490-3210.wav'], 'S1982833532173': ['S198-221-28335-32173-1-210-730.wav', 'S198-221-28335-32173-2-790-3840.wav'], 'S1983206337085': ['S198-221-32063-37085-1-0-310.wav', 'S198-221-32063-37085-2-840-2800.wav', 'S198-221-32063-37085-3-3090-5030.wav'], 'S1983708541838': ['S198-221-37085-41838-1-150-460.wav', 'S198-221-37085-41838-2-570-1020.wav', 'S198-221-37085-41838-3-1860-2170.wav', 'S198-221-37085-41838-4-2460-2780.wav', 'S198-221-37085-41838-5-2840-4760.wav'], 'S1984860050513': ['S198-221-48600-50513-1-170-1920.wav'], 'S1991290418004': ['S199-92-12904-18004-1-550-4760.wav'], 'S1991800423098': ['S199-92-18004-23098-1-90-790.wav', 'S199-92-18004-23098-2-1140-5100.wav'], 'S199263212438': ['S199-92-2632-12438-1-3090-5640.wav', 'S199-92-2632-12438-2-6720-9810.wav'], 'S1993305736857': ['S199-92-33057-36857-1-750-1220.wav', 'S199-92-33057-36857-2-1380-3780.wav'], 'S1993685743578': ['S199-92-36857-43578-1-1270-1600.wav', 'S199-92-36857-43578-2-2250-3340.wav', 'S199-92-36857-43578-3-4520-6010.wav', 'S199-92-36857-43578-4-6060-6710.wav'], 'S1994357846555': ['S199-92-43578-46555-1-1740-2980.wav'], 'S1994655548621': ['S199-92-46555-48621-1-0-2000.wav'], 'S1994862152219': ['S199-92-48621-52219-1-1190-3600.wav'], 'S1995221955830': ['S199-92-52219-55830-1-1640-3620.wav'], 'S2001309117264': ['S200-121-13091-17264-1-760-1640.wav', 'S200-121-13091-17264-2-1720-4180.wav'], 'S2001726421500': ['S200-121-17264-21500-1-860-1780.wav', 'S200-121-17264-21500-2-2400-4240.wav'], 'S2002150023579': ['S200-121-21500-23579-1-0-2080.wav'], 'S20023359300': ['S200-121-2335-9300-1-6070-6970.wav'], 'S200930013091': ['S200-121-9300-13091-1-0-3640.wav'], 'S2011318816927': ['S201-87-13188-16927-1-0-1590.wav', 'S201-87-13188-16927-2-1640-2320.wav', 'S201-87-13188-16927-3-2990-3740.wav'], 'S2011692718586': ['S201-87-16927-18586-1-10-1470.wav'], 'S2011858622491': ['S201-87-18586-22491-1-930-2680.wav', 'S201-87-18586-22491-2-2710-3170.wav'], 'S2013087838412': ['S201-87-30878-38412-1-410-820.wav', 'S201-87-30878-38412-2-2330-2950.wav', 'S201-87-30878-38412-3-4000-4310.wav', 'S201-87-30878-38412-4-4730-6920.wav'], 'S20138609530': ['S201-87-3860-9530-1-690-1540.wav', 'S201-87-3860-9530-2-1590-5670.wav'], 'S2014860553616': ['S201-87-48605-53616-1-720-1650.wav', 'S201-87-48605-53616-2-2300-2630.wav', 'S201-87-48605-53616-3-2880-3370.wav', 'S201-87-48605-53616-4-3590-4230.wav', 'S201-87-48605-53616-5-4350-5020.wav'], 'S201953013188': ['S201-87-9530-13188-1-880-1210.wav', 'S201-87-9530-13188-2-1510-3060.wav', 'S201-87-9530-13188-3-3380-3660.wav'], 'S2022288823504': ['S202-187-22888-23504-1-130-620.wav'], 'S2022350423887': ['S202-187-23504-23887-1-0-390.wav'], 'S2022429025900': ['S202-187-24290-25900-1-0-1610.wav'], 'S2022657033450': ['S202-187-26570-33450-1-20-1690.wav', 'S202-187-26570-33450-2-2270-3940.wav', 'S202-187-26570-33450-3-4300-6880.wav'], 'S2023345037691': ['S202-187-33450-37691-1-0-840.wav', 'S202-187-33450-37691-2-1180-4250.wav'], 'S2023769143450': ['S202-187-37691-43450-1-950-4680.wav', 'S202-187-37691-43450-2-5560-5760.wav'], 'S2024345047307': ['S202-187-43450-47307-1-170-1470.wav', 'S202-187-43450-47307-2-2050-3860.wav'], 'S2024730753525': ['S202-187-47307-53525-1-1060-1630.wav', 'S202-187-47307-53525-2-1840-4660.wav', 'S202-187-47307-53525-3-4960-6220.wav'], 'S2025352559211': ['S202-187-53525-59211-1-0-390.wav', 'S202-187-53525-59211-2-400-1600.wav', 'S202-187-53525-59211-3-2880-4040.wav', 'S202-187-53525-59211-4-4400-5690.wav'], 'S2025921161954': ['S202-187-59211-61954-1-690-2750.wav'], 'S2026195468703': ['S202-187-61954-68703-1-1470-2200.wav', 'S202-187-61954-68703-2-2770-3810.wav', 'S202-187-61954-68703-3-3870-5690.wav'], 'S202635220745': ['S202-187-6352-20745-1-3490-3890.wav', 'S202-187-6352-20745-2-4950-7730.wav', 'S202-187-6352-20745-3-7980-8860.wav', 'S202-187-6352-20745-4-9110-10660.wav', 'S202-187-6352-20745-5-10740-11110.wav', 'S202-187-6352-20745-6-11970-13050.wav', 'S202-187-6352-20745-7-13140-13630.wav'], 'S2026980278329': ['S202-187-69802-78329-1-550-1050.wav', 'S202-187-69802-78329-2-3020-7160.wav', 'S202-187-69802-78329-3-7380-8530.wav'], 'S2031288626519': ['S203-101-12886-26519-1-5220-6000.wav', 'S203-101-12886-26519-2-8220-9430.wav', 'S203-101-12886-26519-3-12430-13410.wav'], 'S2032651930011': ['S203-101-26519-30011-1-1310-2840.wav'], 'S2033099132371': ['S203-101-30991-32371-1-440-1350.wav'], 'S2033237139220': ['S203-101-32371-39220-1-430-810.wav', 'S203-101-32371-39220-2-4310-5310.wav', 'S203-101-32371-39220-3-6380-6850.wav'], 'S20336606851': ['S203-101-3660-6851-1-0-1250.wav', 'S203-101-3660-6851-2-1700-3200.wav'], 'S2033922049161': ['S203-101-39220-49161-1-5450-6350.wav', 'S203-101-39220-49161-2-6370-6750.wav', 'S203-101-39220-49161-3-7350-7870.wav', 'S203-101-39220-49161-4-8340-8800.wav'], 'S203685112886': ['S203-101-6851-12886-1-4310-5860.wav'], 'S2041222217136': ['S204-203-12222-17136-1-30-1890.wav', 'S204-203-12222-17136-2-2870-4150.wav', 'S204-203-12222-17136-3-4180-4870.wav'], 'S2041713627060': ['S204-203-17136-27060-1-1100-4940.wav', 'S204-203-17136-27060-2-5690-9930.wav'], 'S2042706034009': ['S204-203-27060-34009-1-0-850.wav', 'S204-203-27060-34009-2-1280-4430.wav', 'S204-203-27060-34009-3-5400-6920.wav'], 'S20444679075': ['S204-203-4467-9075-1-640-1550.wav', 'S204-203-4467-9075-2-1960-2270.wav', 'S204-203-4467-9075-3-2440-4520.wav'], 'S204907512222': ['S204-203-9075-12222-1-250-3150.wav'], 'S2051162121653': ['S205-305-11621-21653-1-1240-2390.wav', 'S205-305-11621-21653-2-4050-6000.wav', 'S205-305-11621-21653-3-7740-8180.wav'], 'S2053399237758': ['S205-305-33992-37758-1-2180-3770.wav'], 'S2054000046101': ['S205-305-40000-46101-1-1660-2900.wav', 'S205-305-40000-46101-2-3310-3740.wav', 'S205-305-40000-46101-3-3890-5270.wav'], 'S2054610150308': ['S205-305-46101-50308-1-2970-4210.wav'], 'S2055030855976': ['S205-305-50308-55976-1-2700-5670.wav'], 'S2055597657800': ['S205-305-55976-57800-1-0-340.wav', 'S205-305-55976-57800-2-420-1830.wav'], 'S205759811057': ['S205-305-7598-11057-1-1150-3460.wav'], 'S2061327916545': ['S206-231-13279-16545-1-910-3270.wav'], 'S2061654521370': ['S206-231-16545-21370-1-530-2190.wav', 'S206-231-16545-21370-2-2840-3970.wav', 'S206-231-16545-21370-3-4130-4830.wav'], 'S2062137024795': ['S206-231-21370-24795-1-0-310.wav', 'S206-231-21370-24795-2-1340-3310.wav'], 'S2062479534007': ['S206-231-24795-34007-1-1630-2330.wav', 'S206-231-24795-34007-2-3400-6900.wav', 'S206-231-24795-34007-3-6970-9220.wav'], 'S2063400737742': ['S206-231-34007-37742-1-1630-3740.wav'], 'S2063774247521': ['S206-231-37742-47521-1-1900-2630.wav', 'S206-231-37742-47521-2-5820-6160.wav', 'S206-231-37742-47521-3-6520-9780.wav'], 'S2064793654613': ['S206-231-47936-54613-1-70-950.wav', 'S206-231-47936-54613-2-3660-4730.wav', 'S206-231-47936-54613-3-4780-6680.wav'], 'S2065461358830': ['S206-231-54613-58830-1-0-330.wav', 'S206-231-54613-58830-2-2550-4220.wav'], 'S2066186864698': ['S206-231-61868-64698-1-800-2720.wav'], 'S2066451074923': ['S206-231-64510-74923-1-4490-4920.wav', 'S206-231-64510-74923-2-6790-8070.wav', 'S206-231-64510-74923-3-8240-10420.wav'], 'S206888813279': ['S206-231-8888-13279-1-1100-4400.wav'], 'S2071184515783': ['S207-219-11845-15783-1-490-1810.wav', 'S207-219-11845-15783-2-1910-3550.wav', 'S207-219-11845-15783-3-3690-3940.wav'], 'S2071578321012': ['S207-219-15783-21012-1-2580-5170.wav'], 'S20719637624': ['S207-219-1963-7624-1-0-760.wav', 'S207-219-1963-7624-2-1320-2610.wav', 'S207-219-1963-7624-3-2890-3360.wav', 'S207-219-1963-7624-4-3400-5510.wav'], 'S2072101225898': ['S207-219-21012-25898-1-2070-2380.wav', 'S207-219-21012-25898-2-2540-4740.wav'], 'S2072589828871': ['S207-219-25898-28871-1-1930-2980.wav'], 'S2072887132125': ['S207-219-28871-32125-1-0-3180.wav'], 'S2073212538318': ['S207-219-32125-38318-1-840-6200.wav'], 'S2073831847485': ['S207-219-38318-47485-1-260-620.wav', 'S207-219-38318-47485-2-1580-2050.wav', 'S207-219-38318-47485-3-4360-6430.wav', 'S207-219-38318-47485-4-7410-8000.wav', 'S207-219-38318-47485-5-8670-9070.wav'], 'S207762411845': ['S207-219-7624-11845-1-710-4230.wav']}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mC:\\Users\\THEGAM~1\\AppData\\Local\\Temp/ipykernel_11296/3746216348.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcurrentList\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m       \u001b[0msound1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_wav\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./test-mmse/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m       \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msound1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: from_wav() got an unexpected keyword argument 'encoding'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "7tcV_f4AYpIp",
        "outputId": "957e38c6-173d-484f-edce-ac3ae6906a59"
      },
      "source": [
        "chaPar = ChaFile('./testCHAS/S189.cha')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[1;32mC:\\Users\\THEGAM~1\\AppData\\Local\\Temp/ipykernel_11296/2411878248.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchaPar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChaFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./testCHAS/S189.cha'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mD:\\Projects\\Audio-Signal-Feature-Extraction-And-Clustering\\ChaFile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, chaFilePath, SPEAKER_IGNORE, TIER_IGNORE, CDS_ONLY, verbose, language)\u001b[0m\n\u001b[0;32m    284\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetLanguage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessLines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Projects\\Audio-Signal-Feature-Extraction-And-Clustering\\ChaFile.py\u001b[0m in \u001b[0;36msetLanguage\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m    370\u001b[0m                                 \u001b[0mloop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m                                 \u001b[1;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m                                         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m                                         \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m                                                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLANGUAGE_RE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2893: character maps to <undefined>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKxQVW0qs-Dy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7594a1c-6aa1-4e94-97a1-eef57eb641fd"
      },
      "source": [
        "import flash\n",
        "from flash.audio import SpeechRecognition, SpeechRecognitionData\n",
        "from flash.core.data.utils import download_data\n",
        "import textwrap\n",
        "from ChaFile import *\n",
        "import json\n",
        "\n",
        "listOFCHAs = os.listdir('./testCHAS')\n",
        "listOFWavs = os.listdir('./combinedTestWavs/')\n",
        "\n",
        "print(listOFCHAs)\n",
        "for cha in listOFCHAs:\n",
        "  if \".cha\" in cha:\n",
        "    num = cha.split('.')[0]\n",
        "    cha = './testCHAS/{}'.format(cha)\n",
        "    print(cha)\n",
        "    chaPar = ChaFile(cha, language='eng')\n",
        "    lines = chaPar.getLines()\n",
        "    for line in lines:\n",
        "      data = {} \n",
        "      data['file'] = \"./combinedTestWavs/{}-{}-{}.wav\".format(num,line['bullet'][0],line['bullet'][1])\n",
        "      data['text'] = line['emisión']\n",
        "      with open('./combinedTestWavs/jsons/{}-{}-{}.json'.format(num,line['bullet'][0],line['bullet'][1]), 'w') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "for wav in listOFWavs:\n",
        "  if 'S028' in wav:\n",
        "    print(wav)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['S160.cha', 'S161.cha', 'S162.cha', 'S163.cha', 'S164.cha', 'S165.cha', 'S166.cha', 'S167.cha', 'S168.cha', 'S169.cha', 'S170.cha', 'S171.cha', 'S172.cha', 'S173.cha', 'S174.cha', 'S175.cha', 'S176.cha', 'S177.cha', 'S178.cha', 'S179.cha', 'S180.cha', 'S181.cha', 'S182.cha', 'S183.cha', 'S184.cha', 'S185.cha', 'S186.cha', 'S187.cha', 'S188.cha', 'S189-REMOVED.txt', 'S189.cha', 'S190.cha', 'S191.cha', 'S192.cha', 'S193.cha', 'S194.cha', 'S195.cha', 'S196.cha', 'S197.cha', 'S198.cha', 'S199.cha', 'S200.cha', 'S201.cha', 'S202.cha', 'S203.cha', 'S204.cha', 'S205.cha', 'S206.cha', 'S207.cha']\n",
            "./testCHAS/S160.cha\n",
            "./testCHAS/S161.cha\n",
            "./testCHAS/S162.cha\n",
            "./testCHAS/S163.cha\n",
            "./testCHAS/S164.cha\n",
            "./testCHAS/S165.cha\n",
            "./testCHAS/S166.cha\n",
            "./testCHAS/S167.cha\n",
            "./testCHAS/S168.cha\n",
            "./testCHAS/S169.cha\n",
            "./testCHAS/S170.cha\n",
            "./testCHAS/S171.cha\n",
            "./testCHAS/S172.cha\n",
            "./testCHAS/S173.cha\n",
            "./testCHAS/S174.cha\n",
            "./testCHAS/S175.cha\n",
            "./testCHAS/S176.cha\n",
            "./testCHAS/S177.cha\n",
            "./testCHAS/S178.cha\n",
            "./testCHAS/S179.cha\n",
            "./testCHAS/S180.cha\n",
            "./testCHAS/S181.cha\n",
            "./testCHAS/S182.cha\n",
            "./testCHAS/S183.cha\n",
            "./testCHAS/S184.cha\n",
            "./testCHAS/S185.cha\n",
            "./testCHAS/S186.cha\n",
            "./testCHAS/S187.cha\n",
            "./testCHAS/S188.cha\n",
            "./testCHAS/S189.cha\n",
            "./testCHAS/S190.cha\n",
            "./testCHAS/S191.cha\n",
            "./testCHAS/S192.cha\n",
            "./testCHAS/S193.cha\n",
            "./testCHAS/S194.cha\n",
            "./testCHAS/S195.cha\n",
            "./testCHAS/S196.cha\n",
            "./testCHAS/S197.cha\n",
            "./testCHAS/S198.cha\n",
            "./testCHAS/S199.cha\n",
            "./testCHAS/S200.cha\n",
            "./testCHAS/S201.cha\n",
            "./testCHAS/S202.cha\n",
            "./testCHAS/S203.cha\n",
            "./testCHAS/S204.cha\n",
            "./testCHAS/S205.cha\n",
            "./testCHAS/S206.cha\n",
            "./testCHAS/S207.cha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "fc00e7a15c154e9c897db8f3190d9662",
            "2ffa73d8411f4ffea95bd5f2e3f6aa1c",
            "27f8bbfd80e04454b1f097c72b5c2580",
            "5b873e22daf7463a82ef2ca14f7db08d",
            "cacaa16a4bf947f1968ad19264b2073a",
            "dc0036eb77b949d29b1516f1f47f54e0",
            "504e66190e694a4bbc0ee236c2e7e1de",
            "3e6017457be544839fce4742d7d41294",
            "f4cec8bf797643dd991e8169a147bc87",
            "c244c621c8254dbfa4a6dab68b59dd43",
            "f3690f70e3f54899869bcbad03fd5809",
            "10c081a6f96a427c9647385705c89d94",
            "09dbe4ee4af945bf94a9f817bf509212",
            "b1d5f4a594b244cc94ca22356381c306",
            "ef6aff74e7d54a818183c215afd18d39",
            "6671f1a8226e42f796dc5a8a27406ee9",
            "8caee931326e4e029f3477436e40a399",
            "84ea7d402a52465b8c46b0805693f906",
            "ebb4f3537632421bb0e7e53d8dc295c8",
            "86db037ad7b14eee8e19bd0fcce73213",
            "6704e6e21cb74df2aa2ad92c0cf716ab",
            "44f7b24b1cdf4efeaa0aacf750a080f3",
            "dc4018ed0bb945aba2ecff1b423dea7f",
            "beb89aca0cb746d2ace33d41ebdfec87",
            "c6b2c629cd144bd982088aa8a5ccc360",
            "7561820e8692492ab0cf5b9ca4b84fbd",
            "6c9dcda1431441bfbce64f401fc92f6d",
            "c8b872473b6b44c8b75f577f85732e9d",
            "e0ee01261a274035b5d586dd8bcdc919",
            "2178c6baf7324bd9bb155d0b18cced1b",
            "6ad14c9335db4332b6690c8fd4d0502c",
            "151faaeca98d42669fd5cec260f55a0d",
            "57100b9f1010478a8d5eeee74f09e7df",
            "604c111371894b6b9ccbbbd61b400e64",
            "ef50a78983fd40df8d9743ec1de24aa6",
            "1e1931e662404c9db758f7cbb7fd8ec2",
            "78869b3f278843c09fd026a64d907760",
            "c602ecc710104c80994012b1531d2b07",
            "9794a54257ec49e5922432e1792d23aa",
            "a4895ba7198a40aaa85015462a0961da",
            "8b3d569c1b8849f0aeb49bd71e0b440d",
            "f7294cb0e2f44cedaca57e9a0ada910e",
            "593ad0d934db4079a7c4daa84383aa8b",
            "9e02a922fe9a4ee6bbb057dd69aef2eb",
            "761e7eb97362404699d6eb0f9b57e9da",
            "3934e88b127f46dcb35135279153e5c2",
            "a5887d27d3e945c5986a21e640e85666",
            "de8c6832db584a8fa0c1b4d0bd96a465",
            "0ac2451ed7d54d60bd6848b35067c771",
            "f1b6d43454d0451f8491b5cc35ed33e9",
            "ca3e4d58f0f24d819cf036d7ad358030",
            "4746c63c25c947d0b2362b79444e4bc3",
            "d56ef9ea90674eb38facf4e3ba4dc94b",
            "2c2ccf19c9b14ea09c1461d2e21249be",
            "5e7a743d323248ef867b75d94db95299"
          ]
        },
        "id": "4TLmeV0_KVTk",
        "outputId": "f1593c69-aa8b-4b3c-ef61-d8f2a639b946"
      },
      "source": [
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "inputs = tokenizer(\"Hello world!\", return_tensors=\"tf\")\n",
        "#outputs = model(**inputs)\n",
        "print(inputs)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc00e7a15c154e9c897db8f3190d9662"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10c081a6f96a427c9647385705c89d94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc4018ed0bb945aba2ecff1b423dea7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "604c111371894b6b9ccbbbd61b400e64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "761e7eb97362404699d6eb0f9b57e9da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 2088,  999,  102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]])>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G01ngKdbK-o"
      },
      "source": [
        "\n",
        "listOFPickles = os.listdir('./trainTokenPickles')\n",
        "listOFJson = os.listdir('./combinedTestWavs/jsons')\n",
        "\n",
        "tmep = []\n",
        "for jsonFile in listOFJson:\n",
        "  jsonFileName = jsonFile.split('.')[0]\n",
        "  with open('./testTokenPickles/{}.pickle'.format(jsonFileName), 'rb') as handle:\n",
        "    temp.append(pickle.load(handle))\n",
        "trainFeats = temp[1]\n",
        "trainLabels = temp[2]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oLUlZpPzSG-2",
        "outputId": "5520db90-c771-4ca8-a9f4-de5d1ff2bdda"
      },
      "source": [
        "import os\n",
        "listOFPickles = os.listdir('./trainTokenPickles')\n",
        "listOFJson = os.listdir('./combinedTestWavs/jsons')\n",
        "\n",
        "tmep = []\n",
        "for jsonFile in listOFJson:\n",
        "  jsonFileName = jsonFile.split('.')[0]\n",
        "  with open('./testTokenPickles/{}.pickle'.format(jsonFileName), 'rb') as handle:\n",
        "    temp.append(pickle.load(handle))\n",
        "  print(type(temp[0]))\n",
        "  print((temp[1]))\n",
        "  print(type(temp[2]))\n",
        "  print((temp[2]))\n",
        "\n",
        "shortTermStepList = [.001, 0.005, 0.01,0.015, 0.02]\n",
        "for shortTermStep in shortTermStepList:\n",
        "  for shortTermWindow in range(25,105,5):\n",
        "\n",
        "    lof = len(listOFEverything22) - 1\n",
        "    print(shortTermStep)\n",
        "    print(str(shortTermWindow) + \"\\n\")\n",
        "    trainDir = './real mmse scores'\n",
        "    testDir = './test-mmse'\n",
        "    shortTermWindow = float(shortTermWindow)/1000\n",
        "    \n",
        "    temp = pickle.load(open(\"./pickles/combineTrain/SS{}-SW{}.pickle\".format(shortTermStep, shortTermWindow), 'rb'))\n",
        "    trainFeats = temp[0]\n",
        "    trainLabels = temp[1]\n",
        "\n",
        "    temp = pickle.load(open(\"./pickles/combineTest/SS{}-SW{}.pickle\".format(shortTermStep, shortTermWindow), 'rb'))\n",
        "    testFeats = temp[0]\n",
        "    testLabels = temp[1]\n",
        "    #[features_norm[0], first8,mfccFeatures,chroma], regression_labels[0]\n",
        "    trainFeats\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
            "<class 'str'>\n",
            "27\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101,  1996,  2879,  2006,  1996, 14708, 13250,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mC:\\Users\\THEGAM~1\\AppData\\Local\\Temp/ipykernel_24240/913071331.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mshortTermWindow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m105\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mlof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistOFEverything22\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshortTermStep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshortTermWindow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'listOFEverything22' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFbh8jrvIzri"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "def createEmbeddings(tokenizer,model,sentenceTokens):\n",
        "  encoded_input = sentenceTokens\n",
        "  # Compute token embeddings\n",
        "  with torch.no_grad():\n",
        "      model_output = model(**encoded_input)\n",
        "  # Perform pooling. In this case, max pooling.\n",
        "  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "  return sentence_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "d699a982173443e6b419db092f4f72a6",
            "0fabb7a39d5a4681b3cd9b8bbdc99cfe",
            "80c064247973460694774bd7cbd6a924",
            "1ec1ba5447a541dda38582a909bcbc1d",
            "e9b96fbff3fa435c81b7bf855db7d060",
            "09232485d6fb4b858b22d981c0caff8d",
            "f199216a11d64de28a4510928a1bff40",
            "400ba32281d1453b8d185e82b8997ff7"
          ]
        },
        "id": "eLiCTxUCLzOG",
        "outputId": "9bd950aa-7c22-48cb-97e8-af18de01a2a3"
      },
      "source": [
        "import os\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "regex = re.compile('[^a-zA-Z]')\n",
        "#First parameter is the replacement, second parameter is your input string\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "listOFJson = os.listdir('./combinedTrainWavs/jsons')\n",
        "\n",
        "datas = []\n",
        "for jsonFile in listOFJson:\n",
        "  jsonFileName = jsonFile.split('.')[0]\n",
        "  data = json.load(open(\"./combinedTrainWavs/jsons/{}\".format(jsonFile),'r'))\n",
        "  datas.append(re.sub(\"[^a-zA-Z]+\", \" \", data['text']))\n",
        "  #inputs = tokenizer(re.sub(\"[^a-zA-Z]+\", \" \", data['text']), return_tensors=\"np\",truncation=True, padding=True, max_length=50)\n",
        "  #rmseVal = trainDict[jsonFileName.split('-')[0]]\n",
        "\n",
        "inputs = tokenizer(datas, return_tensors=\"pt\",truncation=True, padding='max_length', max_length=50)\n",
        "\n",
        "inputs = createEmbeddings(tokenizer,model,inputs)\n",
        "\n",
        "for jsonFile in range(len(listOFJson)):\n",
        "  jsonFileName = listOFJson[jsonFile].split('.')[0]\n",
        "  rmseVal = trainDict[jsonFileName.split('-')[0]]\n",
        "  with open('./trainTokenPickles/{}.pickle'.format(jsonFileName), 'wb') as handle:\n",
        "      pickle.dump([jsonFileName, rmseVal ,[inputs[jsonFile]]], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\"\"\"\n",
        "with open('./testTokenPickles/{}.pickle'.format(jsonFileName), 'wb') as handle:\n",
        "      pickle.dump([jsonFileName, rmseVal ,], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d699a982173443e6b419db092f4f72a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nwith open('./testTokenPickles/{}.pickle'.format(jsonFileName), 'wb') as handle:\\n      pickle.dump([jsonFileName, rmseVal ,], handle, protocol=pickle.HIGHEST_PROTOCOL)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IkSBEVDRthH",
        "outputId": "14d16dd9-1d30-4b49-d5c2-dac257c24b8a"
      },
      "source": [
        "import json\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "regex = re.compile('[^a-zA-Z]')\n",
        "#First parameter is the replacement, second parameter is your input string\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "listOFJson = os.listdir('./combinedTestWavs/jsons')\n",
        "\n",
        "datas = []\n",
        "for jsonFile in listOFJson:\n",
        "  jsonFileName = jsonFile.split('.')[0]\n",
        "  data = json.load(open(\"./combinedTestWavs/jsons/{}\".format(jsonFile),'r'))\n",
        "  datas.append(re.sub(\"[^a-zA-Z]+\", \" \", data['text']))\n",
        "  #inputs = tokenizer(re.sub(\"[^a-zA-Z]+\", \" \", data['text']), return_tensors=\"np\",truncation=True, padding=True, max_length=50)\n",
        "  #rmseVal = trainDict[jsonFileName.split('-')[0]]\n",
        "\n",
        "inputs = tokenizer(datas, return_tensors=\"pt\",truncation=True, padding='max_length', max_length=50)\n",
        "inputs = createEmbeddings(tokenizer,model,inputs)\n",
        "\n",
        "for jsonFile in range(len(listOFJson)):\n",
        "  jsonFileName = listOFJson[jsonFile].split('.')[0]\n",
        "  rmseVal = testDict[jsonFileName.split('-')[0]]\n",
        "  with open('./testTokenPickles/{}.pickle'.format(jsonFileName), 'wb') as handle:\n",
        "      pickle.dump([jsonFileName, rmseVal ,[inputs[jsonFile]]], handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhVNcfSUhDWK",
        "outputId": "de8b7059-3224-4f17-de6b-6325568ec981"
      },
      "source": [
        "print(len(inputs))\n",
        "print(type(inputs))\n",
        "print((inputs[0].size()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "801\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V06KH_PeNAMy",
        "outputId": "e18fee4e-0070-42c6-e186-60b31e6b7a6e"
      },
      "source": [
        "import torch\n",
        "import json\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "regex = re.compile('[^a-zA-Z]')\n",
        "#First parameter is the replacement, second parameter is your input string\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = TFAutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
        "\n",
        "train_ids = inputs.input_ids\n",
        "out = model(input_ids=train_ids)\n",
        "\n",
        "# the output is a tuple\n",
        "print(type(out))\n",
        "# the tuple contains three elements as explained above)\n",
        "print(len(out))\n",
        "# we only want the hidden_states\n",
        "hidden_states = out[2]\n",
        "print(len(hidden_states))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling'>\n",
            "3\n",
            "13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "_F00DagsRnrO",
        "outputId": "bf5ebcf3-328e-41ea-b5a8-a8ebefbfae56"
      },
      "source": [
        "sentence_embedding = torch.mean(hidden_states[-1], dim=1).squeeze()\n",
        "print(sentence_embedding)\n",
        "print(sentence_embedding.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mC:\\Users\\THEGAM~1\\AppData\\Local\\Temp/ipykernel_672/2129844364.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msentence_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_embedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: mean() received an invalid combination of arguments - got (tensorflow.python.framework.ops.EagerTensor, dim=int), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnYXYYsinear",
        "outputId": "955028ff-8004-4d84-d77c-00ee33d95540"
      },
      "source": [
        "import os\n",
        "\n",
        "trainDict = {}\n",
        "testDict = {}\n",
        "with open('trainRmse.txt','r') as f:\n",
        "  for line in f.readlines():\n",
        "    row = line.split(';')\n",
        "    trainDict[row[0].strip()] = row[3].strip()\n",
        "with open('testRmse.txt','r') as f:\n",
        "  for line in f.readlines():\n",
        "    row = line.split(';')\n",
        "    if row[4].strip() == 'NA':\n",
        "      testDict[row[0].strip()] = '30'\n",
        "    else:\n",
        "      testDict[row[0].strip()] = row[4].strip()\n",
        "\n",
        "trainDict['S001'] = '30'\n",
        "\n",
        "print(trainDict)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ID': 'mmse', 'S001': '30', 'S002': '30', 'S003': '29', 'S004': '30', 'S005': '30', 'S006': '29', 'S007': '28', 'S009': '30', 'S011': '30', 'S012': '29', 'S013': '30', 'S015': '29', 'S016': '30', 'S017': '28', 'S018': '29', 'S019': '27', 'S020': '27', 'S021': '30', 'S024': '30', 'S025': '28', 'S027': '29', 'S028': '29', 'S029': '29', 'S030': '30', 'S032': '28', 'S033': '30', 'S034': '29', 'S035': '30', 'S036': '28', 'S038': '30', 'S039': '28', 'S040': '30', 'S041': '30', 'S043': '30', 'S048': '26', 'S049': '30', 'S051': '29', 'S052': '27', 'S055': '29', 'S056': '30', 'S058': '30', 'S059': '30', 'S061': '30', 'S062': '30', 'S063': '29', 'S064': '29', 'S067': '30', 'S068': '29', 'S070': '28', 'S071': '29', 'S072': '29', 'S073': '29', 'S076': '28', 'S077': '29', 'S079': '11', 'S080': '19', 'S081': '19', 'S082': '11', 'S083': '30', 'S084': '21', 'S086': '18', 'S087': '7', 'S089': '18', 'S090': '8', 'S092': '15', 'S093': '25', 'S094': '17', 'S095': '24', 'S096': '17', 'S097': '15', 'S100': '14', 'S101': '24', 'S103': '27', 'S104': '27', 'S107': '22', 'S108': '13', 'S110': '16', 'S111': '1', 'S114': '16', 'S116': '12', 'S118': '10', 'S122': '17', 'S124': '19', 'S125': '13', 'S126': '13', 'S127': '19', 'S128': '16', 'S129': '19', 'S130': '18', 'S132': '11', 'S135': '14', 'S136': '23', 'S137': '23', 'S138': '17', 'S139': '25', 'S140': '17', 'S141': '19', 'S142': '14', 'S143': '18', 'S144': '17', 'S145': '14', 'S148': '17', 'S149': '12', 'S150': '20', 'S151': '24', 'S153': '12', 'S154': '20', 'S156': '13'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVJ2vfq5tt8M",
        "outputId": "b1b767e2-8050-4456-83d9-c5473aca240b"
      },
      "source": [
        "import csv\n",
        "data = []\n",
        "for wav in os.listdir('./combinedTestWavs/'):\n",
        "  if '.wav' in wav:\n",
        "    data.append([wav, testDict[wav.split('-')[0]]])\n",
        "\n",
        "print(data)\n",
        "f = open('./combinedTestWavs/mmseScore.csv', 'a')\n",
        "writer = csv.writer(f)\n",
        "writer.writerows(data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['S160-0-3957.wav', '28'], ['S160-12826-15114.wav', '28'], ['S160-15114-20622.wav', '28'], ['S160-20622-24498.wav', '28'], ['S160-3957-12857.wav', '28'], ['S161-0-8530.wav', '29'], ['S161-11938-15645.wav', '29'], ['S161-15645-18365.wav', '29'], ['S161-18365-20234.wav', '29'], ['S161-49833-55717.wav', '29'], ['S161-55717-58690.wav', '29'], ['S161-58690-63629.wav', '29'], ['S161-63629-66500.wav', '29'], ['S161-66500-70518.wav', '29'], ['S161-70518-72604.wav', '29'], ['S161-8530-11938.wav', '29'], ['S162-13780-16730.wav', '24'], ['S162-16730-20296.wav', '24'], ['S162-20296-24145.wav', '24'], ['S162-24145-32160.wav', '24'], ['S162-32160-36236.wav', '24'], ['S162-36236-40368.wav', '24'], ['S162-40368-44916.wav', '24'], ['S162-44916-51331.wav', '24'], ['S162-51331-52963.wav', '24'], ['S162-52963-57013.wav', '24'], ['S162-57013-60913.wav', '24'], ['S162-60913-64843.wav', '24'], ['S162-64843-68606.wav', '24'], ['S162-68606-71638.wav', '24'], ['S162-71638-72967.wav', '24'], ['S163-11400-16529.wav', '30'], ['S163-16529-21200.wav', '30'], ['S163-21200-27004.wav', '30'], ['S163-27004-37689.wav', '30'], ['S163-3972-8850.wav', '30'], ['S163-42100-46800.wav', '30'], ['S163-46800-56300.wav', '30'], ['S163-56300-61000.wav', '30'], ['S163-61000-64109.wav', '30'], ['S163-64109-66303.wav', '30'], ['S163-66303-68698.wav', '30'], ['S163-68698-70977.wav', '30'], ['S163-70977-72750.wav', '30'], ['S163-72750-74735.wav', '30'], ['S163-8950-11400.wav', '30'], ['S164-0-6759.wav', '21'], ['S164-11923-14648.wav', '21'], ['S164-14648-16664.wav', '21'], ['S164-16664-18111.wav', '21'], ['S164-18111-18727.wav', '21'], ['S164-18727-24016.wav', '21'], ['S164-24016-27600.wav', '21'], ['S164-6759-11923.wav', '21'], ['S165-12555-15783.wav', '15'], ['S165-15783-21000.wav', '15'], ['S165-21718-28077.wav', '15'], ['S165-28077-34660.wav', '15'], ['S165-34660-38483.wav', '15'], ['S165-38483-44669.wav', '15'], ['S165-44669-47360.wav', '15'], ['S165-48591-53535.wav', '15'], ['S165-53535-56323.wav', '15'], ['S165-5870-7815.wav', '15'], ['S165-67113-68717.wav', '15'], ['S165-71812-77055.wav', '15'], ['S165-77055-77896.wav', '15'], ['S165-7815-8246.wav', '15'], ['S165-80153-86423.wav', '15'], ['S165-90174-96000.wav', '15'], ['S166-102050-106922.wav', '29'], ['S166-106922-108228.wav', '29'], ['S166-108228-109999.wav', '29'], ['S166-109999-115857.wav', '29'], ['S166-115857-119668.wav', '29'], ['S166-11813-12444.wav', '29'], ['S166-119668-122656.wav', '29'], ['S166-122656-128770.wav', '29'], ['S166-12444-18909.wav', '29'], ['S166-134016-140834.wav', '29'], ['S166-140834-149371.wav', '29'], ['S166-149371-160002.wav', '29'], ['S166-160002-165760.wav', '29'], ['S166-1738-3144.wav', '29'], ['S166-18909-25333.wav', '29'], ['S166-25333-28900.wav', '29'], ['S166-28900-35783.wav', '29'], ['S166-35783-40372.wav', '29'], ['S166-43733-45222.wav', '29'], ['S166-45222-46250.wav', '29'], ['S166-5080-6693.wav', '29'], ['S166-550-1738.wav', '29'], ['S166-55455-62888.wav', '29'], ['S166-62888-67392.wav', '29'], ['S166-6693-9884.wav', '29'], ['S166-67392-78690.wav', '29'], ['S166-78690-80619.wav', '29'], ['S166-80619-81782.wav', '29'], ['S166-81782-87414.wav', '29'], ['S166-87414-88836.wav', '29'], ['S166-88836-91659.wav', '29'], ['S166-91659-102050.wav', '29'], ['S167-110828-148000.wav', '28'], ['S167-148000-151227.wav', '28'], ['S167-172005-172560.wav', '28'], ['S167-174388-176861.wav', '28'], ['S167-178513-180412.wav', '28'], ['S167-180412-185876.wav', '28'], ['S167-185876-190327.wav', '28'], ['S167-1885-9809.wav', '28'], ['S167-190327-193394.wav', '28'], ['S167-193394-199962.wav', '28'], ['S167-201248-209314.wav', '28'], ['S167-213064-214803.wav', '28'], ['S167-215510-219367.wav', '28'], ['S167-219367-220686.wav', '28'], ['S167-221358-226445.wav', '28'], ['S167-226444-229071.wav', '28'], ['S167-229071-234991.wav', '28'], ['S167-234991-246500.wav', '28'], ['S167-24088-46364.wav', '28'], ['S167-246500-250053.wav', '28'], ['S167-250053-253976.wav', '28'], ['S167-253976-259645.wav', '28'], ['S167-259645-265513.wav', '28'], ['S167-265513-267007.wav', '28'], ['S167-46364-50784.wav', '28'], ['S167-75470-76285.wav', '28'], ['S167-77115-78036.wav', '28'], ['S167-78036-87045.wav', '28'], ['S167-87045-110828.wav', '28'], ['S167-9809-24088.wav', '28'], ['S168-0-11394.wav', '27'], ['S168-11394-16202.wav', '27'], ['S168-16202-21324.wav', '27'], ['S168-21324-25281.wav', '27'], ['S168-25281-35967.wav', '27'], ['S169-12204-15223.wav', '26'], ['S169-15223-19034.wav', '26'], ['S169-19034-23098.wav', '26'], ['S169-23098-27456.wav', '26'], ['S169-27456-40512.wav', '26'], ['S169-63600-64857.wav', '26'], ['S169-7409-12204.wav', '26'], ['S170-0-7099.wav', '28'], ['S170-17758-40528.wav', '28'], ['S170-7099-17758.wav', '28'], ['S171-17836-26000.wav', '23'], ['S171-26000-42894.wav', '23'], ['S171-42894-51982.wav', '23'], ['S171-51982-66666.wav', '23'], ['S171-66666-67526.wav', '23'], ['S172-11750-18750.wav', '30'], ['S172-18750-25510.wav', '30'], ['S172-25510-28354.wav', '30'], ['S172-28354-31111.wav', '30'], ['S172-31111-36133.wav', '30'], ['S172-36133-38692.wav', '30'], ['S172-38692-41600.wav', '30'], ['S172-41600-46568.wav', '30'], ['S172-46568-52153.wav', '30'], ['S172-52153-58500.wav', '30'], ['S172-58500-70564.wav', '30'], ['S172-9400-11750.wav', '30'], ['S173-19602-23094.wav', '17'], ['S173-23094-27000.wav', '17'], ['S173-27000-30574.wav', '17'], ['S173-30574-33018.wav', '17'], ['S173-35629-39369.wav', '17'], ['S173-39369-42878.wav', '17'], ['S173-42878-44739.wav', '17'], ['S173-44739-47017.wav', '17'], ['S173-47017-52137.wav', '17'], ['S173-53681-57706.wav', '17'], ['S173-57706-58803.wav', '17'], ['S173-61646-66684.wav', '17'], ['S173-72451-73500.wav', '17'], ['S173-73500-74114.wav', '17'], ['S174-103232-106130.wav', '29'], ['S174-111231-113053.wav', '29'], ['S174-11935-19846.wav', '29'], ['S174-19846-24219.wav', '29'], ['S174-24219-33196.wav', '29'], ['S174-33196-42631.wav', '29'], ['S174-50397-65944.wav', '29'], ['S174-65944-71860.wav', '29'], ['S174-71860-81648.wav', '29'], ['S174-81648-86454.wav', '29'], ['S174-86454-92897.wav', '29'], ['S174-92897-103232.wav', '29'], ['S175-0-2522.wav', '30'], ['S175-100518-107326.wav', '30'], ['S175-107326-114116.wav', '30'], ['S175-114116-119936.wav', '30'], ['S175-14073-18724.wav', '30'], ['S175-18724-22233.wav', '30'], ['S175-22233-25473.wav', '30'], ['S175-2522-6133.wav', '30'], ['S175-25473-29199.wav', '30'], ['S175-29199-31162.wav', '30'], ['S175-31162-32375.wav', '30'], ['S175-32375-34023.wav', '30'], ['S175-34023-43476.wav', '30'], ['S175-45382-46099.wav', '30'], ['S175-47562-52666.wav', '30'], ['S175-52666-58458.wav', '30'], ['S175-58458-74691.wav', '30'], ['S175-6133-14073.wav', '30'], ['S175-74691-77289.wav', '30'], ['S175-77289-80294.wav', '30'], ['S175-80294-82393.wav', '30'], ['S175-82393-92928.wav', '30'], ['S175-92928-100518.wav', '30'], ['S176-13878-15200.wav', '27'], ['S176-15200-20234.wav', '27'], ['S176-1996-6385.wav', '27'], ['S176-20234-31238.wav', '27'], ['S176-34038-39622.wav', '27'], ['S176-39622-44298.wav', '27'], ['S176-44076-44990.wav', '27'], ['S176-44990-48511.wav', '27'], ['S176-48511-51603.wav', '27'], ['S176-51603-60915.wav', '27'], ['S176-60915-68136.wav', '27'], ['S176-6385-13878.wav', '27'], ['S176-68136-70260.wav', '27'], ['S176-70260-80000.wav', '27'], ['S176-81084-83544.wav', '27'], ['S176-83544-84710.wav', '27'], ['S176-84710-93754.wav', '27'], ['S176-93832-94689.wav', '27'], ['S177-103602-104700.wav', '30'], ['S177-15661-20964.wav', '30'], ['S177-20964-23200.wav', '30'], ['S177-23200-25011.wav', '30'], ['S177-2377-5120.wav', '30'], ['S177-25011-30681.wav', '30'], ['S177-30681-35518.wav', '30'], ['S177-35518-37633.wav', '30'], ['S177-37633-41944.wav', '30'], ['S177-41944-49819.wav', '30'], ['S177-51002-55574.wav', '30'], ['S177-55574-58583.wav', '30'], ['S177-58583-65166.wav', '30'], ['S177-65166-73941.wav', '30'], ['S177-85911-89521.wav', '30'], ['S177-89521-97750.wav', '30'], ['S178-0-7565.wav', '30'], ['S178-17899-21481.wav', '30'], ['S178-21481-26023.wav', '30'], ['S178-26023-31719.wav', '30'], ['S178-31719-41122.wav', '30'], ['S178-41122-44622.wav', '30'], ['S178-44622-54942.wav', '30'], ['S178-54942-65942.wav', '30'], ['S178-65942-73340.wav', '30'], ['S178-73423-74521.wav', '30'], ['S178-7565-10429.wav', '30'], ['S179-0-4016.wav', '10'], ['S179-13496-18818.wav', '10'], ['S179-18818-24795.wav', '10'], ['S179-24795-36111.wav', '10'], ['S179-36111-38102.wav', '10'], ['S179-4016-8825.wav', '10'], ['S179-46770-52935.wav', '10'], ['S179-52935-58299.wav', '10'], ['S179-8825-13496.wav', '10'], ['S180-15721-22041.wav', '29'], ['S180-22041-31562.wav', '29'], ['S180-2725-15721.wav', '29'], ['S180-31562-39768.wav', '29'], ['S180-39768-53416.wav', '29'], ['S180-56823-66818.wav', '29'], ['S181-11175-17355.wav', '17'], ['S181-17355-21000.wav', '17'], ['S181-2025-4385.wav', '17'], ['S181-21000-23454.wav', '17'], ['S181-23454-27753.wav', '17'], ['S181-27753-29445.wav', '17'], ['S181-29445-49980.wav', '17'], ['S181-4385-8555.wav', '17'], ['S181-49990-52341.wav', '17'], ['S181-52341-55795.wav', '17'], ['S181-55795-65382.wav', '17'], ['S181-65382-67952.wav', '17'], ['S181-67952-70953.wav', '17'], ['S181-70953-77338.wav', '17'], ['S181-78409-93989.wav', '17'], ['S181-8555-11175.wav', '17'], ['S181-93589-94829.wav', '17'], ['S181-94450-95792.wav', '17'], ['S181-95092-96100.wav', '17'], ['S181-96105-97071.wav', '17'], ['S181-97071-97521.wav', '17'], ['S181-97521-98237.wav', '17'], ['S182-105407-106783.wav', '12'], ['S182-108415-121112.wav', '12'], ['S182-121987-127871.wav', '12'], ['S182-128806-141346.wav', '12'], ['S182-28996-36222.wav', '12'], ['S182-36789-37777.wav', '12'], ['S182-39141-42980.wav', '12'], ['S182-46226-60300.wav', '12'], ['S182-62923-67361.wav', '12'], ['S182-67361-68152.wav', '12'], ['S182-71158-77026.wav', '12'], ['S182-77646-85000.wav', '12'], ['S182-85430-87249.wav', '12'], ['S182-9650-16996.wav', '12'], ['S183-11800-14400.wav', '30'], ['S183-14400-17676.wav', '30'], ['S183-17676-20600.wav', '30'], ['S183-20600-23051.wav', '30'], ['S183-23051-30972.wav', '30'], ['S183-30972-35798.wav', '30'], ['S183-35798-43813.wav', '30'], ['S183-43813-49072.wav', '30'], ['S183-49072-51998.wav', '30'], ['S183-51998-55004.wav', '30'], ['S183-57000-58250.wav', '30'], ['S183-58250-63001.wav', '30'], ['S183-63001-65195.wav', '30'], ['S183-65195-68671.wav', '30'], ['S183-68671-72500.wav', '30'], ['S183-72500-82346.wav', '30'], ['S183-7409-11800.wav', '30'], ['S183-82346-91809.wav', '30'], ['S183-91809-95500.wav', '30'], ['S183-95600-97443.wav', '30'], ['S184-12466-14179.wav', '29'], ['S184-14179-17028.wav', '29'], ['S184-17028-19844.wav', '29'], ['S184-19844-25883.wav', '29'], ['S184-25883-33072.wav', '29'], ['S184-33072-36442.wav', '29'], ['S184-36442-39422.wav', '29'], ['S184-39422-43610.wav', '29'], ['S184-43610-45417.wav', '29'], ['S184-45417-48371.wav', '29'], ['S184-5542-12466.wav', '29'], ['S184-57958-63935.wav', '29'], ['S184-63935-67283.wav', '29'], ['S184-67283-73448.wav', '29'], ['S184-73448-77077.wav', '29'], ['S184-77077-79283.wav', '29'], ['S184-79283-82832.wav', '29'], ['S184-82832-90006.wav', '29'], ['S184-97661-99840.wav', '29'], ['S185-11936-18420.wav', '19'], ['S185-18420-23395.wav', '19'], ['S185-23395-26755.wav', '19'], ['S185-28140-29183.wav', '19'], ['S185-29183-32857.wav', '19'], ['S185-3020-5464.wav', '19'], ['S185-35284-35749.wav', '19'], ['S185-35774-40590.wav', '19'], ['S185-40590-42572.wav', '19'], ['S185-42572-45595.wav', '19'], ['S185-45595-49345.wav', '19'], ['S185-49345-50797.wav', '19'], ['S185-50797-54600.wav', '19'], ['S185-5464-9590.wav', '19'], ['S185-9590-11936.wav', '19'], ['S186-12888-20765.wav', '29'], ['S186-1963-7183.wav', '29'], ['S186-20765-29623.wav', '29'], ['S186-29623-38585.wav', '29'], ['S186-38585-42250.wav', '29'], ['S186-42250-43701.wav', '29'], ['S186-43701-55661.wav', '29'], ['S186-56139-63954.wav', '29'], ['S186-63954-65417.wav', '29'], ['S186-65417-66314.wav', '29'], ['S186-7183-12888.wav', '29'], ['S187-102334-104310.wav', '18'], ['S187-106210-109939.wav', '18'], ['S187-125088-136477.wav', '18'], ['S187-12724-17727.wav', '18'], ['S187-137308-146876.wav', '18'], ['S187-146876-152763.wav', '18'], ['S187-152763-155685.wav', '18'], ['S187-158504-161678.wav', '18'], ['S187-17727-21635.wav', '18'], ['S187-21635-25805.wav', '18'], ['S187-25805-38099.wav', '18'], ['S187-38099-60731.wav', '18'], ['S187-3832-12724.wav', '18'], ['S187-60731-69806.wav', '18'], ['S187-82319-101353.wav', '18'], ['S188-24901-37027.wav', '20'], ['S188-37027-45491.wav', '20'], ['S188-45491-58192.wav', '20'], ['S188-7315-23900.wav', '20'], ['S188-93240-94285.wav', '20'], ['S189-1463-4190.wav', '20'], ['S189-16000-17824.wav', '20'], ['S189-17824-23860.wav', '20'], ['S189-23860-29895.wav', '20'], ['S189-29895-31441.wav', '20'], ['S189-31441-36097.wav', '20'], ['S189-36097-38741.wav', '20'], ['S189-38741-45555.wav', '20'], ['S189-4190-8812.wav', '20'], ['S189-45555-48125.wav', '20'], ['S189-48125-59936.wav', '20'], ['S189-59936-74082.wav', '20'], ['S189-8812-16000.wav', '20'], ['S190-101302-104244.wav', '13'], ['S190-10241-20544.wav', '13'], ['S190-20544-37402.wav', '13'], ['S190-54364-58052.wav', '13'], ['S190-58052-61506.wav', '13'], ['S190-61506-78361.wav', '13'], ['S190-78361-90268.wav', '13'], ['S190-90268-98888.wav', '13'], ['S190-98888-101302.wav', '13'], ['S191-1713-2926.wav', '22'], ['S191-23051-26521.wav', '22'], ['S191-26521-44108.wav', '22'], ['S191-2926-23051.wav', '22'], ['S191-44108-57183.wav', '22'], ['S191-68777-70581.wav', '22'], ['S191-72729-84730.wav', '22'], ['S191-84730-103919.wav', '22'], ['S192-106220-109646.wav', '12'], ['S192-109646-112040.wav', '12'], ['S192-112744-116014.wav', '12'], ['S192-117120-118883.wav', '12'], ['S192-120090-122241.wav', '12'], ['S192-125526-126486.wav', '12'], ['S192-16374-29399.wav', '12'], ['S192-29399-47548.wav', '12'], ['S192-2958-4600.wav', '12'], ['S192-47548-50909.wav', '12'], ['S192-5167-16374.wav', '12'], ['S192-67577-74035.wav', '12'], ['S192-74035-88386.wav', '12'], ['S192-88386-94441.wav', '12'], ['S192-94441-106220.wav', '12'], ['S193-11022-17522.wav', '24'], ['S193-17522-19081.wav', '24'], ['S193-19081-22076.wav', '24'], ['S193-22076-30209.wav', '24'], ['S193-2507-11022.wav', '24'], ['S193-30209-35712.wav', '24'], ['S193-35712-40000.wav', '24'], ['S193-40000-42560.wav', '24'], ['S193-46385-48002.wav', '24'], ['S193-48002-49586.wav', '24'], ['S193-49586-51496.wav', '24'], ['S193-51496-54863.wav', '24'], ['S193-54863-57229.wav', '24'], ['S193-57229-58410.wav', '24'], ['S193-58410-62146.wav', '24'], ['S193-62146-67673.wav', '24'], ['S194-10478-12935.wav', '11'], ['S194-12935-16209.wav', '11'], ['S194-16209-25827.wav', '11'], ['S194-25827-27890.wav', '11'], ['S194-27890-30057.wav', '11'], ['S194-32142-34250.wav', '11'], ['S194-4701-9731.wav', '11'], ['S195-14103-16745.wav', '26'], ['S195-16745-18042.wav', '26'], ['S195-18042-21965.wav', '26'], ['S195-21965-24626.wav', '26'], ['S195-24626-27600.wav', '26'], ['S195-27600-32389.wav', '26'], ['S195-38972-39770.wav', '26'], ['S195-7170-8633.wav', '26'], ['S195-8633-14103.wav', '26'], ['S196-15300-17816.wav', '30'], ['S196-17816-21099.wav', '30'], ['S196-21107-31222.wav', '30'], ['S196-31222-38555.wav', '30'], ['S196-38555-38912.wav', '30'], ['S196-3969-8141.wav', '30'], ['S196-8141-15300.wav', '30'], ['S197-13355-19377.wav', '28'], ['S197-20918-24639.wav', '28'], ['S197-24639-28855.wav', '28'], ['S197-28855-37508.wav', '28'], ['S197-37508-42022.wav', '28'], ['S197-51269-52016.wav', '28'], ['S197-8950-13355.wav', '28'], ['S198-0-11078.wav', '19'], ['S198-11078-13538.wav', '19'], ['S198-13538-17378.wav', '19'], ['S198-17378-19655.wav', '19'], ['S198-19655-21767.wav', '19'], ['S198-21767-25126.wav', '19'], ['S198-25126-28335.wav', '19'], ['S198-28335-32173.wav', '19'], ['S198-32063-37085.wav', '19'], ['S198-37085-41838.wav', '19'], ['S198-48600-50513.wav', '19'], ['S199-12904-18004.wav', '30'], ['S199-18004-23098.wav', '30'], ['S199-2632-12438.wav', '30'], ['S199-33057-36857.wav', '30'], ['S199-36857-43578.wav', '30'], ['S199-43578-46555.wav', '30'], ['S199-46555-48621.wav', '30'], ['S199-48621-52219.wav', '30'], ['S199-52219-55830.wav', '30'], ['S200-13091-17264.wav', '25'], ['S200-17264-21500.wav', '25'], ['S200-21500-23579.wav', '25'], ['S200-2335-9300.wav', '25'], ['S200-9300-13091.wav', '25'], ['S201-13188-16927.wav', '30'], ['S201-16927-18586.wav', '30'], ['S201-18586-22491.wav', '30'], ['S201-30878-38412.wav', '30'], ['S201-3860-9530.wav', '30'], ['S201-48605-53616.wav', '30'], ['S201-9530-13188.wav', '30'], ['S202-22888-23504.wav', '30'], ['S202-23504-23887.wav', '30'], ['S202-24290-25900.wav', '30'], ['S202-26570-33450.wav', '30'], ['S202-33450-37691.wav', '30'], ['S202-37691-43450.wav', '30'], ['S202-43450-47307.wav', '30'], ['S202-47307-53525.wav', '30'], ['S202-53525-59211.wav', '30'], ['S202-59211-61954.wav', '30'], ['S202-61954-68703.wav', '30'], ['S202-6352-20745.wav', '30'], ['S202-69802-78329.wav', '30'], ['S203-12886-26519.wav', '18'], ['S203-26519-30011.wav', '18'], ['S203-30991-32371.wav', '18'], ['S203-32371-39220.wav', '18'], ['S203-3660-6851.wav', '18'], ['S203-39220-49161.wav', '18'], ['S203-6851-12886.wav', '18'], ['S204-12222-17136.wav', '28'], ['S204-17136-27060.wav', '28'], ['S204-27060-34009.wav', '28'], ['S204-4467-9075.wav', '28'], ['S204-9075-12222.wav', '28'], ['S205-11621-21653.wav', '23'], ['S205-33992-37758.wav', '23'], ['S205-40000-46101.wav', '23'], ['S205-46101-50308.wav', '23'], ['S205-50308-55976.wav', '23'], ['S205-55976-57800.wav', '23'], ['S205-7598-11057.wav', '23'], ['S206-13279-16545.wav', '28'], ['S206-16545-21370.wav', '28'], ['S206-21370-24795.wav', '28'], ['S206-24795-34007.wav', '28'], ['S206-34007-37742.wav', '28'], ['S206-37742-47521.wav', '28'], ['S206-47936-54613.wav', '28'], ['S206-54613-58830.wav', '28'], ['S206-61868-64698.wav', '28'], ['S206-64510-74923.wav', '28'], ['S206-8888-13279.wav', '28'], ['S207-11845-15783.wav', '27'], ['S207-15783-21012.wav', '27'], ['S207-1963-7624.wav', '27'], ['S207-21012-25898.wav', '27'], ['S207-25898-28871.wav', '27'], ['S207-28871-32125.wav', '27'], ['S207-32125-38318.wav', '27'], ['S207-38318-47485.wav', '27'], ['S207-7624-11845.wav', '27']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "vw6FPAMmHLV9",
        "outputId": "34795127-fa9d-431f-9e85-378a6d85531d"
      },
      "source": [
        "import flash\n",
        "from flash.audio import SpeechRecognition, SpeechRecognitionData\n",
        "from flash.core.data.utils import download_data\n",
        "import textwrap\n",
        "from ChaFile import *\n",
        "import json\n",
        "\n",
        "datamodule = SpeechRecognitionData.from_json(\n",
        "    input_fields=\"file\",\n",
        "    target_fields=\"text\",\n",
        "    train_file=\"./combinedTrainWavs/data.json\",\n",
        "    test_file=\"./combinedTestWavs/data.json\",\n",
        ")\n",
        "\n",
        "model = SpeechRecognition(backbone=\"facebook/wav2vec2-base-960h\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32mC:\\Users\\THEGAM~1\\AppData\\Local\\Temp/ipykernel_11296/3938671235.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m datamodule = SpeechRecognitionData.from_json(\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0minput_fields\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"file\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtarget_fields\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\flash\\core\\data\\data_module.py\u001b[0m in \u001b[0;36mfrom_json\u001b[1;34m(cls, input_fields, target_fields, train_file, val_file, test_file, predict_file, train_transform, val_transform, test_transform, predict_transform, data_fetcher, preprocess, val_split, batch_size, num_workers, sampler, field, **preprocess_kwargs)\u001b[0m\n\u001b[0;32m   1003\u001b[0m             )\n\u001b[0;32m   1004\u001b[0m         \"\"\"\n\u001b[1;32m-> 1005\u001b[1;33m         return cls.from_data_source(\n\u001b[0m\u001b[0;32m   1006\u001b[0m             \u001b[0mDefaultDataSources\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJSON\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_fields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\flash\\core\\data\\data_module.py\u001b[0m in \u001b[0;36mfrom_data_source\u001b[1;34m(cls, data_source, train_data, val_data, test_data, predict_data, train_transform, val_transform, test_transform, predict_transform, data_fetcher, preprocess, val_split, batch_size, num_workers, sampler, **preprocess_kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m         \"\"\"\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         preprocess = preprocess or cls.preprocess_cls(\n\u001b[0m\u001b[0;32m    562\u001b[0m             \u001b[0mtrain_transform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mval_transform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\flash\\core\\utilities\\imports.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[1;33m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m                 raise ModuleNotFoundError(\n\u001b[0m\u001b[0;32m    184\u001b[0m                     \u001b[1;34mf\"Required dependencies not available. Please run: pip install {' '.join(modules)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                 )\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: Required dependencies not available. Please run: pip install 'lightning-flash[audio]'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "Ege5SeFxCPPm",
        "outputId": "cff481e1-b8b6-4db3-a004-f89d65e68a2c"
      },
      "source": [
        "# The user only specifies the input nodes and output heads.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import autokeras as ak\n",
        "import pickle\n",
        "import os\n",
        "import math\n",
        "\n",
        "\n",
        "def tryDifferentWindows():\n",
        "  listOFEverything22 = []\n",
        "  notFirstRun = False\n",
        "  valMet = True\n",
        "  listOFEverything = listOFEverything22.copy()\n",
        "  shortTermStepList = [.001, 0.005, 0.01,0.015, 0.02]\n",
        "  for shortTermStep in shortTermStepList:\n",
        "    for shortTermWindow in range(25,105,5):\n",
        "      lof = len(listOFEverything22) - 1\n",
        "      print(shortTermStep)\n",
        "      print(str(shortTermWindow) + \"\\n\")\n",
        "      trainDir = './real mmse scores'\n",
        "      testDir = './test-mmse'\n",
        "      shortTermWindow = float(shortTermWindow)/1000\n",
        "      \n",
        "      temp = pickle.load(open(\"./pickles/train/SS{}-SW{}.pickle\".format(shortTermStep, shortTermWindow), 'rb'))\n",
        "      trainFeats = temp[0]\n",
        "      trainLabels = temp[1]\n",
        "\n",
        "      temp = pickle.load(open(\"./pickles/test/SS{}-SW{}.pickle\".format(shortTermStep, shortTermWindow), 'rb'))\n",
        "      testFeats = temp[0]\n",
        "      testLabels = temp[1]\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "      \n",
        "      with open(\"autoKeras-1-10.pickle\", 'wb') as handle:\n",
        "        pickle.dump(listOFEverything, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "      \n",
        "\n",
        "  return listOFEverything\n",
        "\n",
        "listOFEverything = tryDifferentWindows()\n",
        "\n",
        "with open(\"rf-pca-1-10.pickle\", 'wb') as handle:\n",
        "  pickle.dump(listOFEverything, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-4249d361504a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtestLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainFeats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestFeats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    758\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m     \"\"\"\n\u001b[0;32m--> 760\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3328\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3329\u001b[0m       batch_dim.assert_is_compatible_with(tensor_shape.Dimension(\n\u001b[0;32m-> 3330\u001b[0;31m           tensor_shape.dimension_value(t.get_shape()[0])))\n\u001b[0m\u001b[1;32m   3331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3332\u001b[0m     variant_tensor = gen_dataset_ops.tensor_slice_dataset(\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m       raise ValueError(\"Dimensions %s and %s are not compatible\" %\n\u001b[0;32m--> 289\u001b[0;31m                        (self, other))\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmerge_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Dimensions 1 and 2798 are not compatible"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7IDUN1tt8vS",
        "outputId": "2cb25858-7edc-4545-cf4e-b24ce7026e49"
      },
      "source": [
        "# The user only specifies the input nodes and output heads.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import autokeras as ak\n",
        "import pickle\n",
        "import os\n",
        "import math\n",
        "\n",
        "shortTermStep = 0.015\n",
        "shortTermWindow = 50\n",
        "shortTermWindow = float(shortTermWindow)/1000\n",
        "\n",
        "temp = pickle.load(open(\"./pickles/train/SS{}-SW{}.pickle\".format(shortTermStep, shortTermWindow), 'rb'))\n",
        "trainFeats = temp[0]\n",
        "trainLabels = temp[1]\n",
        "\n",
        "temp = pickle.load(open(\"./pickles/test/SS{}-SW{}.pickle\".format(shortTermStep, shortTermWindow), 'rb'))\n",
        "testFeats = temp[0]\n",
        "testLabels = temp[1]\n",
        "\n",
        "# x_train as pandas.DataFrame, y_train as pandas.Series\n",
        "x_train = pd.read_csv(open('./real mmse scores/mmseScore.csv'))\n",
        "print(type(x_train))  # pandas.DataFrame\n",
        "\n",
        "y_train = x_train.pop(\"score\")\n",
        "print(type(y_train))  # pandas.Series\n",
        "\n",
        "# You can also use pandas.DataFrame for y_train.\n",
        "y_train = pd.DataFrame(y_train)\n",
        "print(type(y_train))  # pandas.DataFrame\n",
        "\n",
        "# You can also use numpy.ndarray for x_train and y_train.\n",
        "x_train = x_train.values\n",
        "y_train = y_train.values\n",
        "print(type(x_train))  # numpy.ndarray\n",
        "print(type(y_train))  # numpy.ndarray\n",
        "\n",
        "\n",
        "x_test = pd.read_csv('./test-mmse/testMMSEscores.csv')\n",
        "y_test = x_test.pop(\"score\")\n",
        "\n",
        "# It tries 10 different models.\n",
        "reg = ak.StructuredDataRegressor(max_trials=10, overwrite=True)\n",
        "# Feed the structured data regressor with training data.\n",
        "reg.fit(x_train, y_train, epochs=20)\n",
        "\n",
        "predicted_y = reg.predict(x_test)\n",
        "# Evaluate the best model with testing data.\n",
        "print(\"55555555555555555555555555555555555555555555555\")\n",
        "print(math.sqrt(reg.evaluate(x_test, y_test)[0]))\n",
        "print(math.sqrt(reg.evaluate(x_test, y_test)[1]))\n",
        "rmses = []\n",
        "predicted = []\n",
        "\"\"\"\n",
        "for itest, fTest in enumerate(f_test):\n",
        "  R = regr.predict(fTest.reshape(1,-1))[0]\n",
        "  predicted.append(R)\n",
        "  mse = sklearn.metrics.mean_squared_error(actual, predicted)\n",
        "  rmse = math.sqrt(mse)\n",
        "\n",
        "rmses.append([rmse,neighVal])\n",
        "classifiers.append(regr)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "reg = ak.StructuredDataRegressor(max_trials=3, overwrite=True)\n",
        "reg.fit(trainFeats[0], trainLabels, epochs=2)\n",
        "\n",
        "\n",
        "reg = ak.StructuredDataRegressor(overwrite=True, max_trials=3)\n",
        "reg.fit('./real mmse scores/mmseScore.csv',\"score\", epochs=10)\n",
        "\n",
        "reg = ak.StructuredDataRegressor(overwrite=True, max_trials=3)\n",
        "reg.fit('./real mmse scores/',\"0\", epochs=10)\n",
        "\n",
        "reg = ak.StructuredDataRegressor(max_trials=3, overwrite=True)\n",
        "reg.fit(trainFeats[0], trainLabels, epochs=2)\n",
        "\n",
        "ak.AutoModel(\n",
        "    inputs=[ak.ImageInput(), ak.TextInput()],\n",
        "    outputs=[ak.ClassificationHead(), ak.RegressionHead()],\n",
        "    project_name=\"Test-Auto\")\n",
        "\n",
        "\n",
        "ak.fit(\n",
        "    x=trainFeats,\n",
        "    y=trainLabels,\n",
        "    batch_size=32,\n",
        "    epochs=None,\n",
        "    callbacks=None,\n",
        "    validation_data=[testFeats,testLabels],\n",
        "    verbose=1\n",
        ")\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 02s]\n",
            "val_loss: 17.249467849731445\n",
            "\n",
            "Best val_loss So Far: 17.24751853942871\n",
            "Total elapsed time: 00h 00m 28s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Epoch 1/20\n",
            "88/88 [==============================] - 0s 829us/step - loss: 540.6190 - mean_squared_error: 540.6190\n",
            "Epoch 2/20\n",
            "88/88 [==============================] - 0s 766us/step - loss: 468.9670 - mean_squared_error: 468.9670\n",
            "Epoch 3/20\n",
            "88/88 [==============================] - 0s 770us/step - loss: 318.8898 - mean_squared_error: 318.8898\n",
            "Epoch 4/20\n",
            "88/88 [==============================] - 0s 732us/step - loss: 147.7642 - mean_squared_error: 147.7642\n",
            "Epoch 5/20\n",
            "88/88 [==============================] - 0s 745us/step - loss: 76.1662 - mean_squared_error: 76.1662\n",
            "Epoch 6/20\n",
            "88/88 [==============================] - 0s 810us/step - loss: 74.9106 - mean_squared_error: 74.9106\n",
            "Epoch 7/20\n",
            "88/88 [==============================] - 0s 707us/step - loss: 75.7289 - mean_squared_error: 75.7289\n",
            "Epoch 8/20\n",
            "88/88 [==============================] - 0s 769us/step - loss: 71.9269 - mean_squared_error: 71.9269\n",
            "Epoch 9/20\n",
            "88/88 [==============================] - 0s 765us/step - loss: 71.7901 - mean_squared_error: 71.7901\n",
            "Epoch 10/20\n",
            "88/88 [==============================] - 0s 700us/step - loss: 73.3024 - mean_squared_error: 73.3024\n",
            "Epoch 11/20\n",
            "88/88 [==============================] - 0s 815us/step - loss: 71.5122 - mean_squared_error: 71.5122\n",
            "Epoch 12/20\n",
            "88/88 [==============================] - 0s 823us/step - loss: 72.4433 - mean_squared_error: 72.4433\n",
            "Epoch 13/20\n",
            "88/88 [==============================] - 0s 716us/step - loss: 68.6498 - mean_squared_error: 68.6498\n",
            "Epoch 14/20\n",
            "88/88 [==============================] - 0s 773us/step - loss: 67.7828 - mean_squared_error: 67.7828\n",
            "Epoch 15/20\n",
            "88/88 [==============================] - 0s 733us/step - loss: 71.0672 - mean_squared_error: 71.0672\n",
            "Epoch 16/20\n",
            "88/88 [==============================] - 0s 742us/step - loss: 70.3114 - mean_squared_error: 70.3114\n",
            "Epoch 17/20\n",
            "88/88 [==============================] - 0s 719us/step - loss: 71.1554 - mean_squared_error: 71.1554\n",
            "Epoch 18/20\n",
            "88/88 [==============================] - 0s 723us/step - loss: 70.6808 - mean_squared_error: 70.6808\n",
            "Epoch 19/20\n",
            "88/88 [==============================] - 0s 739us/step - loss: 70.3089 - mean_squared_error: 70.3089\n",
            "Epoch 20/20\n",
            "88/88 [==============================] - 0s 713us/step - loss: 71.1409 - mean_squared_error: 71.1409\n",
            "INFO:tensorflow:Assets written to: ./structured_data_regressor/best_model/assets\n",
            "39/39 [==============================] - 0s 712us/step\n",
            "55555555555555555555555555555555555555555555555\n",
            "39/39 [==============================] - 0s 685us/step - loss: 112.2202 - mean_squared_error: 112.2202\n",
            "10.593403228215491\n",
            "39/39 [==============================] - 0s 521us/step - loss: 112.2202 - mean_squared_error: 112.2202\n",
            "10.593403228215491\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nreg = ak.StructuredDataRegressor(max_trials=3, overwrite=True)\\nreg.fit(trainFeats[0], trainLabels, epochs=2)\\n\\n\\nreg = ak.StructuredDataRegressor(overwrite=True, max_trials=3)\\nreg.fit(\\'./real mmse scores/mmseScore.csv\\',\"score\", epochs=10)\\n\\nreg = ak.StructuredDataRegressor(overwrite=True, max_trials=3)\\nreg.fit(\\'./real mmse scores/\\',\"0\", epochs=10)\\n\\nreg = ak.StructuredDataRegressor(max_trials=3, overwrite=True)\\nreg.fit(trainFeats[0], trainLabels, epochs=2)\\n\\nak.AutoModel(\\n    inputs=[ak.ImageInput(), ak.TextInput()],\\n    outputs=[ak.ClassificationHead(), ak.RegressionHead()],\\n    project_name=\"Test-Auto\")\\n\\n\\nak.fit(\\n    x=trainFeats,\\n    y=trainLabels,\\n    batch_size=32,\\n    epochs=None,\\n    callbacks=None,\\n    validation_data=[testFeats,testLabels],\\n    verbose=1\\n)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfRRD_U8Iffg",
        "outputId": "9c434e21-7027-46e6-a6c7-86846d9895e2"
      },
      "source": [
        "for i, weights in enumerate(['uniform', 'distance']):\n",
        "  print(i)\n",
        "  print(weights)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "uniform\n",
            "\n",
            "1\n",
            "distance\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt-No55za9iq"
      },
      "source": [
        "listOFEverything22 = pickle.load(open(\"listOFEverything.pickle\", 'rb'))\n",
        "for line in listOFEverything22:\n",
        "  if count == 0:\n",
        "    listOFEverything22.append"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEu64zqotuw5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U-MQphbPot0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "1538f32d-2aa9-4247-9a92-b5b16bde0a0f"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fixing random state for reproducibility\n",
        "#np.random.seed(19680801)\n",
        "listOFEverything22 = pickle.load(open(\"listOFEverything.pickle\", 'rb'))\n",
        "print(len(listOFEverything22))\n",
        "\n",
        "frameRate = []\n",
        "error = []\n",
        "overlap = []\n",
        "\n",
        "for sets in listOFEverything:\n",
        "  error.append(sets[3][1])\n",
        "  frameRate.append((sets[0] * 1000))\n",
        "  overlap.append((sets[1]*1000))\n",
        "\n",
        "N = 280\n",
        "x = frameRate\n",
        "y = error\n",
        "\n",
        "plt.scatter(x, y, alpha=0.5)\n",
        "x = [0,100]\n",
        "y = [0,30]\n",
        "plt.xlabel('FrameRate') \n",
        "plt.ylabel('Error')\n",
        "plt.title('Plot of SVM RBF Regresion')\n",
        "plt.plot(x, y, color='red',linestyle='dashed')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "OiTDatkMyI5I",
        "outputId": "2426ced7-0cd8-418d-9f32-fb2a115d1692"
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "listOFEverything22 = pickle.load(open(\"listOFEverything.pickle\", 'rb'))\n",
        "print(len(listOFEverything22))\n",
        "\n",
        "frameRate = []\n",
        "error = []\n",
        "overlap = []\n",
        "\n",
        "for sets in listOFEverything:\n",
        "  error.append(sets[3][2])\n",
        "  frameRate.append((sets[0] * 1000))\n",
        "  overlap.append((sets[1] * 1000))\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "x = frameRate\n",
        "y = error\n",
        "z = overlap\n",
        "\n",
        "\n",
        "ax.scatter(x, y, z, c='r', marker='o')\n",
        "\n",
        "ax.set_xlabel('FrameRate')\n",
        "ax.set_ylabel('Error')\n",
        "ax.set_zlabel('Overlap')\n",
        "plt.title('Plot of DT Regresion')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg8tvrVCYzkW"
      },
      "source": [
        "from pyAudioAnalysis import audioTrainTest as aT\n",
        "import os\n",
        "baseDir = './ADReSS data/ADReSS-IS2020-data/train/norm_wave_sorted/real mmse scores'\n",
        "\n",
        "aT.feature_extraction_train_regression(baseDir, 1.0, 1.0, aT.shortTermWindow, aT.shortTermStep, \"randomforest\", \"randomforestRegress\", False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2NktfmIVU3D"
      },
      "source": [
        "from pyAudioAnalysis import audioTrainTest as aT\n",
        "import os\n",
        "baseDir = './ADReSS data/ADReSS-IS2020-data/train/norm_wave_sorted/real mmse scores'\n",
        "\n",
        "\n",
        "aT.feature_extraction_train_regression(baseDir, 1.0, 1.0, aT.shortTermWindow, aT.shortTermStep, \"svm\", \"svmRegression\", False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-_kb_0Ox8fF"
      },
      "source": [
        "import os\n",
        "baseDir = './ADReSS data/ADReSS-IS2020-data/train/norm_wave_sorted/real mmse scores'\n",
        "\n",
        "len(os.listdir(baseDir)) cx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_HE8sKMbYOI"
      },
      "source": [
        "type(os.listdir(baseDir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T33b6AggwFo"
      },
      "source": [
        "import shutil\n",
        "\n",
        "shutil.copy('./ADReSS data/ADReSS-IS2020-data/train/cc_meta_data.txt' , './ADReSS data/ADReSS-IS2020-data/train/norm_wave_sorted/mmse scores/cc_meta_data.csv')\n",
        "shutil.copy('./ADReSS data/ADReSS-IS2020-data/train/cd_meta_data.txt' , './ADReSS data/ADReSS-IS2020-data/train/norm_wave_sorted/mmse scores/cd_meta_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EaswmWz487Y"
      },
      "source": [
        "from pyAudioAnalysis import audioTrainTest as aT\n",
        "early = './ADReSS data/ADReSS-IS2020-data/train/norm_wave_sorted/early'\n",
        "moderate = './ADReSS data/ADReSS-IS2020-data/train/norm_wave_sorted/moderate'\n",
        "norm = './ADReSS data/ADReSS-IS2020-data/train/norm_wave_sorted/norm'\n",
        "severe = './ADReSS data/ADReSS-IS2020-data/train/norm_wave_sorted/severe'\n",
        "\n",
        "from pyAudioAnalysis import MidTermFeatures as aF\n",
        "features, class_names, _ = aF.multiple_directory_feature_extraction([norm,early,moderate,severe], 1.0, 1.0,\n",
        "                                                 aT.shortTermWindow, aT.shortTermStep,\n",
        "                                                 compute_beat=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZcnXAVfeAX9"
      },
      "source": [
        "feature_matrix, labels = aT.features_to_matrix(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh4yH4xkL-iX"
      },
      "source": [
        "import pickle\n",
        "featnLabel = [feature_matrix, labels]\n",
        "pickle.dump( featnLabel, open( \"all4FeauturenLabels.p\", \"wb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiwH_X5VNe1M"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(feature_matrix, labels, test_size=0.05, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl_KVar5bSLg"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=100000)\n",
        "logisticRegr.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bhn7LO66Nvom"
      },
      "source": [
        "score = logisticRegr.score(x_test, y_test)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_ZxlkdYhxbK"
      },
      "source": [
        "import shutil\n",
        "f = open('./ADReSS data/ADReSS-IS2020-data/train/cc_meta_data.txt')\n",
        "lines = f.readlines()\n",
        "f.close()\n",
        "f = open('./ADReSS data/ADReSS-IS2020-data/train/cd_meta_data.txt')\n",
        "lines2 = f.readlines()\n",
        "f.close()\n",
        "fileCopyList = []\n",
        "listOfWavs = os.listdir('./ADReSS data/ADReSS-IS2020-data/train/Normalised_audio-chunks/cc/')\n",
        "\n",
        "\n",
        "\n",
        "for line in lines:\n",
        "  line = line.rstrip().strip().split(';')\n",
        "  try:\n",
        "    line[3] = int(line[3])\n",
        "    line[0] = line[0].replace(\" \", \"\")\n",
        "  except:\n",
        "    continue\n",
        "  for wav in listOfWavs:\n",
        "    if line[0] in wav:\n",
        "      srcDir = './ADReSS data/ADReSS-IS2020-data/train/Normalised_audio-chunks/cc/{}'.format(str(wav))\n",
        "      baseDir = './ADReSS data/ADReSS-IS2020-data/train/norm_wave_sorted' \n",
        "      if line[3] <= 9:\n",
        "        try:\n",
        "          fileCopyList.append([srcDir,'{}/severe/{}'.format(baseDir, str(wav))])\n",
        "        except:\n",
        "          print('FAILED1: {}', wav)\n",
        "      elif (line[3] <= 20) and (line[3] >= 10):\n",
        "        try:\n",
        "          fileCopyList.append([srcDir,'{}/moderate/{}'.format(baseDir, str(wav))])\n",
        "        except:\n",
        "          print('FAILED2: {}', wav)\n",
        "      elif (line[3] <= 25) and (line[3] >= 21):\n",
        "        try:\n",
        "          fileCopyList.append([srcDir,'{}/early/{}'.format(baseDir, str(wav))])\n",
        "        except:\n",
        "          print('FAILED3: {}', wav)\n",
        "      elif line[3] >= 26:\n",
        "        try:\n",
        "          fileCopyList.append([srcDir,'{}/norm/{}'.format(baseDir, str(wav))])\n",
        "        except:\n",
        "          print('FAILED4: {}', wav) \n",
        "\n",
        "listOfWavs = os.listdir('./ADReSS data/ADReSS-IS2020-data/train/Normalised_audio-chunks/cd/')\n",
        "for line in lines2:\n",
        "  line = line.rstrip().strip().split(';')\n",
        "  try:\n",
        "    line[3] = int(line[3])\n",
        "    line[0] = line[0].replace(\" \", \"\")\n",
        "  except:\n",
        "    continue\n",
        "  for wav in listOfWavs:\n",
        "    if line[0] in wav:\n",
        "      srcDir = './ADReSS data/ADReSS-IS2020-data/train/Normalised_audio-chunks/cd/{}'.format(str(wav))\n",
        "      baseDir = './ADReSS data/ADReSS-IS2020-data/train/norm_wave_sorted' \n",
        "      if line[3] <= 9:\n",
        "        try:\n",
        "          fileCopyList.append([srcDir,'{}/severe/{}'.format(baseDir, str(wav))])\n",
        "        except:\n",
        "          print('FAILED1: {}', wav)\n",
        "      elif (line[3] <= 20) and (line[3] >= 10):\n",
        "        try:\n",
        "          fileCopyList.append([srcDir,'{}/moderate/{}'.format(baseDir, str(wav))])\n",
        "        except:\n",
        "          print('FAILED2: {}', wav)\n",
        "      elif (line[3] <= 25) and (line[3] >= 21):\n",
        "        try:\n",
        "          fileCopyList.append([srcDir,'{}/early/{}'.format(baseDir, str(wav))])\n",
        "        except:\n",
        "          print('FAILED3: {}', wav)\n",
        "      elif line[3] >= 26:\n",
        "        try:\n",
        "          fileCopyList.append([srcDir,'{}/norm/{}'.format(baseDir, str(wav))])\n",
        "        except:\n",
        "          print('FAILED4: {}', wav) \n",
        "\n",
        "\n",
        "\n",
        "print(fileCopyList)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}